[
  {
    "objectID": "overview.html",
    "href": "overview.html",
    "title": "Föreläsningar MASB11",
    "section": "",
    "text": "F0. Beskrivande statistik (självstudier)/Descriptive statistics (self studies)\nKvalitativa och kvantitativa data/Qualitative and quantitative data\nMedian/Median\nMedelvärde/Mean\nVarians/Variance\nStandardavvikelse/Standard deviation\nKvantil/Quantile\nStapeldiagram/Bar chart\nHistogram/Histogram\nLådadiagram/Boxplot\nKorrelationskoefficient/Coefficient of correlation\nKvantil-kvantil plot/Quantile-quantile plot\n\n\nF1. Grunderna i sannolikhetslära\nSannolikhet/Probability\nSlumpmässigt försök/Random experiment\nUtfall/Outcome\nUtfallsrum/Outcome space\nHändelse/Event\nRegler för sannolikhet/Probability rules\nVenn-diagram/Venn diagram\nKomplementhändelse/Complementary event\nUteslutande händelse/Complementary event\nAdditionssatsen/The additive theorem of probability\nOberoende händelser/Independent events\nBetingad sannolikhet/Conditional probability\nMultiplikationssatsen/The multiplication theorem of probability\nTotal sannolikhet/The law of total probability\nBayes sats/Bayes’ theorem (Bayes’ rule)\nKombinatorik/Combinatorics\n\n\nF2. Diskreta slumpvariabler/Discrete random variables\nSlumpvariabler/Random variables\nDiskreta och kontinuerliga slumpvariabler/Discrete and continuous r.v.\nSannolikhetsfunktion/Probability function\nFördelningsfunktion/Distribution function\nBernoullifördelning/Bernoulli distribution\nLikformig heltalsfördelning/Discrete uniform distribution\nVäntevärde, varians, standardavvikelse /Expected value, variance, standard deviation\nVäntevärde för en diskret s.v./Expected value of a discrete r.v.\nVariansen för en diskret s.v./Variance of a discrete r.v.\nPoissonfördelning/Poisson distribution\nBinomialfördelning/Binomial distribution\nVäntevärde av en funktion av en slumpvariabel/Expected value of a function of a random variable\n\n\nF3. Diskreta och kontinuerliga slumpvariabler/Discrete and continuous random variables\nKontinuerliga slumpvariabler/Continuous random variables\nLikformig fördelning/Uniform distribution\nTäthetsfunktion/Density function\nFördelningsfunktion för en kontinuerlig s.v./Distribution function for a continuous random variable\nKomplementhändelse för kontinuerlig s.v./Complementary event for a continuous r.v.\nVäntevärde och varians för en kontinuerlig s.v./Expected value of a continuous r.v.\nVarians för en kontinuerlig s.v./Variance of a continuous r.v.\nExponentialfördelning/Exponential distribution\nNormalfördelning/Normal distribution\n\n\nF4. Kontinuerliga slumpvariabler, normalfördelning/Continuous random variables, normal distribution\nLognormalfördelning/Lognormal distribution\nStandardiserad normalfördelning/Standardised normal distribution\nNormalkvantil/Normal quantiles\nVäntevärde och varians av en linjärkombination av en slumpvariabel/Expected value and variance of a linear combination of a random variable\nVäntevärde och varians för summor av slumpvariabler/Expected value and variance for sums of random variables\nFördelningen för en summa av normalfördelade slumpvariabler/Distribution of a sum of normally distributed random variables\nPopulation/parametrar – Stickprov/statistiska (stickprovsmått)/Population/parameters - Sample/statistica\n\n\nF5. Stickprovsfördelningar, Centrala gränsvärdessatsen/Sampling distributions, Central Limit Theorem\nStatistisk modell / Statistical model\nFördelning för stickprov / Sampling distributions\nCentrala gränsvärdessatsen (CGS)/Central Limit Theorem\nCGS och binomialfördelningen/CGS and the binomial distribution\nNormalapproximationer/Normal approximations\nFrån sannolikhetsteori till statistisk inferens/From probability theory to statistical inference\nSkattningar av parameter (estimator)/Parameter estimation (estimates)\nVäntevärdesriktig och effektiv estimator/Unbiased and effective estimator\nt-fördelning/t-distribution\n\n\nF6. Skattning, Konfidensinterval/Estimation, confidence interval\nStickprovsfördelningar för känd och okänd varians/Sampling distributions for known and unknown variance\nSkattning och konfidensintervall/Estimation and confidence interval\nKonfidensintervall känd och okänd varians t-fördelning/Confidence interval with known and unknown variance\nSkattning av parametrar i en lognormalfördelning/Parameter estimation in a lognormal distribution\n\n\nF7. Hypotestest/Hypothesis testing\nHypotestest på tre sätt/Hypothesis testing in three ways\n\nTeststorhet och kritiskt område/Test statistic with critical area\nKonfidensintervall/Confidence interval\nMed ett p-värde (direktmetoden)/P-value (direct method)\n\nMöjliga felslut/Possible errors\nStyrka på ett hypotestest/Power of an hypothesis test\n\n\nF8. Två stickprov/Two random samples\n\\(\\chi^2\\)-fördelningen/\\(\\chi^2\\) distribution\nHypotestest för varians/Hypothesis test for variance\nJämföra två populationer/Compare populations\nStickprov i par/Paired samples\nTvå oberoende stickprov/Indpendent samples\nLika och olika varians/Equal and non-equal variance\nTesta om lika varians/Test if variances are equal\nF-fördelningen/F distribution\n\n\nF9. Inferens för diskret och kategorisk data/Inference of discrete and categorical data\nDirekt-metoden och Normalapproximation/Direct method and normal approximation\nIcke-parametriskt test/Non-parametric test\nInferens om en proportion/Inference of a proportion\nInferens om skillnad mellan två proportioner/Inference on the difference between to proportions\nAnalys av kategoriska data:/Analsyis of categorical data\n\nTesta modellanpassning/Test model fit\nHomogenitetstest/Testing homogeneity\nOberoendetest/Independence test\n\nParametriskt vs Icke-parametriskt test/Parametric vs non-parametric test\n\n\nF10. Enkel linjär regression/Simple linear regression\nBivariat normalfördelning/Bivariate normal distribution\nKovarians/Covariance\nKorrelation/Correlation\nKorrelationsanalys/Correlation analysis\nRegressionsanalys/Regression analysis\n\n\nF11. Variansanalys/Analysis of variance\nVariansanalys/Analysis of variance\nRepetition\n\n\nF12. Reserv och repetition\nRepetition"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Föreläsningsanteckningar",
    "section": "",
    "text": "F0. Beskrivande statistik (självstudier)\nKvalitativa och kvantitativa data\nMedian\nMedelvärde\nVarians\nStandardavvikelse\nKvantil\nStapeldiagram\nHistogram\nLådadiagram\nKorrelationskoefficient\nKvantil-kvantil plot\n\n\nF1. Grunderna i sannolikhetslära\nSannolikhet\nSlumpmässigt försök\nUtfall\nUtfallsrum\nHändelse\nRegler för sannolikhet\nVenn-diagram\nKomplementhändelse\nUteslutande händelse\nAdditionssatsen\nOberoende händelser\nBetingad sannolikhet\nMultiplikationssatsen\nTotal sannolikhet\nBayes sats\nKombinatorik\nFöreläsning 1\n\n\nF2. Diskreta slumpvariabler\nSlumpvariabler\nDiskreta och kontinuerliga slumpvariabler\nSannolikhetsfunktion\nFördelningsfunktion\nBernoullifördelning\nLikformig heltalsfördelning\nVäntevärde, varians, standardavvikelse\nVäntevärde för en diskret s.v.\nVariansen för en diskret s.v.\nPoissonfördelning\nBinomialfördelning\nVäntevärde av en funktion av en slumpvariabel\nFöreläsning 2\n\n\nF3. Diskreta och kontinuerliga slumpvariabler\nKontinuerliga slumpvariabler\nLikformig fördelning\nTäthetsfunktion\nFördelningsfunktion för en kontinuerlig s.v.\nKomplementhändelse för kontinuerlig s.v.\nVäntevärde och varians för en kontinuerlig s.v.\nVarians för en kontinuerlig s.v.\nExponentialfördelning\nNormalfördelning\nFöreläsning 3\n\n\nF4. Kontinuerliga slumpvariabler, normalfördelning\nLognormalfördelning\nStandardiserad normalfördelning\nNormalkvantil\nVäntevärde och varians av en linjärkombination av en slumpvariabel\nVäntevärde och varians för summor av slumpvariabler\nFördelningen för en summa av normalfördelade slumpvariabler\nPopulation/parametrar – Stickprov/statistiska (stickprovsmått)\nFöreläsning 4\n\n\nF5. Stickprovsfördelningar, Centrala gränsvärdessatsen\nStatistisk modell\nFördelning för stickprov\nCentrala gränsvärdessatsen (CGS)\nCGS och binomialfördelningen\nNormalapproximationer\nFrån sannolikhetsteori till statistisk inferens\nSkattningar av parameter (estimator)\nVäntevärdesriktig och effektiv estimator\nt-fördelning\nFöreläsning 5\n\n\nF6. Skattning, Konfidensinterval\nStickprovsfördelningar för känd och okänd varians\nSkattning och konfidensintervall\nKonfidensintervall känd och okänd varians t-fördelning\nSkattning av parametrar i en lognormalfördelning\nFöreläsning 6\n\n\nF7. Hypotestest\nHypotestest på tre sätt\n\nTeststorhet och kritiskt område\nKonfidensintervall\nMed ett p-värde (direktmetoden)\n\nMöjliga felslut\nStyrka på ett hypotestest\nFöreläsning 7\n\n\nF8. Två stickprov\n\\(\\chi^2\\)-fördelningen\nHypotestest för varians\nJämföra två populationer\nStickprov i par\nTvå oberoende stickprov\nLika och olika varians\nTesta om lika varians\nF-fördelningen\nFöreläsning 8\n\n\nF9. Inferens för diskret och kategorisk data\nDirekt-metoden och Normalapproximation\nIcke-parametriskt test\nInferens om en proportion\nInferens om skillnad mellan två proportioner\nAnalys av kategoriska data:\n\nTesta modellanpassning\nHomogenitetstest\nOberoendetest\n\nParametriskt vs Icke-parametriskt test\nFöreläsning 9\n\n\nF10. Enkel linjär regression\nBivariat normalfördelning\nKovarians\nKorrelation\nKorrelationsanalys\nRegressionsanalys\nFöreläsning 10\n\n\nF11. Variansanalys\nVariansanalys\nRepetition\nFöreläsning 11\n\n\nF12. Reserv och repetition\nRepetition"
  },
  {
    "objectID": "F9_anteckningar.html",
    "href": "F9_anteckningar.html",
    "title": "F9. Statistisk inferens på diskreta och kategoriska data",
    "section": "",
    "text": "Testa genom att anta eller inte anta en fördelning för populationen\n\n\n\n\n\n\nExempel. Energiförbrukning\n\n\n\nMan vill undersöka om energiförbrukning i vila är annorlunda hos personer drabbade av cystisk fibros jämfört med friska personer. Tio par matchades ihop, en i varje par hade sjukdomen medan den andre var frisk. För övrigt var personerna i varje par lika beträffande kön, ålder, vikt och längd. Resultat i energiförbrukning (kcal/dag):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPar\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\nCF\n1153\n1132\n1165\n1460\n1634\n1493\n1358\n1453\n1185\n1824\n\n\nFriska\n996\n1080\n1182\n1452\n1162\n1619\n1140\n1123\n1113\n1463\n\n\nSkillnad CF-Friska\n157\n52\n-17\n8\n427\n-126\n218\n330\n72\n361\n\n\n\nStickprovsstorleken \\(n=10\\) är för liten för att använda sig av Centrala Gränsvärdessatsen för normalapproximation.\n\nHur gör vi om det är rimligt att tänka sig att differenserna kommer från en normalfördelning?\nHur gör vi utan antagande om fördelning?\n\n\n\n\n\nLåt oss undersöka om antagandet verkar stämma genom att göra en kvantil-kvantil-plot för normalfördelningen.\n\nx = c(157, 52, -17, 8, 427, -126, 218, 330, 72, 361)\nqqnorm(x)\nqqline(x)\n\n\n\n\n\n\n\n\nJa, det verkar stämma.\nModell: \\(X = \\text{\"Skillnad i energiförbrukning inom par\"} \\sim N(\\mu,\\sigma)\\)\nHypoteser: \\(H_0: \\mu = 0\\) mot \\(H_1: \\mu \\neq 0\\)\nTestregel: Vi väljer en signifikansnivå på \\(\\alpha=0.05\\) och testar genom att bilda ett 95%-igt två-sidigt konfidensintervall för väntevärdet och förkasta \\(H_0\\) om intervallet inte täcker noll.\n\\[I_{\\mu}: \\bar{x} \\pm t_{\\alpha/2}(n-1)\\frac{s}{\\sqrt{n}}\\]\nVi beräknar följande \\(\\bar{x} = 148.2\\) och \\(s = 182\\) från stickprovet och erhåller intervallet \\((17.99,278.41)\\)\n⇒ \\(H_0\\) förkastas på signifikansnivå 5%.\n\n\n\nVi söker efter ett test där vi inte antar någon fördelning om stickprovet. Ett sådant test kallas även för ett icke-parametriskt test.\nExempel är \\(\\chi^2\\)-test, Mann-Whitneys U-test.\n\n\nVi undersöker hur många differenser som är positiva respektive negativa.\n\n\n\n\n\n\nExempel. Energiförbrukning (forts.)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPar\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\nSkillnad CF-Friska\n157\n52\n-17\n8\n427\n-126\n218\n330\n72\n361\n\n\nTecken\n+\n+\n-\n+\n+\n-\n+\n+\n+\n+\n\n\n\n\n\nModell: \\(W = \\text{\"Antal positiva skillnader utav 10 möjliga\"}\\sim Bin(10,p)\\) där \\(p\\) är sannolikheten att man får en positiv skillnad.\nVi observerar att \\(w = 8\\)\nHypoteser:\n\\(H_0: p = 0.5\\) (det är lika troligt att det blir + som -)\n\\(H_1: p &gt; 0.5\\) (det är mer troligt att det blir + än -)\nTestregel: Vi väljer signifikansnivå till att vara \\(\\alpha = 0.05\\).\nUnder \\(H_0\\) är \\(W\\sim Bin(10,0.5)\\)\nVi testar med direktmetoden:\n\\(p\\)-värdet \\(=P(W\\geq 8|H_0) = 1 -P(W\\leq 7|H_0) = 1 - 0.945 = 0.055\\)\n⇒ \\(H_0\\) förkastas kan ej förkastas eftersom \\(p\\)-värdet &gt; \\(\\alpha\\).\n\n\n\n\n\n\n\n\n\n\nExempel. Idrott på skola\n\n\n\nI en skola ville man göra en liten pilotstudie för att se om en annorlunda idrottsträning på kort tid skulle kunna påverka skolbarnens fysiska prestationer. Man valde ut 16 barn, som var likvärdiga beträffande den fysiska kapaciteten. Barnen delades slumpmässigt in i två grupper. Under en månad följde hälften av barnen (grupp A) den normala undervisningen i ämnen Idrott och hälsa, medan de övriga barnen (grupp B) dessutom fick delta i den speciella träningen. När en månad hade gått fick barnen vid ett gemensamt tillfälle springa en kort terrängbana och deras tider noterades. Två barn i grupp A var sjuka under testdagen. Resultat (sekunder):\n\n\n\nGrupp A\n64\n62\n73\n54\n66\n71\n\n\n\n\nGrupp B\n53\n74\n70\n59\n42\n38\n48\n60\n\n\n\n\n\nOm man inte vill anta en fördelning för de två stickproven, kan man istället göra ett icke-parametriskt (eller fördelningsfritt) test. Mann-Whitney’s rangsummetest går ut på att man rangordnar värdena i båda stickproven och räknar ut rangsumman för dem, sen skapar man en teststorhet och jämför den mot ett kritiskt område för teststorheten. Nedan har jag ersatt observationerna med dess ranger, och räknar ut rangsumman per grupp.\n\nRangvärden\n\n\n\n\n\n\n\n\n\n\n\nRangsumma\n\n\n\n\nGrupp A\n9\n8\n13\n5\n10\n12\n\n\n57\n\n\nGrupp B\n4\n14\n11\n6\n2\n1\n3\n7\n48\n\n\n\nVi går inte igenom icke-parametriska test i detalj. Det är dock viktigt att känna till att det finns icke-parametriska test och vilka för-och nackdelar de har.\n\n\n\n\n\n\n\n\n\n\n\nSituation\nAntar normalfördelning\nParametriskt test\nAntar ingen fördelning\nIcke-parametriskt test\n\n\n\n\nEtt stickprov\nt-test\nteckentest\n\n\nTvå parade stickprov\nt-test på differens\nteckentest på differenser\n\n\nTvå oberoende stickprov\nt-test för två oberoende stickprov\nRangsummetest (Mann-Whitney)\n\n\nFlera oberoende stickprov\nensidig variansanalys\nRangsummetest (Kruskal-Wallis)\n\n\n\n\n\n\n(+) Behöver inte göra antaganden om fördelning hos data\n(+) Fungerar för små stickprov\n(+) “Robust” mot avvikande värden (engelska: outliers)\n(-) Är inte lika “känsliga” som (har lägre styrka än) de test som baseras på normalfördelning.\n(-) Nollhypotesen är oftast inte lika specificerad som i “traditionella” test\n(-) Utnyttjar inte all information om fördelningen som ges i data - baseras ofta på ranger, inte på de aktuella värdena"
  },
  {
    "objectID": "F9_anteckningar.html#a-antag-att-stickprovet-kommer-från-en-normalfördelning",
    "href": "F9_anteckningar.html#a-antag-att-stickprovet-kommer-från-en-normalfördelning",
    "title": "F9. Statistisk inferens på diskreta och kategoriska data",
    "section": "",
    "text": "Låt oss undersöka om antagandet verkar stämma genom att göra en kvantil-kvantil-plot för normalfördelningen.\n\nx = c(157, 52, -17, 8, 427, -126, 218, 330, 72, 361)\nqqnorm(x)\nqqline(x)\n\n\n\n\n\n\n\n\nJa, det verkar stämma.\nModell: \\(X = \\text{\"Skillnad i energiförbrukning inom par\"} \\sim N(\\mu,\\sigma)\\)\nHypoteser: \\(H_0: \\mu = 0\\) mot \\(H_1: \\mu \\neq 0\\)\nTestregel: Vi väljer en signifikansnivå på \\(\\alpha=0.05\\) och testar genom att bilda ett 95%-igt två-sidigt konfidensintervall för väntevärdet och förkasta \\(H_0\\) om intervallet inte täcker noll.\n\\[I_{\\mu}: \\bar{x} \\pm t_{\\alpha/2}(n-1)\\frac{s}{\\sqrt{n}}\\]\nVi beräknar följande \\(\\bar{x} = 148.2\\) och \\(s = 182\\) från stickprovet och erhåller intervallet \\((17.99,278.41)\\)\n⇒ \\(H_0\\) förkastas på signifikansnivå 5%."
  },
  {
    "objectID": "F9_anteckningar.html#b-antag-ingen-fördelning-för-stickprovet",
    "href": "F9_anteckningar.html#b-antag-ingen-fördelning-för-stickprovet",
    "title": "F9. Statistisk inferens på diskreta och kategoriska data",
    "section": "",
    "text": "Vi söker efter ett test där vi inte antar någon fördelning om stickprovet. Ett sådant test kallas även för ett icke-parametriskt test.\nExempel är \\(\\chi^2\\)-test, Mann-Whitneys U-test.\n\n\nVi undersöker hur många differenser som är positiva respektive negativa.\n\n\n\n\n\n\nExempel. Energiförbrukning (forts.)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPar\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\nSkillnad CF-Friska\n157\n52\n-17\n8\n427\n-126\n218\n330\n72\n361\n\n\nTecken\n+\n+\n-\n+\n+\n-\n+\n+\n+\n+\n\n\n\n\n\nModell: \\(W = \\text{\"Antal positiva skillnader utav 10 möjliga\"}\\sim Bin(10,p)\\) där \\(p\\) är sannolikheten att man får en positiv skillnad.\nVi observerar att \\(w = 8\\)\nHypoteser:\n\\(H_0: p = 0.5\\) (det är lika troligt att det blir + som -)\n\\(H_1: p &gt; 0.5\\) (det är mer troligt att det blir + än -)\nTestregel: Vi väljer signifikansnivå till att vara \\(\\alpha = 0.05\\).\nUnder \\(H_0\\) är \\(W\\sim Bin(10,0.5)\\)\nVi testar med direktmetoden:\n\\(p\\)-värdet \\(=P(W\\geq 8|H_0) = 1 -P(W\\leq 7|H_0) = 1 - 0.945 = 0.055\\)\n⇒ \\(H_0\\) förkastas kan ej förkastas eftersom \\(p\\)-värdet &gt; \\(\\alpha\\)."
  },
  {
    "objectID": "F9_anteckningar.html#rangsummetest",
    "href": "F9_anteckningar.html#rangsummetest",
    "title": "F9. Statistisk inferens på diskreta och kategoriska data",
    "section": "",
    "text": "Exempel. Idrott på skola\n\n\n\nI en skola ville man göra en liten pilotstudie för att se om en annorlunda idrottsträning på kort tid skulle kunna påverka skolbarnens fysiska prestationer. Man valde ut 16 barn, som var likvärdiga beträffande den fysiska kapaciteten. Barnen delades slumpmässigt in i två grupper. Under en månad följde hälften av barnen (grupp A) den normala undervisningen i ämnen Idrott och hälsa, medan de övriga barnen (grupp B) dessutom fick delta i den speciella träningen. När en månad hade gått fick barnen vid ett gemensamt tillfälle springa en kort terrängbana och deras tider noterades. Två barn i grupp A var sjuka under testdagen. Resultat (sekunder):\n\n\n\nGrupp A\n64\n62\n73\n54\n66\n71\n\n\n\n\nGrupp B\n53\n74\n70\n59\n42\n38\n48\n60\n\n\n\n\n\nOm man inte vill anta en fördelning för de två stickproven, kan man istället göra ett icke-parametriskt (eller fördelningsfritt) test. Mann-Whitney’s rangsummetest går ut på att man rangordnar värdena i båda stickproven och räknar ut rangsumman för dem, sen skapar man en teststorhet och jämför den mot ett kritiskt område för teststorheten. Nedan har jag ersatt observationerna med dess ranger, och räknar ut rangsumman per grupp.\n\nRangvärden\n\n\n\n\n\n\n\n\n\n\n\nRangsumma\n\n\n\n\nGrupp A\n9\n8\n13\n5\n10\n12\n\n\n57\n\n\nGrupp B\n4\n14\n11\n6\n2\n1\n3\n7\n48\n\n\n\nVi går inte igenom icke-parametriska test i detalj. Det är dock viktigt att känna till att det finns icke-parametriska test och vilka för-och nackdelar de har."
  },
  {
    "objectID": "F9_anteckningar.html#översikt-av-tester-för-kontinuerliga-data",
    "href": "F9_anteckningar.html#översikt-av-tester-för-kontinuerliga-data",
    "title": "F9. Statistisk inferens på diskreta och kategoriska data",
    "section": "",
    "text": "Situation\nAntar normalfördelning\nParametriskt test\nAntar ingen fördelning\nIcke-parametriskt test\n\n\n\n\nEtt stickprov\nt-test\nteckentest\n\n\nTvå parade stickprov\nt-test på differens\nteckentest på differenser\n\n\nTvå oberoende stickprov\nt-test för två oberoende stickprov\nRangsummetest (Mann-Whitney)\n\n\nFlera oberoende stickprov\nensidig variansanalys\nRangsummetest (Kruskal-Wallis)"
  },
  {
    "objectID": "F9_anteckningar.html#för--och-nackdelar-med-icke-parametriska-tester",
    "href": "F9_anteckningar.html#för--och-nackdelar-med-icke-parametriska-tester",
    "title": "F9. Statistisk inferens på diskreta och kategoriska data",
    "section": "",
    "text": "(+) Behöver inte göra antaganden om fördelning hos data\n(+) Fungerar för små stickprov\n(+) “Robust” mot avvikande värden (engelska: outliers)\n(-) Är inte lika “känsliga” som (har lägre styrka än) de test som baseras på normalfördelning.\n(-) Nollhypotesen är oftast inte lika specificerad som i “traditionella” test\n(-) Utnyttjar inte all information om fördelningen som ges i data - baseras ofta på ranger, inte på de aktuella värdena"
  },
  {
    "objectID": "F9_anteckningar.html#väntevärde-och-varians-för-skattningen-av-propotionen",
    "href": "F9_anteckningar.html#väntevärde-och-varians-för-skattningen-av-propotionen",
    "title": "F9. Statistisk inferens på diskreta och kategoriska data",
    "section": "Väntevärde och varians för skattningen av propotionen",
    "text": "Väntevärde och varians för skattningen av propotionen\nVisa att skattningen av proportionen är väntevärdesriktning och härled dess varians.\n\\(E(\\hat{p})=E(\\frac{X}{n}) = \\frac{E(X)}{n} = \\frac{n\\cdot p}{n} = p\\) ⇒ VVR!\n\\(V(\\hat{p})=V(\\frac{X}{n}) = \\frac{V(X)}{n^2} = \\frac{n\\cdot p \\cdot (1-p)}{n^2} = \\frac{p \\cdot (1-p)}{n}\\)\nVariansen för skattningen av proportionen minskar med \\(n\\)!"
  },
  {
    "objectID": "F9_anteckningar.html#normalapproximation-av-skattningen-av-proportionen",
    "href": "F9_anteckningar.html#normalapproximation-av-skattningen-av-proportionen",
    "title": "F9. Statistisk inferens på diskreta och kategoriska data",
    "section": "Normalapproximation av skattningen av proportionen",
    "text": "Normalapproximation av skattningen av proportionen\nOm \\(n\\cdot p \\cdot (1-p) &gt; 10\\) kan vi normalapproximera binomialfördelningen till en normalfördelning\n\\[X \\overset{A} \\sim N(np,\\sqrt{np(1-p)})\\]\nVi med samma resonemang kan vi approximera samplingsfördelningen för skattningen av proportionen till en normalfördelning\n\\[\\frac{X}{n} \\overset{A} \\sim N(p,\\sqrt{\\frac{p(1-p)}{n}})\\] där \\(\\hat{p} = \\frac{x}{n}\\)."
  },
  {
    "objectID": "F9_anteckningar.html#hypotestest-för-proportion",
    "href": "F9_anteckningar.html#hypotestest-för-proportion",
    "title": "F9. Statistisk inferens på diskreta och kategoriska data",
    "section": "Hypotestest för proportion",
    "text": "Hypotestest för proportion\nSå här kan hypoteser för en proportion se ut, där vi har en tvåsidig mothypotes.\n\\(H_0: p = p_0\\)\n\\(H_1: p \\neq p_0\\)\n\nNotera att det är sällan man vill testa om en proportion är noll. Istället kan man vara intresserad av om den är lika med ett visst värde \\(p_0\\).\n\n\nKonfidensintervall för en skattning av en proportion\nNär vi gör en normalapproximation kan vi gå vidare och skapa ett tvåsidigt konfidensintervall genom att stoppa in skattningen på proportionen i medelfelet:\n\\[I_p: \\hat{p} \\pm \\lambda_{\\alpha/2}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\]\nIntervallet \\(I_p\\) kan sedan användas för hypotestest. Vi förkastar nollhypotesen på signifikansnivå \\(\\alpha\\) om \\(p_0\\) inte ingår i intervallet.\n\n\nTeststorhet för en skattning av en proportion\nVi kan även testa hypoteserna med en teststorhet och kritiskt område.\nOm \\(H_0\\) är sann, gäller att \\(X \\sim Bin(n,p_0)\\)\nHär kan vi undersöka om det går att normalapproximera genom att kolla om \\(n\\cdot p_0 \\cdot (1-p_0) &gt; 10\\).\nTeststorheten är \\[z = \\left| \\frac{\\hat{p}-p_0}{\\sqrt{\\frac{p_0(1-p_0)}{n}}}\\right|\\]\n\nLägg märke till att denna gång specificerar vi medelfelet med \\(p_0\\) och inte med \\(\\hat{p}\\).\n\nVi förkastar \\(H_0\\) om \\(z &gt; \\lambda_{\\alpha/2}\\).\n\n\n\n\n\n\nExempel. Pollenallergi\n\n\n\nI Stockholms län gjorde man 1990 en undersökning av förekomsten av pollenallergi bland vissa känsliga grupper. man valde slumpmässigt ut 500 personer i åldern 20-64 år och av dessa hade 23 % pollenallergi.\n\nVad kan vi säga om andelen pollenallergiker i populationen?\n\n\n\n\\(X = \\text{\"antal pollenallergiker\"} \\sim Bin(500,p)\\)\nKan vi normalapproximera? Ja, eftersom \\(500\\cdot 0.23 \\cdot (1-0.23) = 88.55 &gt; 10\\)\nVi skapar ett 95%-igt konfidensintervall för proportionen allergiker till:\n\\[I_p: 0.23 \\pm 1.96\\sqrt{\\frac{0.23(1-0.23)}{500}} = (0.19, 0.27)\\]"
  },
  {
    "objectID": "F9_anteckningar.html#två-proportioner",
    "href": "F9_anteckningar.html#två-proportioner",
    "title": "F9. Statistisk inferens på diskreta och kategoriska data",
    "section": "Två proportioner",
    "text": "Två proportioner\nModell:\n\\(X = \\text{\"antal pollenallergiker 1990\"} \\sim Bin(500,p_x)\\)\n\\(Y = \\text{\"antal pollenallergiker 1994\"} \\sim Bin(500,p_y)\\)\nHypoteser:\n\\(H_0: p_x = p_y\\)\n\\(H_1: p_x \\neq p_y\\)\nDetta är samma sak som\n\\(H_0: p_x - p_y = 0\\)\n\\(H_1: p_x - p_y \\neq 0\\)"
  },
  {
    "objectID": "F9_anteckningar.html#test-med-ett-konfidensintervall-för-skillnad-i-proportioner",
    "href": "F9_anteckningar.html#test-med-ett-konfidensintervall-för-skillnad-i-proportioner",
    "title": "F9. Statistisk inferens på diskreta och kategoriska data",
    "section": "Test med ett konfidensintervall för skillnad i proportioner",
    "text": "Test med ett konfidensintervall för skillnad i proportioner\nKan vi normalapproximera? Vi har redan visat det för \\(X\\). Vi kan normalapproximera \\(Y\\) eftersom \\(500\\cdot 0.29 \\cdot (1-0.29) = 102.95 &gt; 10\\)\n\\[I_{p_x-p_y}: \\hat{p}_x-\\hat{p}_y \\pm \\underbrace{\\lambda_{\\alpha/2}}_{\\alpha=0.05}\\sqrt{\\frac{\\hat{p}_x(1-\\hat{p}_x)}{500}+\\frac{\\hat{p}_y(1-\\hat{p}_y)}{500} } = (-0.114,-0.006)\\]\n\\(H_0\\) förkastas på signifikansnivå 0.05 eftersom intervallet inte täcker noll."
  },
  {
    "objectID": "F9_anteckningar.html#test-med-teststorhet-och-kritiskt-område",
    "href": "F9_anteckningar.html#test-med-teststorhet-och-kritiskt-område",
    "title": "F9. Statistisk inferens på diskreta och kategoriska data",
    "section": "Test med teststorhet och kritiskt område",
    "text": "Test med teststorhet och kritiskt område\nNär \\(H_0\\) är sann gäller att \\(p_x = p_y = p_0\\)\nVi skattar \\(p_0\\) genom att kombinera information från 1990 och 1994 (poolad skattning)\n\\[\\hat{p}_0 = \\frac{x + y}{n_x + n_y} = \\frac{0.23\\cdot 500 + 0.29\\cdot 500}{500 + 500} = 0.26\\]\nKan vi normalapproximera? Ja, eftersom \\(500\\cdot \\hat{p}_0 \\cdot (1-\\hat{p}_0) = 96.2 &gt; 10\\)\nTeststorhet\n\\[z = \\left| \\frac{\\hat{p}_x - \\hat{p}_y - 0}{\\sqrt{\\hat{p}_0 (1-\\hat{p}_0)(\\frac{1}{n_x}+\\frac{1}{n_y})}} \\right| = 2.1628\\]\nFörkasta \\(H_0\\) på signifikansnivå 0.05 eftersom \\(z &gt; \\lambda_{\\alpha/2} = 1.96\\).\n⇒ Det har skett en förändring i andelen allergiker i populationen under perioden 1990 till 1994."
  },
  {
    "objectID": "F9_anteckningar.html#kategoriska-data",
    "href": "F9_anteckningar.html#kategoriska-data",
    "title": "F9. Statistisk inferens på diskreta och kategoriska data",
    "section": "Kategoriska data",
    "text": "Kategoriska data\nEtt slumpförsök kan utfalla på \\(k\\) olika sätt. Gör \\(n\\) oberoende försök, och räkna hur många försök som hamnar i varje kategori.\n\n\n\n\n\n\nExempel. Genetik\n\n\n\nVarje individ i en viss population hör i genetiskt hänseende till en av fyra kategorier \\(K_1\\),\\(K_2\\),\\(K_3\\) och \\(K_4\\). I en studie undersöktes 160 slumpmässigt utvalda individer och placerades in i lämplig kategori. Resultatet blev:\n\n\n\nkategori\n\\(K_1\\)\n\\(K_2\\)\n\\(K_3\\)\n\\(K_4\\)\n\n\nfrekvens\n78\n42\n27\n13"
  },
  {
    "objectID": "F9_anteckningar.html#tre-situtationer-med-kategoriska-data",
    "href": "F9_anteckningar.html#tre-situtationer-med-kategoriska-data",
    "title": "F9. Statistisk inferens på diskreta och kategoriska data",
    "section": "Tre situtationer med kategoriska data",
    "text": "Tre situtationer med kategoriska data\nVi kommer gå igenom tre vanliga situationer för hypotestest med kategoriska data med ett \\(\\chi^2\\)-test.\n\nTesta modellanpassning\nKorstabellsanalys för homogenitetstest\nKorstabellsanalys för oberoendetest"
  },
  {
    "objectID": "F9_anteckningar.html#test-av-modellanpassning",
    "href": "F9_anteckningar.html#test-av-modellanpassning",
    "title": "F9. Statistisk inferens på diskreta och kategoriska data",
    "section": "Test av modellanpassning",
    "text": "Test av modellanpassning\n\n\n\n\n\n\nExempel. Genetik (forts.)\n\n\n\nVarje individ i en viss population hör i genetiskt hänseende till en av fyra kategorier \\(K_1\\),\\(K_2\\),\\(K_3\\) och \\(K_4\\). I en studie undersöktes 160 slumpmässigt utvalda individer och placerades in i lämplig kategori. Resultatet blev:\n\nObservations-tabell\n\n\nkategori\n\\(K_1\\)\n\\(K_2\\)\n\\(K_3\\)\n\\(K_4\\)\n\n\nfrekvens\n78\n42\n27\n13\n\n\n\n\nTeoretiskt sätt ska de fyra kategoriernas storlekar förhålla sig som 9:3:3:1. Talar de observerade data emot teorin?\n\n\n\nLåt \\(O_i\\) vara observationer av kategori \\(i=1,\\dots,r\\). Totalt sett har vi \\(n\\) observationer.\nHypoteser:\n\\(H_0: p_1=\\frac{9}{16}, p_2=\\frac{3}{16}, p_3=\\frac{3}{16}, p_4=\\frac{1}{16}\\) (den teoretiska modellen)\n\\(H_1: \\text{några av dessa sannolikheter är fel}\\) (den teoretiska modellen stämmer inte)\nTestregel:\nLåt \\(E_i=n\\cdot p_i\\) vara förväntat antal utfall för kategori \\(i\\) när \\(H_0\\) är sann.\n\nE-tabell\n\n\nkategori\n\\(K_1\\)\n\\(K_2\\)\n\\(K_3\\)\n\\(K_4\\)\n\n\nfrekvens\n\\(160\\frac{9}{16}=90\\)\n30\n30\n10\n\n\n\nVi skapar en teststorhet som när \\(H_0\\) är sann blir\n\\[\\chi^2=\\sum_{i=1}^r \\frac{(O_i-E_i)^2}{E_i} \\sim \\chi^2(r-1)\\]\nFörkasta \\(H_0\\) om teststorheten \\(\\chi^2 &gt; \\chi^2_{\\alpha}\\)\nI exemplet blir teststorheten \\(\\chi^2 = \\sum_{i=1}^r \\frac{(O_i-E_i)^2}{E_i} = \\frac{(78-90)^2}{90} +\\frac{(42-30)^2}{30}\\)+\\(\\frac{(42-30)^2}{30}\\)+\\(\\frac{(13-10)^2}{10} = 7.6\\)\nEn kvantil ur en \\(\\chi^2\\)-fördelning med 4-1 frihetgrader är 7.8147279\n⇒ \\(H_0\\) kan ej förkastas på signfikansnivån 0.05"
  },
  {
    "objectID": "F9_anteckningar.html#korstabellsanalys-test-av-oberoende",
    "href": "F9_anteckningar.html#korstabellsanalys-test-av-oberoende",
    "title": "F9. Statistisk inferens på diskreta och kategoriska data",
    "section": "Korstabellsanalys (test av oberoende)",
    "text": "Korstabellsanalys (test av oberoende)\n\n\n\n\n\n\nExempel. Magsår och blodgrupp\n\n\n\nFinns det ett samband mellan blodgrupp och risken för magsår? Blodgruppen bestämdes för 1655 magsårspatienter och för en kontrollgrupp om 10 000 personer från samma stad. Resultat:\n\nO-tabell\n\n\n\n\n\n\n\n\n\n\n\n0\nA\nB\nAB\nTotalt\n\n\n\n\nMagsårspatienter\n911\n59\n124\n41\n\\(n_{1.}\\)=1655\n\n\nKontrollgrupp\n4578\n4219\n890\n313\n\\(n_{2.}\\)=10000\n\n\nTotalt\n\\(n_{.1}\\)=5489\n\\(n_{.2}\\)=4798\n\\(n_{.3}\\)=1014\n\\(n_{.4}\\)=354\n\\(n\\) = 11655\n\n\n\n\n\nLåt \\(p_{i.}\\) och \\(p_{.j}\\) vara sannolikheter att en observation tillhör kategorin på rad \\(i\\) respektive kolumn \\(j\\).\nVi kan skatta \\(p_{i.}\\) med \\(n_{i.}\\), antalet observationer i rad \\(i\\), genom det totala antalet observationer \\(n\\):\n\\[p_{i.} = \\frac{n_{i.}}{n}\\]\nHypoteser:\n\\(H_0: p_{ij} = p_{i.}p_{.j}\\) för alla \\(i\\) och \\(j\\) (blodgrupp och magsår är oberoende)\n\\(H_1: H_0 \\text{ stämmer inte}\\)\nTestregel: När \\(H_0\\) är sann är det förväntade antalet observationer i varje kombination av kategorer \\(E_{ij}=np_{ij}=np_{i.}p_{.j}\\) vilket när vi sätter in skattningar av sannolikheterna blir \\(n\\frac{n_{i.}}{n}\\frac{n_{.j}}{n} = \\frac{n_{i.}n_{.j}}{n}\\)\nT.ex. \\(E_{11}=\\frac{n_{1.}n_{.1}}{n} = \\frac{1655 \\cdot 5489}{11655} = 779.4332904\\)\n\nE-tabell\n\n\n\n0\nA\nB\nAB\n\n\n\n\nMagsårspatienter\n779.4\n681.3\n144.0\n50.3\n\n\nKontrollgrupp\n4709.6\n4116.7\n870.0\n303.7\n\n\n\nUnder \\(H_0\\) är samplingfördelningen för teststorheten\n\\[\\chi^2=\\sum_{i=1}^r \\sum_{j=1}^c \\frac{(O_{ij}-E_{ij})^2}{E_{ij}} \\sim \\chi^2((r-1)(c-1))\\] där \\(r\\) är antal kategorier fördelat på raderna och \\(c\\) är antal kategorier fördelat på kolumnerna.\n\\(\\chi^2=\\frac{(911-779.4)^2}{779.4} + \\frac{(579-681.3)^2}{681.3} \\dots = 49.0153\\)\nVi förkastar \\(H_0\\) eftersom teststorheten är större än kvantilen \\(\\chi^2_{\\alpha}((2-1)(4-1)) = 7.8147279\\)\n\nOberoende eller homogenitetstest\nVi kallar korstabellstestet för ett\n\noberoendetest om vi vill visa att förekomsten av de två kategorierna är oberoende av varandra\nhomogenitetstest om vill visa att två stickprov kommer från samma fördelning, de är homogena."
  },
  {
    "objectID": "F9_anteckningar.html#korstabellsanalys-homogenitetstest",
    "href": "F9_anteckningar.html#korstabellsanalys-homogenitetstest",
    "title": "F9. Statistisk inferens på diskreta och kategoriska data",
    "section": "Korstabellsanalys (Homogenitetstest)",
    "text": "Korstabellsanalys (Homogenitetstest)\nVi har \\(c\\) kategorier och \\(r\\) stickprov.\n\nO-tabell\n\n\n\n\nkategorier\n\n\n\n\n\n\n\nstickprov\n1\n2\n\\(\\dots\\)\nc\nsumma\n\n\n1\n\\(O_{11}\\)\n\\(O_{12}\\)\n\n\\(O_{1c}\\)\n\\(n_{1.}\\)\n\n\n2\n\\(O_{21}\\)\n\n\n\n\n\n\n\\(\\vdots\\)\n\n\n\n\n\n\n\nr\n\\(O_{r1}\\)\n\n\n\\(O_{rc}\\)\n\\(n_{r.}\\)\n\n\nsumma\n\\(n_{.1}\\)\n\n\n\\(n_{.c}\\)\n\\(n\\)\n\n\n\nNollhypotesen när vi gör ett homogeneitetstest är att sannolikheten att hamna i en kategori ska vara densamma oavsett vilket stickprov man tillhör.\n\\(H_0: p_{1j} = p_{2j} = \\dots = p_{rj}\\text{ för alla kategorier }j\\)\n\\(H_1: H_0 \\text{ stämmer inte}\\)\nSkattning av sannolikheter, teststorhet och testregel är densamma som för oberoendetest."
  },
  {
    "objectID": "F8_anteckningar.html",
    "href": "F8_anteckningar.html",
    "title": "F8. Två stickprov",
    "section": "",
    "text": "Båda sätten att beskriva en fördelning för en slumpvariabel förekommer:\n\\[X \\sim N(\\mu,\\sigma)\\]\n\\[X \\in N(\\mu,\\sigma)\\]\nUllrika använder \\(\\sim\\) med tolkning “fördelad som” och \\(\\in\\) med tolkning “som en delmängd i”.\nDet är ok att använda \\(X \\in N(\\mu,\\sigma)\\)."
  },
  {
    "objectID": "F8_anteckningar.html#chi2-fördelningen",
    "href": "F8_anteckningar.html#chi2-fördelningen",
    "title": "F8. Två stickprov",
    "section": "\\(\\chi^2\\)-fördelningen",
    "text": "\\(\\chi^2\\)-fördelningen\nEn summa av kvadraten av \\(k\\) oberoende standardiserade normalfördelade slumpvariabler följer en \\(\\chi^2\\)-fördelningen med \\(k\\) frihetsgrader\nOm \\(Z_i\\sim N(0,1)\\) och alla \\(i=1,\\dots,k\\) är oberoende så gäller att\n\\[\\sum_{i=1}^k Z_i^2\\sim \\chi^2(k)\\]"
  },
  {
    "objectID": "F8_anteckningar.html#simulering-av-chi2-fördelningen-från-n01",
    "href": "F8_anteckningar.html#simulering-av-chi2-fördelningen-från-n01",
    "title": "F8. Två stickprov",
    "section": "Simulering av \\(\\chi^2\\)-fördelningen från N(0,1)",
    "text": "Simulering av \\(\\chi^2\\)-fördelningen från N(0,1)\n\n# simulering av en chi2 med 1 frihetsgrad\nhist((rnorm(10^4))^2,probability=TRUE,main=latex2exp::TeX(\"$\\\\chi^2(1)$\"))\nxx = seq(0,25,by = 0.1)\nlines(xx,dchisq(xx,1),col='darkred')\n\n\n\n\n\n\n\n\n\n# simulering av en chi2 med fyra frihetsgrader\nhist((rnorm(10^4))^2+(rnorm(10^4))^2+(rnorm(10^4))^2+(rnorm(10^4))^2,probability=TRUE,ylim = c(0,0.2),main=latex2exp::TeX(\"$\\\\chi^2(4)$\"))\nxx = seq(0,25,by = 0.1)\nlines(xx,dchisq(xx,4),col='darkred')"
  },
  {
    "objectID": "F8_anteckningar.html#samplingsfördelning-för-stickprovsvariansen-1",
    "href": "F8_anteckningar.html#samplingsfördelning-för-stickprovsvariansen-1",
    "title": "F8. Två stickprov",
    "section": "Samplingsfördelning för stickprovsvariansen",
    "text": "Samplingsfördelning för stickprovsvariansen\nFöreställ dig att vi upprepande gör nya observationer av en normalfördelad slumpvariabel \\(X \\sim N(\\mu,\\sigma)\\) och beräknar stickprovsvariansen. På samma sätt som vi då kan betrakta stickprovsmedelvärdet \\(\\bar{x}\\) som en slumpvariabel, kan vi även göra med stickprovsvariansen \\(s_2\\)\n\\[s^2 = \\frac{\\sum_{i=1}^n (X_i-\\bar{x})^2}{n-1}\\]\nVi skriver om så det blir en summa \\((n-1)s^2\\). Om vi sen delar med det sanna värdet på variansen får vi kvoten\n\\[\\frac{(n-1)s^2}{\\sigma^2} = \\frac{\\sum_{i=1}^n (X_i-\\bar{x})^2}{\\sigma^2}\\]\nDenna kvot går sen att skriva om som en summa av kvadratiska slumpvariabler. Eftersom vi drar ifrån skattning av väntevärdet och delar med variansen, kan man visa att det är en summa av kvadrerade standardiserade slumpvariabler, dock endast n-1 stycken oberoende. Vi går inte igenom det i denna kursen, men man kan visa att kvoten följer en \\(\\chi^2\\)-fördelning:\n\\[\\frac{(n-1)s^2}{\\sigma^2} \\sim \\chi^2(n-1)\\]"
  },
  {
    "objectID": "F8_anteckningar.html#simulering-av-samplingsfördelning-för-stickprovsvarians",
    "href": "F8_anteckningar.html#simulering-av-samplingsfördelning-för-stickprovsvarians",
    "title": "F8. Två stickprov",
    "section": "Simulering av samplingsfördelning för stickprovsvarians",
    "text": "Simulering av samplingsfördelning för stickprovsvarians\n\n## simulering av samplingsfördelning för stickprovsvarians\n\nmu = 3.3 #välj vad ni vill\nsigma2 = 20 #välj vad ni vill\nn = 10 #antal värden i stickprovet\nniter = 1e4 #antal \"gånger\" vi samlar in nya data\nsam &lt;- replicate(niter,\n  {\n  x &lt;- rnorm(n,mu,sqrt(sigma2))\n  s2 &lt;- var(x)\n  s2*(n-1)/sigma2\n  }\n)\n\nhist(sam,prob = TRUE,main=latex2exp::TeX(\"$\\\\chi\\\\^2(n-1)\"),xlab = latex2exp::TeX(\"(n-1)s\\\\^2/\\\\sigma\\\\^2\"))\npp &lt;- ppoints(200)\nxx &lt;- qchisq(pp,n-1)\nyy &lt;- dchisq(xx,n-1)\nlines(xx,yy,col='darkred')\n\n\n\n\n\n\n\n\nNotera att kvoten mellan stickprovsvariansen och den verkliga variansen kan skrivas om till att vara en \\(\\chi^2\\)-fördelad slumpvariabel genom \\(n-1\\)\n\\[\\frac{s^2}{\\sigma^2}=\\frac{(n-1)s^2}{(n-1)\\sigma^2}=\\frac{\\frac{(n-1)s^2}{\\sigma^2}}{(n-1)} \\tag{1}\\]"
  },
  {
    "objectID": "F8_anteckningar.html#två-oberoende-stickprov",
    "href": "F8_anteckningar.html#två-oberoende-stickprov",
    "title": "F8. Två stickprov",
    "section": "Två oberoende stickprov",
    "text": "Två oberoende stickprov\n\\(X_i = \\text{\"syresättning hos patient i som får medicinen\"}\\)\n\\(Y_j = \\text{\"syresättning hos patient j som får placebo\"}\\)\nStrategi: Studien bör utföras så man kan anta att \\(X_i\\), \\(i=1,\\dots,n_x\\) och \\(Y_j\\), \\(j=1,\\dots,n_y\\) är oberoende, t.ex. genom att slumpmässigt placera in patienter i de två grupperna (randomisera)."
  },
  {
    "objectID": "F8_anteckningar.html#stickprov-i-par",
    "href": "F8_anteckningar.html#stickprov-i-par",
    "title": "F8. Två stickprov",
    "section": "Stickprov i par",
    "text": "Stickprov i par\n\\(X_i = \\text{\"syresättning hos patient i före behandling\"}\\)\n\\(Y_i = \\text{\"syresättning hos patient i efter behandling\"}\\)\nStrategi: Bilda en ny slumpvariabel \\(\\Delta_i = X_i - Y_i\\) och gör statistisk analys på den.\n\n\n\n\n\n\nExempel: Ökar alkoholkonsumtion fetthalten i levern?\n\n\n\nMan valde ut 12 försökspersoner, vilka kan betraktas som ett slumpmässigt urval bland friska personer i 25-årsåldern. Försökspersonerna har under en längre tid avstått från all alkoholkonsumtion och prover på deras levrar har tagits. Därefter har de fått dricka 4 burkar öl per dag och efter en månad har nya leverprover tagits. Följande leverfetter erhölls:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPerson nr\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\nFöre\n0.25\n0.19\n0.13\n0.23\n0.15\n0.14\n0.24\n0.23\n0.17\n0.15\n0.10\n0.17\n\n\nEfter\n0.50\n0.28\n0.18\n0.18\n0.34\n0.41\n0.33\n0.26\n0.35\n0.42\n0.22\n0.29\n\n\nDifferens Efter-Före\n0.25\n0.09\n0.05\n-0.05\n0.19\n0.27\n0.09\n0.03\n0.18\n0.27\n0.12\n0.12\n\n\n\n\n\nModell: \\(X=\\text{\"Förändring fetthalt\"} \\sim N(\\mu,\\sigma)\\)\nDetta ser ut att vara en rimlig modell när vi jämför teoretiska med empiriska kvantiler:\n\ndiff &lt;- c(0.25, 0.09, 0.05, -0.05, 0.19, 0.27, 0.09, 0.03, 0.18, 0.27, 0.12, 0.12)\nqqnorm(diff)\nqqline(diff)\n\n\n\n\n\n\n\n\nSätt upp lämpliga hypoteser och testa om dessa data stöder misstanken att alkoholkonsumtionen ökar fetthalten i levern.\nHypoteser:\n\\(H_0: \\mu = 0\\) (Alkohol påverkar ej fetthalt i levern)\n\\(H_1: \\mu &gt; 0\\) (Alkohol ökar fetthalt i levern)\n\nTeststorhet med kritiskt område\n\nOm \\(\\sigma\\) är känd, bildar vi \\(z = \\frac{\\bar{x}-0}{\\sigma/\\sqrt{12}}\\)\n\nFörkasta \\(H_0\\) på signifikansnivå \\(\\alpha = 0.05\\) om \\(z &gt; \\lambda_{\\alpha}\\)\n\nOm \\(\\sigma\\) är okänd, bildar vi \\(t = \\frac{\\bar{x}-0}{s/\\sqrt{12}}\\)\n\nFörkasta \\(H_0\\) på signifikansnivå \\(\\alpha = 0.05\\) om \\(t &gt; t_{\\alpha}(n-1)\\)\nI detta exempel är standardavvikelsen okänd. Vi förkastar \\(H_0\\) eftersom \\(t = \\frac{ 0.1342-0}{0.1008/\\sqrt{12}} = 4.61 &gt; 1.8\\)\n\n\nTest med konfidensintervall\nVi skapar ett nedåt begränsat konfidensintervall för väntevärdet.\n\\(\\begin{split} I_\\mu: & (\\bar{x} - t_{\\alpha}(n-1)\\frac{s}{\\sqrt{n}},\\infty) = \\\\ & (0.1342 - 1.796 \\frac{0.1008}{\\sqrt{12}}, \\infty) = (0.082,\\infty) \\end{split}\\)\nVi förkastar \\(H_0\\) på signifikansnivå \\(\\alpha = 0.05\\) eftersom det 95%-iga konfidensintervallet inte täcker 0.\n\n\nTest med direktmetoden\nOm \\(H_0\\) är sann gäller att \\(t=\\frac{\\bar{x}-0}{s/\\sqrt{n}} \\sim t(n-1)\\) (t-fördelad).\n\\(P(\\text{\"vad vi fick eller värre\"}|H_0) = P(t &gt; 4.61) =\\) \\(1 - P(t \\leq 4.61) = 1 - 0.9996 = 4\\times 10^{-4}\\)\nVi förkastar \\(H_0\\) på signifikansnivå \\(\\alpha = 0.05\\) eftersom \\(p\\)-värdet \\(&lt; \\alpha\\)."
  },
  {
    "objectID": "F8_anteckningar.html#varför-använda-p-värde",
    "href": "F8_anteckningar.html#varför-använda-p-värde",
    "title": "F8. Två stickprov",
    "section": "Varför använda p-värde",
    "text": "Varför använda p-värde\n\np-värde används ofta i utskrifter från statistiska program\ndet är smidigt att använda i situationer då man inte kan normalapproximera\n\n\n\n\n\n\n\nExempel. Sjukdomsfall\n\n\n\nI ett område, beläget nära ett raffinaderi, inträffade under en 5-årsperiod 9 fall av leukemi mot “förväntade” 4.3 fall. Är området mer drabbat än resten av landet? För en “ovanlig” sjukdom kan ofta variationen i antalet sjukdomsfall beskrivas med en poissonfördelning.\nModell: \\(X = \\text{\"antal leukemifall i området\"} \\sim Po(\\lambda)\\)\nHypoteser:\n\\(H_0: \\lambda = 4.3\\)\n\\(H_1: \\lambda &gt; 4.3\\)\nOm \\(H_0\\) är sann gäller att \\(X\\sim Po(4.3)\\)\nVi observerade \\(x = 9\\) fall. Hur troligt är det under \\(H_0\\)-fördelningen?\n\n\n\n\n\n\n\n\n\n\n\n\nAnvändning av p-värde när man kan normalapproximera\nTesta hypoteserna med en signifikansnivå på \\(\\alpha=0.05\\).\n\\(\\begin{split} p\\text{-värdet} & = P(\\text{\"det vi fick eller värre\"}) = P(X\\geq 9) = \\\\& 1-P(X\\leq 8) = 0.032 \\end{split}\\)\nVi kan förkasta \\(H_0\\) eftersom \\(p\\)-värdet &lt; 0.05.\nOm vi istället hade valt en signifikansnivå på \\(\\alpha = 0.01\\), så hade vi inte kunnat förkasta \\(H_0\\) eftersom \\(p\\)-värdet &gt; 0.01.\n\n\n\n\n\n\nWarning\n\n\n\n\np-värdet är inte sannolikheten att nollhypotesen är sann\nman måste välja signifikansnivå även om man använder direktmetoden\np-värdet säger inget om hur stor skillnaden är\n\n\n\n\n\nHur det kunde sett ut om vi kunde normalapproximera\nMan kan normalapproximera en Poissonfördelning när väntevärdet \\(\\lambda &gt; 15\\). Vad händer om vi istället hade haft en mer vanligt förekommande sjukdom?\nModell: \\(Y = \\text{\"antal influensafall i området\"} \\sim Po(\\lambda)\\)\nHypoteser:\n\\(H_0: \\lambda = 16\\)\n\\(H_1: \\lambda &gt; 16\\)\nOm \\(H_0\\) är sann gäller att \\(X\\sim Po(16)\\)\nVi observerade \\(x = 30\\) fall. Hur troligt är det under \\(H_0\\)-fördelningen?\nTesta hypoteserna med en signifikansnivå på \\(\\alpha=0.05\\).\n\\(\\begin{split} p\\text{-värdet} & = P(\\text{\"det vi fick eller värre\"}) = P(Y\\geq 30) = \\\\& 1-P(Y\\leq 29) = 1 - P(\\frac{Y-\\lambda}{\\sqrt{\\lambda}} \\leq \\frac{29-16}{4}) = \\\\ & 1 - P(Z \\leq 3.25) \\overset{A} = 1 -\\Phi(3.25) = 0.001 \\end{split}\\)\nVi kan förkasta \\(H_0\\) eftersom \\(p\\)-värdet &lt; 0.05."
  },
  {
    "objectID": "F8_anteckningar.html#testa-differens-i-väntevärden",
    "href": "F8_anteckningar.html#testa-differens-i-väntevärden",
    "title": "F8. Två stickprov",
    "section": "Testa differens i väntevärden",
    "text": "Testa differens i väntevärden\nModell:\n\\(X = \\text{\"levervärde utan medicin\"} \\sim N(\\mu_x,\\sigma_x)\\)\n\\(Y = \\text{\"levervärde med medicin\"} \\sim N(\\mu_y,\\sigma_y)\\)\nVi antar att varianserna är lika \\(\\sigma^2_x=\\sigma^2_y=\\sigma^2\\)\nHypoteser: Vi vill testa\n\\(H_0: \\mu_x = \\mu_y\\)\n\\(H_1: \\mu_x \\neq \\mu_y\\)\nVilket är samma sak som\n\\(H_0: \\mu_x - \\mu_y = 0\\)\n\\(H_1: \\mu_x - \\mu_y \\neq 0\\)\nSkattning: Vi skattar väntevärdet med stickprovsmedelvärden \\(\\hat{\\mu}_x = \\bar{x}= 148.2\\) och \\(\\hat{\\mu}_y = \\bar{y}= 151.7\\)\nTestregel: Vi testar hypoteserna på en signifikansnivå \\(\\alpha\\) genom att skapa ett \\((1-\\alpha)\\cdot 100\\)%-igt konfidensintervall för differensen i väntevärden.\nKonfidensintervallet vi skapar ska ha formen skattning plus/minus en lämplig kvantil multiplicerat med standardavvikelsen på skattningen beräknad från dessa samplingsfördelning.\n\\[I_{\\mu_x - \\mu_y} = \\hat{\\mu}_x-\\hat{\\mu}_y \\pm \\text{kvantil}\\cdot\\sqrt{V(\\hat{\\mu}_x-\\hat{\\mu}_y)}\\]"
  },
  {
    "objectID": "F8_anteckningar.html#variansen-för-skattning-av-differens-i-väntevärden",
    "href": "F8_anteckningar.html#variansen-för-skattning-av-differens-i-väntevärden",
    "title": "F8. Två stickprov",
    "section": "Variansen för skattning av differens i väntevärden",
    "text": "Variansen för skattning av differens i väntevärden\n\\(\\begin{split} & V(\\hat{\\mu}_x-\\hat{\\mu}_y) = V(\\bar{x}-\\bar{y}) \\underset{antar \\\\ oberoende} = V(\\bar{x})+V(\\bar{y}) = \\\\ & \\frac{\\sigma_x^2}{n_x} + \\frac{\\sigma_y^2}{n_y} \\underset{lika \\\\ varians} = \\frac{\\sigma^2}{n_x} + \\frac{\\sigma^2}{n_y} = \\sigma^2 (\\frac{1}{n_x} + \\frac{1}{n_y}) \\end{split}\\)\n\nHur skattar vi variansen med två stickprov? Jo, genom att kombinera ihop (Engelska pool) de två stickprovens kvadratiska avvikelser till respektive medelvärde.\n\n\\[s^2_{pooled}=\\frac{\\sum (x_i-\\bar{x})^2 + \\sum (y_i-\\bar{y})^2}{n_x+n_y-2} = \\frac{(n_x-1)s_x^2 + (n_y-1)s_y^2}{n_x+n_y-2}\\]\ndär \\(s_x^2 = \\frac{\\sum (x_i-\\bar{x})^2}{n-1}\\) och motsvarande för \\(s_y^2\\) kan fås genom att köra en rutin på miniräknare eller program.\n\\(s^2_x = 10.0^2\\), \\(s^2_y = 8.0^2\\)\n\\(n_x = 50\\), \\(n_y = 25\\)\n\\(s^2_{pooled} = \\frac{49\\cdot 10.0^2 + 24\\cdot 8.0^2}{50+25-2} = 88.2\\)"
  },
  {
    "objectID": "F8_anteckningar.html#samplingsfördelning-för-skattning-av-differens-i-väntevärden",
    "href": "F8_anteckningar.html#samplingsfördelning-för-skattning-av-differens-i-väntevärden",
    "title": "F8. Två stickprov",
    "section": "Samplingsfördelning för skattning av differens i väntevärden",
    "text": "Samplingsfördelning för skattning av differens i väntevärden\nEftersom både \\(X\\) och \\(Y\\) är normalfördelade kommer samplingsfördelningen för stickprovsmedelvärdena \\(\\bar{x}\\) och \\(\\bar{y}\\) också vara det. Likaså differensen mellan dessa, d.v.s.\n\\[\\bar{x} - \\bar{y} \\sim N(\\mu_x - \\mu_y, \\sqrt{\\sigma^2 (\\frac{1}{n_x} + \\frac{1}{n_y})})\\]"
  },
  {
    "objectID": "F8_anteckningar.html#konfidensintervall-för-differens-i-väntevärden",
    "href": "F8_anteckningar.html#konfidensintervall-för-differens-i-väntevärden",
    "title": "F8. Två stickprov",
    "section": "Konfidensintervall för differens i väntevärden",
    "text": "Konfidensintervall för differens i väntevärden\nEftersom variansen \\(\\sigma^2\\) är okänd och skattas med stickproven, kommer vi använda en t-fördelning med \\(n_x+n_y-2\\) frihetsgrader när vi bygger konfidensintervallet.\n\\[I_{\\mu_x - \\mu_y} = \\bar{x}-\\bar{y} \\pm t_{\\alpha/2}(n_x+n_y-2)\\cdot\\sqrt{s^2_{pooled}(\\frac{1}{n_x} + \\frac{1}{n_y})}\\]\n\n\n\n\n\n\nExempel. Jämförelse mellan behandlingar (forts.)\n\n\n\n\\[\\begin{split} I_{\\mu_x - \\mu_y} & = 148.2-151.7 \\pm \\underbrace{t_{0.05/2}(50+25-2)}_{1.99}\\sqrt{88.2(\\frac{1}{50} + \\frac{1}{25})}   = \\\\ & (-8.08,1.08) \\end{split}\\]\nVi kan inte förkasta \\(H_0\\) på signifikansnivå 5% eftersom det 95%-iga konfidensintervallet täcker noll."
  },
  {
    "objectID": "F8_anteckningar.html#samplingsfördelning-för-kvoten-mellan-två-stickprovsvarianser",
    "href": "F8_anteckningar.html#samplingsfördelning-för-kvoten-mellan-två-stickprovsvarianser",
    "title": "F8. Två stickprov",
    "section": "Samplingsfördelning för kvoten mellan två stickprovsvarianser",
    "text": "Samplingsfördelning för kvoten mellan två stickprovsvarianser\n\nF-fördelning\nEn F-fördelning är, liksom t-fördelningen, en konstruerad samplingsfördelning (av Fisher), men som visar sig vara användbar i flera sammanhang. Den är fördelningen för kvoten av två \\(\\chi^2\\)-fördelade slumpvariabler.\n\\[\\frac{\\chi_1^2/\\nu_1}{\\chi_2^2/\\nu_2} \\sim F(\\nu_1,\\nu_2)\\]\ndär \\(\\nu_1\\) och \\(\\nu_2\\) är F-fördelningens frihetsgrader.\n\n\nKvoten mellan två stickprovsvarianser\nLåt oss bilda följande kvot och använda det vi visade i Equation 1\n\\[\\frac{s^2_x/\\sigma_x^2}{s^2_y/\\sigma_y^2}=\\frac{\\frac{(n_x-1)s^2_x/\\sigma_x^2}{n_x-1}}{\\frac{(n_y-1)s^2_y/\\sigma_y^2}{n_y-1}} \\sim F(n_x-1,n_y-1)\\]\nOm populationerna \\(X\\) och \\(Y\\) är normalfördelade, eller \\(n_x\\) och \\(n_y\\) är tillräckligt höga för att tillämpa centrala gränsvärdessatsen, kommer både täljaren och nämnaren att vara kvoter av \\(\\chi^2\\)-fördelade slumpvariabler och respektive frihetsgrad. Det innebär att kvoten följer en F-fördelning med \\(n_x-1\\) och \\(n_y-1\\) frihetsgrader.\nMen vi känner inte till varianserna!\n\n\nKvoten under \\(H_0\\)\nUnder \\(H_0\\) är varianserna lika, och då kan vi förenkla kvoten eftersom varianserna tar ut varandra. F-fördelningen är då samplingsfördelningen för kvoten \\(F\\) av stickprovsvarianser om nollhypotesen är sann.\n\\[F = \\frac{s^2_x/\\sigma^2}{s^2_y/\\sigma^2} = \\frac{s^2_x}{s^2_y} \\sim F(n_x-1,n_y-1)\\]\n\n\nTestregel\nVi förkastar \\(H_0\\) med vald signifikansnivå \\(\\alpha\\) genom att jämföra teststorheten \\(F\\) med en kvantil ur F-fördelningen:\n\\[F = \\frac{s^2_x}{s^2_y} &gt; F_{\\alpha}(n_x-1,n_y-1)\\]\ndär kvantilen definieras utifrån sannolikhetsarean ovanför kvantilen.\n\n\n\n\n\n\n\nJämförelse mellan behandlingar (forts.)\n\n\n\nLåt oss nu testa på en signifikansnivå \\(\\alpha=0.05\\) om antagandet om lika varians stämmer.\n\\(F = \\frac{s^2_x}{s^2_y} = \\frac{10.0^2}{8.0^2} = 1.5625\\)\n\\(H_0\\) förkastas ej eftersom teststorheten ligger i samplingsfördelningen under \\(H_0\\), d.v.s. \\(F &lt; F_{0.025}(n_x-1,n_y-1) = 2.11\\)\n\nOm du inte kan räkna ut exakt eller frihetsgraderna du har saknas i tabellen, läs av i tabellen för de frihetsgrader som ligger närmast. I detta fall \\(f_1 = 50\\) och \\(f_2=24\\) för \\(\\alpha=0.05\\)\n\n\n\nTips - F-fördelningen är inte symmetrisk. Du kan ändå jämföra mot en kvantil även om du har en tvåsidig hypotes, genom att se till att den stösta stickprovsvariansen är i täljaren och att du har rätt ordning på frihetsgraderna"
  },
  {
    "objectID": "F7_anteckningar.html",
    "href": "F7_anteckningar.html",
    "title": "F7. Hypotestest",
    "section": "",
    "text": "Exempel. Rattonykterhet\n\n\n\nGräns för rattonykterhet är 0.2 promille. Vid en trafikkontroll görs tre bestämningar av alkoholhalten i blodet.\nModell: De tre mätningarna \\(x_1, x_2, x_3\\) antas vara ett stickprov från slumpvariabeln \\(X = \\text{\"uppmätt alkoholhalt\"}\\) som antas vara normalfördelat \\(N(\\mu,0.07)\\) där \\(\\mu\\) är personsens verkliga alkoholhalt och 0.07 är ett mått på mätinstrumentets precision.\nFör Kalle blev \\(\\bar{x}=0.27\\) promille. Ska han dömas för rattonykterhet?"
  },
  {
    "objectID": "F7_anteckningar.html#konfidensintervall",
    "href": "F7_anteckningar.html#konfidensintervall",
    "title": "F7. Hypotestest",
    "section": "Konfidensintervall",
    "text": "Konfidensintervall\nKonfidensintervall:\nOm \\(\\sigma\\) är känd:\n\\(I_{\\mu}: \\bar{x} \\pm \\lambda_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\)\nOm \\(\\sigma\\) är okänd och skattas med \\(s\\):\n\\(I_{\\mu}: \\bar{x} \\pm t_{\\alpha/2}(n-1)\\frac{s}{\\sqrt{n}}\\)\nRegel: Förkasta \\(H_0\\) om \\(\\mu_0\\) inte ligger i \\(I_\\mu\\)"
  },
  {
    "objectID": "F7_anteckningar.html#teststorhet-med-kritiskt-området",
    "href": "F7_anteckningar.html#teststorhet-med-kritiskt-området",
    "title": "F7. Hypotestest",
    "section": "Teststorhet med kritiskt området",
    "text": "Teststorhet med kritiskt området\nTeststorhet:\nOm \\(\\sigma\\) är känd:\n\\(z = |\\frac{\\bar{x}-\\mu_0}{\\sigma/\\sqrt{n}}|\\)\nOm \\(\\sigma\\) är okänd och skattas med \\(s\\):\n\\(t = |\\frac{\\bar{x}-\\mu_0}{s/\\sqrt{n}}|\\)\nRegel: Förkasta \\(H_0\\) om \\(z &gt; \\lambda_{\\alpha/2}\\) alternativt om \\(t &gt; t_{\\alpha/2}(n-1)\\)"
  },
  {
    "objectID": "F7_anteckningar.html#hypotesprövning-med-direktmetoden",
    "href": "F7_anteckningar.html#hypotesprövning-med-direktmetoden",
    "title": "F7. Hypotestest",
    "section": "Hypotesprövning med direktmetoden",
    "text": "Hypotesprövning med direktmetoden\nRegel: Förkasta \\(H_0\\) om\n\\(\\begin{split} & \\text{p-värdet} = \\\\&  P(\\text{\"att få det vi fick eller värre\"}|H_0 \\text{ är sann}) &lt; \\alpha \\end{split}\\)\n\n\n\n\n\n\nExempel. Rattonykterhet\n\n\n\nHypoteser:\n\\(H_0: \\mu \\leq 0.2\\) mot \\(H_1: \\mu &gt; 0.2\\)\n\\(\\begin{split} & \\text{p-värdet} = P(\\bar{x} \\geq 0.27|\\mu = 0.2) = \\\\ &  P(\\frac{\\bar{x}-\\mu}{\\sigma/\\sqrt{n}} \\geq \\frac{0.27 - 0.2}{0.07/\\sqrt{3}}) = P(Z \\geq \\sqrt{3}) =  1-\\Phi(\\sqrt{3}) = \\\\ & 1 - \\Phi(1.73) = 1 - 0.958 =0.042 \\end{split}\\)\nRegel: Förkasta \\(H_0\\) eftersom p-värdet är mindre än den valda signifikansnivån på 0.05.\nOm vi hade valt signifikansnivå 0.01 istället, hade vi istället inte förkastat \\(H_0\\) eftersom p-värdet är högre än den valda signifikansnivån.\n\n\n\n\n\n\n\n\nÄndra inte signfikansnivå under testets gång\n\n\n\nMan ska välja signifikansnivå innan man gör testet. Det är tyvärr vanligt att forskare/statistiker väljer den lägsta signifikansnivån som leder till att nollhypotesen förkasats, men det leder i det långa loppet till övertro på resultat.\nSignifikansnivå bestäms när man planerar en studie."
  },
  {
    "objectID": "F7_anteckningar.html#definition-av-styrka",
    "href": "F7_anteckningar.html#definition-av-styrka",
    "title": "F7. Hypotestest",
    "section": "Definition av styrka",
    "text": "Definition av styrka\nSannolikheten att förkasta nollhypotesen när den är falsk är styrkan av ett test.\nStyrkan beror på det sanna värdet på parametern som testas.\n\\(S(\\mu) = P(\\text{Förkasta }H_0 |\\mu\\text{ är det sanna väntevärdet})\\)\nStyrkan är \\(1-\\beta = 1 - P(\\text{Fel av typ II})\\)\n\n\n\n\n\n\nExempel: Rattonykterhet\n\n\n\nHypoteser: \\(H_0: \\mu = 0.2\\) mot \\(H_1: \\mu &gt; 0.2\\)\nSignifikansnivå: \\(\\alpha = 0.05\\)\nTestregel: Förkasta \\(H_0\\) om \\(\\bar{x} &gt; \\mu_0 + \\lambda_{\\alpha}\\frac{\\sigma}{\\sqrt{n}} = 0.2 + 1.64 \\frac{0.07}{\\sqrt{3}} = 0.27\\)\n\\(\\begin{split} & S(\\mu) = P(\\bar{x} &gt; 0.27|\\mu) = P(\\frac{\\bar{x}-\\mu}{\\sigma/\\sqrt{n}} &gt; \\frac{0.27-\\mu}{0.07/\\sqrt{3}}) = // & P(Z &gt; \\frac{0.27-\\mu}{0.07/\\sqrt{3}}) = 1 - \\Phi(\\frac{0.27-\\mu}{0.07/\\sqrt{3}}) \\end{split}\\)\n\ndf &lt;- data.frame(mu = seq(0.1,0.4,by=0.005)) # möjliga mu-värden\nn = 3\nteststorhet = 0.2+1.64*0.07/sqrt(n)\ndf$styrka_mu_n3 &lt;- 1 - pnorm((teststorhet-df$mu)/(0.07/sqrt(n))) #styrka då n=3\nn = 6\nteststorhet = 0.2+1.64*0.07/sqrt(n)\ndf$styrka_mu_n6 &lt;- 1 - pnorm((teststorhet-df$mu)/(0.07/sqrt(n)))\n\nggplot(df,aes(x=mu,y=styrka_mu_n3)) +\n  geom_line() +\n  geom_line(aes(x=mu,y=styrka_mu_n6), col = 'blue') +\n  ylab(\"P(Kalle döms)\") +\n  xlab(\"Faktisk promille hos Kalle\") +\n  ggtitle(\"Testets strykefunktion\") +\n  annotate(\"text\",x = 0.3, y = 0.6, label = \"n = 3\") + \n  annotate(\"text\",x = 0.25, y = 0.75, label = \"n = 6\", col = \"blue\")\n\n\n\n\n\n\n\n\n\n\nStyrkefunktionen beror på\n\nDet sanna värdet på parametern \\(\\mu\\)\nStickprovsstorleken \\(n\\)\nVariansen i populationen \\(\\sigma^2\\)\nSignifikansnivån - ju lägre \\(\\alpha\\), desto sämre styrka\n\n\nEtt hypotestest innebär en avvägning mellan att göra fel av typ I och II\n\n\nhypotes(sigma=0.07,n=3,mu0=0.2,alfa=0.05,riktning=\"&gt;\",mu.sant = 0.35)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExempel: Livsmedelsfärg\n\n\n\nEn läkemedelstillverkare använder en viss livsmedelsfärg. Man vill veta hur färgen påverkar utseendet på läkemedlet. Utan färgämnet brukar grumlighet vara 4.0. Modell: \\(X = \\text{\"Grumlighet\"}\\) \\(X \\sim N(\\mu, 0.2)\\)\nMan mäter grumligheten på \\(n=10\\) slumpmässigt valda tabletter. Stickprovsmedelvärdet på grumligheten blev \\(\\bar{x}=4.1\\).\n\nTesta om läkemedlet är för grumligt på signifikansnivå \\(\\alpha = 0.05\\)\n\nHypoteser: \\(H_0: \\mu = 4.0\\) mot \\(H_1: \\mu &gt; 4.0\\)\nTestregel: Förkasta \\(H_0\\) om \\(\\bar{x} &gt; \\mu_0 + \\lambda_{0.05}\\frac{\\sigma}{\\sqrt{n}} = 4.0 + 1.64 \\frac{0.2}{\\sqrt{10}} = 4.104\\)\n⇒ Vi kan ej förkasta \\(H_0\\)\n\nVad är testets styrka när grumligheten är 3.8?\n\n\\(\\begin{split} & S(\\mu) = P(\\text{Reject }H_0|\\mu = 4.3) = \\\\ & P(\\bar{x} &gt; 4.104|\\mu = 4.3) = P(\\frac{\\bar{x}-\\mu}{\\sigma/\\sqrt{n}} &gt; \\frac{4.104-4.3}{0.2/\\sqrt{10}}) = \\\\ &  P(Z &gt;-3.1) = 1-\\Phi(-3.1) = \\Phi(3.1) = 0.999 \\end{split}\\)\n⇒ Styrkan är 0.999"
  },
  {
    "objectID": "F6_anteckningar.html",
    "href": "F6_anteckningar.html",
    "title": "F6. Skattningar och konfidensintervall",
    "section": "",
    "text": "Population\n\nAlla slumpvariabler (oavsett fördelning) har väntevärde och varians \\[\\mu = E(X)\\] \\[\\sigma^2 = V(X)=E((X-\\mu)^2) = E(X^2)-\\mu^2\\]\n\nStickprov\n\nStickprovsmedelvärde och stickprovsvarians är mått som sammanfattar läge och spridning i ett prov med slumpmässigt gjorda observationer från en population\n\\[\\bar{x} = \\frac{\\sum x_i}{n}\\]\n\\[s^2 = \\frac{\\sum (x_i-\\bar{x})^2 }{n-1} = \\frac{\\sum x^2 - n(\\bar{x})^2 }{n-1}\\]\n\n\n\n\nVVR:a estimatorer för väntevärde och varians är\n\n\\[\\hat{\\mu} = \\frac{\\sum_{i=1}^n x_i}{n} = \\bar{x}\\]\n\\[\\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n-1} = s^2\\]\n\n\n\n\\[\\bar{x} \\sim N(\\mu,\\frac{\\sigma}{\\sqrt{n}})\\]\nVilket när vi standardiserar är samma sak som att\n\\[\\frac{\\bar{x} - \\mu}{\\sigma / \\sqrt{n}} \\sim N(0,1)\\]\n\n\n\nStudent-t fördelningen är samplingsfördelningen för kvoten \\[\\frac{\\bar{x}-\\mu}{s/\\sqrt{n}} \\sim t(n-1)\\]\nVäntevärde och varians i en Student t-fördelning är alltid 0 respektive 1. Utseendet på täthetsfunktionen styrs med fördelningens frihetsgrader, vilket för denna kvot är \\(n-1\\).\n\n\n\nmu = 3.3\nsigma = 2\n\nn = 5\n\nt_sample &lt;- replicate(1000,{\n  x &lt;- rnorm(n,mu,sigma)\n  m &lt;- mean(x)\n  s &lt;- sd(x)\n  (m-mu)/(s/sqrt(n))})\n\n\ntt = seq(min(t_sample),max(t_sample),by=0.01)\npdf_norm = dnorm(tt)\npdf_t = dt(tt,n-1)\nhist(t_sample,prob=TRUE,ylim=c(0,max(pdf_norm)),breaks = 20)\nlines(tt,pdf_norm,col='red')\nlines(tt,pdf_t,col='blue')\n\n\n\n\n\n\n\n\n\n\n\nqqnorm(t_sample, main = \"\")\nqqline(t_sample, col='red')\ntitle('QQ-plot för Normal')\n\n\n\n\n\n\n\n\n\n\nplot(qt(ppoints(1000),n-1),sort(t_sample))\nabline(0,1,col='blue')\ntitle('QQ-plot för Student-t')\n\n\n\n\n\n\n\n\n\n\n\nt = seq(-6,6,by=0.01)\npdf_norm = dnorm(tt)\nplot(tt,pdf_norm,type = 'l',xlab='t',ylab='täthet',col='red')\nn = 5\npdf_t_5 = dt(tt,n-1)\nlines(tt,pdf_t_5,col = 'blue')\nn = 10\npdf_t_10 = dt(tt,n-1)\nlines(tt,pdf_t_10,col = 'green')\ntitle('Student-t går mot en normalfördelning när n ökar')\n\n\n\n\n\n\n\n\n\n\n\n\nPå samma sätt som vi kan skapa en kvantil i en normalfördelning baserat på kvantilen i en normalfördelning\n\\[\\bar{x}_{1-\\alpha} = \\mu + \\lambda_{\\alpha} \\cdot \\frac{\\sigma}{\\sqrt{n}}\\]\nkan vi skapa en kvantil i samplingsfördelningen för stickprovsmedelvärdet när variansen är okänd och skattas med \\(s^2\\)\n\\[\\bar{x}_{1-\\alpha} = \\mu + t_{\\alpha}(n-1) \\cdot \\frac{s}{\\sqrt{n}}\\]\ndär \\(t_{\\alpha}(n-1)\\) är kvantil i t-fördelning med \\(n-1\\) frihetsgrader"
  },
  {
    "objectID": "F6_anteckningar.html#population-och-stickprov",
    "href": "F6_anteckningar.html#population-och-stickprov",
    "title": "F6. Skattningar och konfidensintervall",
    "section": "",
    "text": "Population\n\nAlla slumpvariabler (oavsett fördelning) har väntevärde och varians \\[\\mu = E(X)\\] \\[\\sigma^2 = V(X)=E((X-\\mu)^2) = E(X^2)-\\mu^2\\]\n\nStickprov\n\nStickprovsmedelvärde och stickprovsvarians är mått som sammanfattar läge och spridning i ett prov med slumpmässigt gjorda observationer från en population\n\\[\\bar{x} = \\frac{\\sum x_i}{n}\\]\n\\[s^2 = \\frac{\\sum (x_i-\\bar{x})^2 }{n-1} = \\frac{\\sum x^2 - n(\\bar{x})^2 }{n-1}\\]"
  },
  {
    "objectID": "F6_anteckningar.html#väntevärdesriktiga-skattningar-av-väntevärde-och-varians",
    "href": "F6_anteckningar.html#väntevärdesriktiga-skattningar-av-väntevärde-och-varians",
    "title": "F6. Skattningar och konfidensintervall",
    "section": "",
    "text": "VVR:a estimatorer för väntevärde och varians är\n\n\\[\\hat{\\mu} = \\frac{\\sum_{i=1}^n x_i}{n} = \\bar{x}\\]\n\\[\\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n-1} = s^2\\]"
  },
  {
    "objectID": "F6_anteckningar.html#samplingsfördelning-för-barx-när-variansen-är-känd",
    "href": "F6_anteckningar.html#samplingsfördelning-för-barx-när-variansen-är-känd",
    "title": "F6. Skattningar och konfidensintervall",
    "section": "",
    "text": "\\[\\bar{x} \\sim N(\\mu,\\frac{\\sigma}{\\sqrt{n}})\\]\nVilket när vi standardiserar är samma sak som att\n\\[\\frac{\\bar{x} - \\mu}{\\sigma / \\sqrt{n}} \\sim N(0,1)\\]"
  },
  {
    "objectID": "F6_anteckningar.html#samplingsfördelning-för-barx-när-variansen-är-okänd-och-skattas-med-s2",
    "href": "F6_anteckningar.html#samplingsfördelning-för-barx-när-variansen-är-okänd-och-skattas-med-s2",
    "title": "F6. Skattningar och konfidensintervall",
    "section": "",
    "text": "Student-t fördelningen är samplingsfördelningen för kvoten \\[\\frac{\\bar{x}-\\mu}{s/\\sqrt{n}} \\sim t(n-1)\\]\nVäntevärde och varians i en Student t-fördelning är alltid 0 respektive 1. Utseendet på täthetsfunktionen styrs med fördelningens frihetsgrader, vilket för denna kvot är \\(n-1\\).\n\n\n\nmu = 3.3\nsigma = 2\n\nn = 5\n\nt_sample &lt;- replicate(1000,{\n  x &lt;- rnorm(n,mu,sigma)\n  m &lt;- mean(x)\n  s &lt;- sd(x)\n  (m-mu)/(s/sqrt(n))})\n\n\ntt = seq(min(t_sample),max(t_sample),by=0.01)\npdf_norm = dnorm(tt)\npdf_t = dt(tt,n-1)\nhist(t_sample,prob=TRUE,ylim=c(0,max(pdf_norm)),breaks = 20)\nlines(tt,pdf_norm,col='red')\nlines(tt,pdf_t,col='blue')\n\n\n\n\n\n\n\n\n\n\n\nqqnorm(t_sample, main = \"\")\nqqline(t_sample, col='red')\ntitle('QQ-plot för Normal')\n\n\n\n\n\n\n\n\n\n\nplot(qt(ppoints(1000),n-1),sort(t_sample))\nabline(0,1,col='blue')\ntitle('QQ-plot för Student-t')\n\n\n\n\n\n\n\n\n\n\n\nt = seq(-6,6,by=0.01)\npdf_norm = dnorm(tt)\nplot(tt,pdf_norm,type = 'l',xlab='t',ylab='täthet',col='red')\nn = 5\npdf_t_5 = dt(tt,n-1)\nlines(tt,pdf_t_5,col = 'blue')\nn = 10\npdf_t_10 = dt(tt,n-1)\nlines(tt,pdf_t_10,col = 'green')\ntitle('Student-t går mot en normalfördelning när n ökar')"
  },
  {
    "objectID": "F6_anteckningar.html#kvantil-i-t-fördelningen",
    "href": "F6_anteckningar.html#kvantil-i-t-fördelningen",
    "title": "F6. Skattningar och konfidensintervall",
    "section": "",
    "text": "På samma sätt som vi kan skapa en kvantil i en normalfördelning baserat på kvantilen i en normalfördelning\n\\[\\bar{x}_{1-\\alpha} = \\mu + \\lambda_{\\alpha} \\cdot \\frac{\\sigma}{\\sqrt{n}}\\]\nkan vi skapa en kvantil i samplingsfördelningen för stickprovsmedelvärdet när variansen är okänd och skattas med \\(s^2\\)\n\\[\\bar{x}_{1-\\alpha} = \\mu + t_{\\alpha}(n-1) \\cdot \\frac{s}{\\sqrt{n}}\\]\ndär \\(t_{\\alpha}(n-1)\\) är kvantil i t-fördelning med \\(n-1\\) frihetsgrader"
  },
  {
    "objectID": "F6_anteckningar.html#tolking-av-konfidensintervall",
    "href": "F6_anteckningar.html#tolking-av-konfidensintervall",
    "title": "F6. Skattningar och konfidensintervall",
    "section": "Tolking av konfidensintervall",
    "text": "Tolking av konfidensintervall\nKonfidensintervall förekommer inom frekventistisk statistisk inferens. De tolkas som hur ofta konfidensintervall beräknade på samma sätt täcker det sanna värdet om man skulle gå ut och samla in nya stickprov.\nNedan illustrerar vi ett 95%-igt konfidensintervall när man har 10 respektive 20 mätningar, där vi ser att intervallen blir smalare ju fler mätningar man har.\nDe röda intervallen motsvarar de intervall som inte täcker det sanna väntevärdet. Det bör ske i \\(1-0.95 = 5\\%\\) av fallen.\n\nsource(\"kod/funktioner_raknamedvariation_light.R\")\n\nskattningar(mu=195, sigma=3, n1=10, n2=20, alternativ = 'konfint') \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExempel: radon fortsättning\n\n\n\nGränsen för radon i ett hus är 200 \\(Bq/m^3\\). Vi gör 10 mätningar med medelvärdet 195 \\(Bq/m^3\\). Kan vi vara säkra på att radon-nivån i huset är under gränsvärdet?\nEfter att ha räknat ut konfidensintervallet \\(I_{\\mu}: (193.1, 196.9)\\), är vi färdiga att påstå att vi är ganska säkra på att radonhalten är under gränsvärdet.\n\n\n\n\n\n\n\n\nVar kommer ordet konfidens ifrån och varför använder man inte ordet sannolikhet?\n\n\n\nEnkelt uttryckt, var termen sannolikhet upptagen av Bayesiansk inferens. Sannolikhetsintervall används inom Bayesiansk statistisk inferens och har tolkningen hur säkra vi är att det sanna värdet på parameter är i intervallet. Det är vanligt att konfidensintervall tolkas på samma sätt.\nEn annan orsak är att de som kom på konfidensintervall också ville att det skulle inte missuppfattas att vara en sannolikhet för det sanna värdet på parametern \\(\\mu\\), vilket enligt frekventistik sätt att tänka inte i sig själv är en slumpvariabel och därför inte kan ha en fördelning.\nNotera att alla intervall inte är konfidensintervall, även om det är vanligt att man kallar dem det. För att det ska bli ett konfidensintervall behöver man ha satt upp en modell för populationen och härlett samplingsfördelning för den skattade parametern."
  },
  {
    "objectID": "F6_anteckningar.html#okänd-varians",
    "href": "F6_anteckningar.html#okänd-varians",
    "title": "F6. Skattningar och konfidensintervall",
    "section": "Okänd varians",
    "text": "Okänd varians\nNär variansen \\(\\sigma_2\\) är okänd skattar vi den med stickprovsvariansen \\(s^2\\)\nEftersom vi skattar variansen använder vi en kvantil från samplingsfördelningen för kvoten \\(t\\) och standardfelet \\(\\frac{s}{\\sqrt{n}}\\)\n\\[I_{\\mu}: \\bar{x} \\pm t_{\\alpha/2}(n-1) \\frac{s}{\\sqrt{n}}\\]\n\n\n\n\n\n\nUppgift 1\n\n\n\nMan har ett slumpmässigt stickprov från \\(X\\sim N(\\mu,2)\\)\n44.3 45.1 46.1 45.3\nStickprovsmedelvärdet beräknas till \\(\\bar{x} = 45.2\\)\n\nAnge ett 95%-igt konfidensintervall för väntevärdet \\(\\mu\\)\n\n\\(\\alpha = 0.05\\)\nStandardavvikelse \\(\\sigma=2\\) är känd\n\\[I_{\\mu}: \\bar{x} \\pm \\lambda_{0.05/2} \\frac{\\sigma}{\\sqrt{n}} = 45.2 \\pm 1.96 \\frac{2}{\\sqrt{4}} = (43.24, 47.16)\\]\n\nHur ser ett 99%-igt konfidensinterall ut?\n\n\\[I_{\\mu}: \\bar{x} \\pm \\lambda_{0.01/2} \\frac{\\sigma}{\\sqrt{n}} = 45.2 \\pm 2.58 \\frac{2}{\\sqrt{4}} = (42.62, 47.78)\\]\n\n\n\n\n\n\n\n\nUppgift 2\n\n\n\nMan mäter 10 gånger från \\(X\\sim N(\\mu,\\sigma)\\)\n7.3 7.2 7.8 7.1 8.0 6.9 7.5 8.1 7.7 7.5\n\nx = c(7.3, 7.2, 7.8, 7.1, 8.0, 6.9, 7.5, 8.1, 7.7, 7.5)\nm = mean(x)\ns = sd(x)\n\nBeräkna ett 95%-igt konfidensintervall för väntevärdet \\(\\mu\\)!\nStickprovsmedelvärdet är \\(\\bar{x} = 7.51\\)\n\\(\\alpha = 0.05\\)\nStandardavvikelse för populationen \\(\\sigma\\) är okänd och skattas med stickprovsstandardavvikelsen \\(s = 0.393\\)\n\\[I_{\\mu}: \\bar{x} \\pm t_{0.05/2}(n-1) \\frac{s}{\\sqrt{n}} = 7.51 \\pm 2.26 \\frac{0.393}{\\sqrt{10}} = (7.23,7.79)\\]"
  },
  {
    "objectID": "F5_anteckningar.html",
    "href": "F5_anteckningar.html",
    "title": "F5. Summor, centrala gränsvärdessatsen, och statistisk modell",
    "section": "",
    "text": "Hittills har vi tittat på slumpvariabler med kända parametrar\nI verkligheten vet vi oftast inte vilka värden dess parametrar har.\nVi måste “skatta” parametrar från de observationer vi har gjort (d.v.s. från våra insamlade data)"
  },
  {
    "objectID": "F5_anteckningar.html#exempel.-myrors-vikt.",
    "href": "F5_anteckningar.html#exempel.-myrors-vikt.",
    "title": "F5. Summor, centrala gränsvärdessatsen, och statistisk modell",
    "section": "Exempel. Myrors vikt.",
    "text": "Exempel. Myrors vikt.\n\n\n\nAntag att vikt hos myror följer en normalfördelning, men där vi vi inte vet vad väntevärdet är.\nVi hittar en myrstack och väljer ut 10 myror\n\nModell: \\(X_i=\\text{\"myra i's vikt i mg\"}\\) \\[X_i \\sim N(\\mu,\\sigma)\\]\nstickprov: \\(\\{x_1,x_2,\\dots,x_{10}\\}\\)\n\nVi skattar väntevärdet med stickprovsmedelvärdet\n\n\\[\\hat{\\mu} = \\bar{x}\\]\n\n\n\n\n\nVi beräknar medelvärdet på stickprovet till \\(\\bar{x} = 5.2\\) mg.\n\n\nOm vi väljer ut 10 nya myror, kommer medelvärdet av dessa också vara 5.2 mg? Troligtvis inte.\n\nFöreställ dig att du samlar in stickprov av myror många gånger och beräknar medelvärdet varje gång. Det finns en slumpmässig variation hos stickprovsmedelvärdena. Man kan säga att medelvärdet av upprepade stickprov är en slumpvariabel som vi kan beteckna som \\(\\bar{X}\\)."
  },
  {
    "objectID": "F5_anteckningar.html#när-populationen-antas-vara-normalfördelad",
    "href": "F5_anteckningar.html#när-populationen-antas-vara-normalfördelad",
    "title": "F5. Summor, centrala gränsvärdessatsen, och statistisk modell",
    "section": "När populationen antas vara normalfördelad",
    "text": "När populationen antas vara normalfördelad\n\nVi antog att en myras vikt är normalfödelad.\nEnligt regeln “en summa av normalfördelade blir också normalfördelad” kommer stickprovsmedelvärdet också vara normalfördelad\nVi betraktar myrors vikter som observationer från samma fördelning (de är likafördelade)\nVi betrakter dem även som oberoende\n\nDå gäller: \\[\\bar{X} \\sim N(\\mu,\\frac{\\sigma}{\\sqrt{n}})\\]\ndär \\(n=10\\). Vi kan sen skatta väntevärdet \\(\\mu\\) och standardavikelsen \\(\\sigma\\) med stickprovet vi har.\n\nmu = 5 # väntevärde\nsigma = 1 # standardavvikelse\n\nn = 10 # antal värden i stickprovet\n\niter = 1000 # antal gånger vi gör ett nytt stickprov och beräknar dess medelvärde\n\nstickprovsmedelvärde &lt;- replicate(iter,mean(rnorm(n,mu,sigma))) \n\nhist(stickprovsmedelvärde,prob = TRUE)\nxx = seq(4,6,by=0.01)\nyy = dnorm(xx,mu,sigma/sqrt(n))\nlines(xx,yy,col='blue') #lägger på täthetsfunktionen för N(mu,sigma/sqrt(n))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\bar{X}\\) eller \\(\\bar{x}\\)\n\n\n\nDet går bra att använda \\(\\bar{x}\\) för att beteckna slumpvariabeln för stickprovsmedelvärdet, så länge det framgår av sammanhanget att det är en slumpvariabel och inte ditt framräknade värde. T.ex.\n\\[\\bar{x} \\sim N(\\mu,\\frac{\\sigma}{\\sqrt{n}})\\]"
  },
  {
    "objectID": "F5_anteckningar.html#när-vi-inte-vet-vilken-fördelning-populationen-har",
    "href": "F5_anteckningar.html#när-vi-inte-vet-vilken-fördelning-populationen-har",
    "title": "F5. Summor, centrala gränsvärdessatsen, och statistisk modell",
    "section": "När vi inte vet vilken fördelning populationen har",
    "text": "När vi inte vet vilken fördelning populationen har\n\nVad händer om vi inte vet vilken fördelning myrornas vikt kan tänkas ha?\nVilken fördelning kommer då stickprovsmedelvärdet att ha?\n\n\nn = 10 # antal värden i stickprovet\n\niter = 1000 # antal gånger vi gör ett nytt stickprov och beräknar dess medelvärde\n\n\nstickprovsmedelvärde &lt;- replicate(iter,mean(rexp(n))) # den sanna fördelningen är exponential\nhist(stickprovsmedelvärde,prob = TRUE)\n\n\n\n\n\n\n\nstickprovsmedelvärde &lt;- replicate(iter,mean(runif(n))) # den sanna fördelningen är likformig\nhist(stickprovsmedelvärde,prob = TRUE)\n\n\n\n\n\n\n\nstickprovsmedelvärde &lt;- replicate(iter,mean(rlnorm(n))) # den sanna fördelningen är lognormal\nhist(stickprovsmedelvärde,prob = TRUE)"
  },
  {
    "objectID": "F5_anteckningar.html#exempel.-100-tabletter",
    "href": "F5_anteckningar.html#exempel.-100-tabletter",
    "title": "F5. Summor, centrala gränsvärdessatsen, och statistisk modell",
    "section": "Exempel. 100 tabletter",
    "text": "Exempel. 100 tabletter\nVikten (i gram) av en slumpmässigt vald värktablett från ett parti med likadana tabletter är en s.v. med väntevärde \\(\\mu = 0.65\\) och varians \\(\\sigma^2=0.0004\\)\n\nVad är väntevärde och varians för den totala vikten av 100 tabletter (vars vikter är oberoende av varandra)?\n\nModell: Låt \\(X_i = \\text{\"vikt hos tablett i\"}\\), där \\(i = 1,\\dots,100\\)\nSätt \\(Y_{100} = \\sum_{i=1}^{100} X_i\\)\n\\(E(Y_{100})=100\\cdot \\mu = 65\\)\n\\(V(Y_{100})=100\\cdot \\sigma^2 = 0.04\\)\n\nVad är sannolikheten att 100 värktabletter tillsammans väger högst 65.3 gram?\n\nEftersom 100 är ett ganska stor antal, säger vi att enligt centrala gränsvärdessatsen är summan av vikterna approximativt normalfördelad.\n\\[\\begin{split} & P(Y_{100} \\leq 65.3) = P(\\frac{Y_{100}-\\mu}{\\sigma} \\leq \\frac{65.3-\\mu}{\\sigma}) = \\\\ & \\Phi(\\frac{65.3-65}{\\sqrt{0.04}}) = \\Phi(\\frac{0.3}{0.2}) = \\Phi(1.5) =  0.9332 \\end{split}\\]"
  },
  {
    "objectID": "F5_anteckningar.html#vad-är-skillnaden-mellan-sannolikhetsteori-och-statistisk-inferens",
    "href": "F5_anteckningar.html#vad-är-skillnaden-mellan-sannolikhetsteori-och-statistisk-inferens",
    "title": "F5. Summor, centrala gränsvärdessatsen, och statistisk modell",
    "section": "Vad är skillnaden mellan sannolikhetsteori och statistisk inferens?",
    "text": "Vad är skillnaden mellan sannolikhetsteori och statistisk inferens?\nSannolikhetsteori\n\nSannolikhetsfördelningarna är helt kända (vi känner till alla populationsparametrar i modellen)\n\nStatistisk inferens\n\nSannolikhetsfördelningarna är inte kända, typ av fördelning eller innehåller okända parametrar\nVi använder data för att skatta parametrar och välja modell, och dra slutsatser med hjälp av modellerna"
  },
  {
    "objectID": "F5_anteckningar.html#skevhet-och-precision",
    "href": "F5_anteckningar.html#skevhet-och-precision",
    "title": "F5. Summor, centrala gränsvärdessatsen, och statistisk modell",
    "section": "Skevhet och precision",
    "text": "Skevhet och precision\n\nVäntevärdesriktig (VVR) innebär ingen skevhet (på engelska: bias)\nLåg varians motsvarar hög precision hos en estimator"
  },
  {
    "objectID": "F4_anteckningar.html",
    "href": "F4_anteckningar.html",
    "title": "F4. Kontinuerliga slumpvariabler, linjärkombination och summor av slumpvariabler",
    "section": "",
    "text": "Kvantilen fås genom att läsa fördelningsfunktionen baklänges. Figuren visar \\(z_{.25}\\) (röd) och \\(z_{.90}\\) för en standardiserad normalfördelning \\(Z \\sim N(0,1)\\).\n\n\n\n\n\n\n\n\n\nI R beräknar vi kvantil från en normalfördelning med funktionen qnorm(p), där p är sannolikheten som är nedanför det sökta kvantilvärdet.\nKvantiler erhålls för fördelningar på samma sätt genom att sätta q framför namnet på fördelningen, t.ex. qexp eller qt.\n\n\nEtt sätt att undersöka modellanpassning är att jämföra teoretiska kvantiler (från modellen) med empiriska kvantiler (från stickprovet).\n\n\n\n\n\n\nEmpirisk fördelningsfunktion\n\n\n\nDen empiriska fördelningsfunktionen kan skapas genom följande steg:\n\nsortera de \\(n\\) värdena i stickprovet - detta är värden på x-skalan\ndela in intervallet 0 till 1 i \\(n\\) lika stora bitar\nmotsvarande värdet på y-skalan börjar i noll och för varje värde på x-skalan (börja med det minsta), öka med en bit i taget\n\n\nload(\"data/lab1_filer/jordprov.Rdata\")\nx &lt;- sort(jordprov$al) # sortera\nn &lt;- length(x) # antal värden i stickprovet\neFx &lt;- (1:n)/n # steg\nplot(x,eFx,main=\"Empirisk fördelningsfunktion\") # rita eFx mot x\n\n\n\n\n\n\n\n\nDet är detta som görs, fast med en snyggare layout, med functionen plot.ecdf()\n\nplot.ecdf(jordprov$al)\n\n\n\n\n\n\n\n\nEmpirisk fördelningsfunktion på wiki\n\n\n\n\nLåt oss säga att vi vill undersöka om en stickprov kan tänkas komma från en normalfördelning.\n\nDen teoretiska fördelningen är då en normalfördelning med parametrar skattade från stickprovet.\n\n\nm = mean(jordprov$al)\ns = sd(jordprov$al)\n\n\nVi beräknar kvantiler givet den teoretiska fördelningen som motsvarar den empiriska fördelningsfunktionens y-skala\n\n\ntFx &lt;- qnorm(eFx,m,s)\n\n\nSen ritar vi de teoretiska kvantilerna mot de sorterade värdena i stickprovet. De borde ligga på en rak linje om den teoretiska modellen är en bra fördelning för stickprovet.\n\n\nplot(tFx,x)\nabline(0,1) # lägg till en 1:1-linje\n\n\n\n\n\n\n\n\nSamma sak görs med funktionerna qqnorm och qqline men med lite snyggare layout\n\nqqnorm(jordprov$al)\nqqline(jordprov$al)"
  },
  {
    "objectID": "F4_anteckningar.html#kvantil-kvantil-plot",
    "href": "F4_anteckningar.html#kvantil-kvantil-plot",
    "title": "F4. Kontinuerliga slumpvariabler, linjärkombination och summor av slumpvariabler",
    "section": "",
    "text": "Ett sätt att undersöka modellanpassning är att jämföra teoretiska kvantiler (från modellen) med empiriska kvantiler (från stickprovet).\n\n\n\n\n\n\nEmpirisk fördelningsfunktion\n\n\n\nDen empiriska fördelningsfunktionen kan skapas genom följande steg:\n\nsortera de \\(n\\) värdena i stickprovet - detta är värden på x-skalan\ndela in intervallet 0 till 1 i \\(n\\) lika stora bitar\nmotsvarande värdet på y-skalan börjar i noll och för varje värde på x-skalan (börja med det minsta), öka med en bit i taget\n\n\nload(\"data/lab1_filer/jordprov.Rdata\")\nx &lt;- sort(jordprov$al) # sortera\nn &lt;- length(x) # antal värden i stickprovet\neFx &lt;- (1:n)/n # steg\nplot(x,eFx,main=\"Empirisk fördelningsfunktion\") # rita eFx mot x\n\n\n\n\n\n\n\n\nDet är detta som görs, fast med en snyggare layout, med functionen plot.ecdf()\n\nplot.ecdf(jordprov$al)\n\n\n\n\n\n\n\n\nEmpirisk fördelningsfunktion på wiki\n\n\n\n\nLåt oss säga att vi vill undersöka om en stickprov kan tänkas komma från en normalfördelning.\n\nDen teoretiska fördelningen är då en normalfördelning med parametrar skattade från stickprovet.\n\n\nm = mean(jordprov$al)\ns = sd(jordprov$al)\n\n\nVi beräknar kvantiler givet den teoretiska fördelningen som motsvarar den empiriska fördelningsfunktionens y-skala\n\n\ntFx &lt;- qnorm(eFx,m,s)\n\n\nSen ritar vi de teoretiska kvantilerna mot de sorterade värdena i stickprovet. De borde ligga på en rak linje om den teoretiska modellen är en bra fördelning för stickprovet.\n\n\nplot(tFx,x)\nabline(0,1) # lägg till en 1:1-linje\n\n\n\n\n\n\n\n\nSamma sak görs med funktionerna qqnorm och qqline men med lite snyggare layout\n\nqqnorm(jordprov$al)\nqqline(jordprov$al)"
  },
  {
    "objectID": "F4_anteckningar.html#väntevärdet-av-slumpvariabeln-x",
    "href": "F4_anteckningar.html#väntevärdet-av-slumpvariabeln-x",
    "title": "F4. Kontinuerliga slumpvariabler, linjärkombination och summor av slumpvariabler",
    "section": "Väntevärdet av slumpvariabeln \\(X\\)",
    "text": "Väntevärdet av slumpvariabeln \\(X\\)\n\\(X\\) är diskret: \\(E(X) = \\sum_{alla \\ x}xf(x)\\)\n\\(X\\) är kontinuerlig: \\(E(X) = \\int_{-\\infty}^{\\infty}xf(x)dx\\)\n\n\n\n\n\n\nVäntevärdet för en likformig fördelning\n\n\n\n\\(X \\sim U(a,b)\\)\n\\[f(x) = \\left\\{ \\begin{array}{lr}\n        \\frac{1}{b-a} & \\text{om }a \\leq x \\leq b\\\\\n        0 & \\text{annars}\n        \\end{array}\\right.\\]\n\\[\\begin{split} E(X)  =\\int_{-\\infty}^{\\infty}xf(x)dx = & \\\\ \\int_{a}^{b}\\frac{x}{b-a}dx = [\\frac{x^2}{2(b-a)}]_{x=a}^{b}  = & \\\\ \\frac{b^2-a^2}{2(b-a)} = \\frac{(b+a)(b-a)}{2(b-a)} = \\frac{a+b}{2}  \\end{split}\\]\nVäntevärdet av \\(X \\sim U(0,10)\\) är \\(E(X)=\\frac{0+10}{2} = 5\\)"
  },
  {
    "objectID": "F4_anteckningar.html#väntevärdet-av-en-funktion-g-av-x",
    "href": "F4_anteckningar.html#väntevärdet-av-en-funktion-g-av-x",
    "title": "F4. Kontinuerliga slumpvariabler, linjärkombination och summor av slumpvariabler",
    "section": "Väntevärdet av en funktion \\(g\\) av \\(X\\)",
    "text": "Väntevärdet av en funktion \\(g\\) av \\(X\\)\n\\(X\\) är diskret: \\(E(g(X)) = \\sum_{alla \\ x}g(x)f(x)\\)\n\\(X\\) är kontinuerlig: \\(E(g(X)) = \\int_{-\\infty}^{\\infty}g(x)f(x)dx\\)\n\n\n\n\n\n\nVäntevärdet för en funktion av en likformig fördelning\n\n\n\nfortsättning på \\(X \\sim U(0,10)\\)\n\\(g(x) = x^2\\)\n\\[\\begin{split}  E(g(X)) = \\int_{-\\infty}^{\\infty}g(x)f(x)dx = \\int_{0}^{10}x^2\\frac{1}{10}dx = & \\\\  [\\frac{x^3}{3\\cdot 10}]_{x=0}^{10} = \\frac{10^3}{30} = \\frac{100}{3} \\end{split}\\]"
  },
  {
    "objectID": "F4_anteckningar.html#väntevärde-av-en-linjärkombination-av-en-diskret-slumpvariabel",
    "href": "F4_anteckningar.html#väntevärde-av-en-linjärkombination-av-en-diskret-slumpvariabel",
    "title": "F4. Kontinuerliga slumpvariabler, linjärkombination och summor av slumpvariabler",
    "section": "Väntevärde av en linjärkombination av en diskret slumpvariabel",
    "text": "Väntevärde av en linjärkombination av en diskret slumpvariabel\n\\(X\\) är en diskret slumpvariabel\n\nVad är väntevärdet \\(E(X + b)\\)?\n\n\\[\\begin{split} E(X+b)=\\sum_{alla \\ x}(x+b)f(x) = \\sum_{alla \\ x}xf(x) + \\sum_{alla \\ x}bf(x) = & \\\\ E(X) + b\\sum_{alla \\ x}f(x) = E(X) + b \\end{split}\\]\n\nVad är väntevärdet \\(E(aX + b)\\)?\n\n\\[\\begin{split} E(aX+b)=\\sum_{alla \\ x}(ax+b)f(x) = & \\\\ \\sum_{alla \\ x}axf(x) + \\sum_{alla \\ x}bf(x) = & \\\\ a\\sum_{alla \\ x}xf(x) + b\\sum_{alla \\ x}f(x) = aE(X) + b \\end{split}\\]"
  },
  {
    "objectID": "F4_anteckningar.html#varians-av-en-linjärkombination-av-en-diskret-slumpvariabel",
    "href": "F4_anteckningar.html#varians-av-en-linjärkombination-av-en-diskret-slumpvariabel",
    "title": "F4. Kontinuerliga slumpvariabler, linjärkombination och summor av slumpvariabler",
    "section": "Varians av en linjärkombination av en diskret slumpvariabel",
    "text": "Varians av en linjärkombination av en diskret slumpvariabel\n\nVarians på minst tre sätt\n\\(E(X) = \\mu\\)\n\\[\\begin{split} & V(X) =  \\underbrace{E[(X-E(X))^2]}_{\\text{sätt } I} =   \\underbrace{\\sum (x-\\mu)^2f(x)}_{\\text{sätt }II} = \\\\ & \\sum (x^2 - 2x\\mu + \\mu^2)f(x) = \\sum x^2f(x) - \\sum 2x\\mu f(x) + \\sum \\mu^2 f(x) = \\\\ &  E(X^2) - 2\\mu \\sum xf(x) + \\mu^2 \\sum f(x) = E(X^2)-2(E(X))^2+(E(X))^2 =  \\\\ &  \\underbrace{E(X^2)-[E(X)]^2}_{\\text{sätt }III} \\end{split}\\]\n\n\\(X\\) är en diskret slumpvariabel\n\nVad är variansen \\(V(X + b)\\)?\n\n\\[\\begin{split} & V(X + b) = E[(X+b-E(X+b))^2] = \\\\ & E[(X+b-E(X)-b)^2] = E[(X-E(X))^2] = V(X) \\end{split}\\]\n\nEn konstant \\(b\\) flyttar fördelningen men ändrar inte dess spridning.\n\n\nVad är variansen \\(V(a \\cdot X + b)\\)?\n\n\\[\\begin{split} & V(a \\cdot X + b) = E[(aX+b-aE(X)-b)^2] = E[(aX-aE(X))^2]= \\\\ & E[a^2(X-E(X))^2] = a^2E[(X-E(X))^2] = a^2V(X) \\end{split}\\]"
  },
  {
    "objectID": "F4_anteckningar.html#viktiga-specialfall",
    "href": "F4_anteckningar.html#viktiga-specialfall",
    "title": "F4. Kontinuerliga slumpvariabler, linjärkombination och summor av slumpvariabler",
    "section": "Viktiga specialfall",
    "text": "Viktiga specialfall\n\nFördelning för en summa av oberoende likafördelade normalfördelningar\n\\(\\sum_{i=1}^n X_i \\sim N(n\\mu,\\sqrt{n}\\sigma)\\)\n\n\nFördelning för ett medelvärde av oberoende likafördelade normalfördelningar\n\\(\\bar{X} \\sim N(\\mu,\\frac{\\sigma}{\\sqrt{n}})\\)\n\n\n\n\n\n\nExempel 1\n\n\n\nLåt \\(X \\sim N(0,1)\\) och \\(Y=3X+2\\)\nBeräkna \\(E(Y)\\) och \\(D(Y)\\)\n\\(E(Y)=E(3X+2)=3E(X)+2=3\\cdot 0 + 2 = 2\\)\n\\(V(Y)=V(3X+2)=3^2V(X)=3^2\\cdot 1\\)\n\\(D(Y)=\\sqrt{V(Y)}=\\sqrt{3^2} = 3\\)\n\n\n\n\n\n\n\n\nExempel 2\n\n\n\nLåt \\(X \\sim N(1,1)\\) och \\(Y \\sim N(-1,2)\\) vara oberoende slumpvariabler\n\nVilken fördelning har \\(X+Y\\)?\n\nDet kommer vara en normalfördelning med väntevärde\n\\(E(X+Y) = E(X) + E(Y) = 1 + (-1) = 0\\)\noch varians\n\\(V(X+Y)=V(X)+V(Y)=1^2+2^2=5\\)\n\nVilken fördelning har \\(X-Y\\)?\n\nDet kommer vara en normalfördelning med väntevärde\n\\(E(X-Y) = E(X) + E(-Y) = E(X) + (-1)\\cdot E(Y) = 1 + (-1)\\cdot(-1) = 1 + 1 = 2\\)\noch varians\n\\(V(X-Y)=V(X)+V(-Y)=V(X)+(-1)^2\\cdot V(Y)= V(X)+V(Y) = 1^2+2^2 = 5\\)\n\n\n\n\n\n\n\n\nExempel 3\n\n\n\nDe oberoende slumpvariabeln \\(X_1\\) och \\(X_2\\) tillhör båda \\(N(1,2)\\)\n\nAnge fördelning för \\(\\frac{X_1+X_2}{2}\\)\n\nDetta är ett medelvärde av två oberoende och normalfördelade slumpvariabler med samma väntevärde \\(\\mu=1\\) och varians \\(\\sigma^2=2\\)\n\\(\\bar{X}=\\frac{X_1+X_2}{2}\\)\nEnligt regler för linjärkombination av slumpvariabler och kännedom att “summa av normal är normal” blir\n\\(\\bar{X} \\sim N(\\mu,\\frac{\\sigma}{\\sqrt{2}})\\)\nMen låt oss härleda väntevärde och varians\n\\[E(\\bar{X}) = E(\\frac{X_1+X_2}{2}) = \\frac{1}{2}E(X_1+X_2) = \\frac{1}{2} (1 + 1) = 1\\]\n\\[V(\\bar{X}) = V(\\frac{X_1+X_2}{2}) = \\frac{1}{2^2}V(X_1+X_2)=\\frac{1}{2^2}(2^2+2^2)=2\\]\n\nNotera att summan har lägre varians än respektive s.v.! Orsaken är att de två s.v. “tar ut varandra” vilket bidrar till lägre variation i summan."
  },
  {
    "objectID": "F3_anteckningar.html",
    "href": "F3_anteckningar.html",
    "title": "F3. Kontinuerliga slumpvariabler",
    "section": "",
    "text": "Låt \\(\\mu\\) vara väntevärdet för slumpvariabeln \\(X\\)\nVariansen beskriver spridningen runt väntevärdet. Mer specifikt är variansen det föräntade kvadratiska avståndet från väntevärdet.\n\\(V(X) = E((X-E(X))^2)\\)\neller\n\\(V(X) = E((X-\\mu)^2)\\)\n\n\n\n\n\n\nNote\n\n\n\nOrsaken till att man kvadrerar avstånden är att det kommer vara både negativa och positiva avstånd, och att de kan “ta ut varandra” om man summerar direkt.\nStandardavvikelsen är \\(\\sqrt{V(X)}\\) och får ner spridningsmåttet på samma skala som slumpvariabeln \\(X\\)."
  },
  {
    "objectID": "F3_anteckningar.html#fördelningsfunktion-för-en-exponentialfördelning",
    "href": "F3_anteckningar.html#fördelningsfunktion-för-en-exponentialfördelning",
    "title": "F3. Kontinuerliga slumpvariabler",
    "section": "Fördelningsfunktion för en exponentialfördelning",
    "text": "Fördelningsfunktion för en exponentialfördelning\nFördelningsfunktionen för en exponentialfördelad slumpvariabel är\n\\[F(x) = 1 - e^{-\\lambda x}\\]"
  },
  {
    "objectID": "F3_anteckningar.html#täthetsfunktion-för-normalfördelning",
    "href": "F3_anteckningar.html#täthetsfunktion-för-normalfördelning",
    "title": "F3. Kontinuerliga slumpvariabler",
    "section": "Täthetsfunktion för normalfördelning",
    "text": "Täthetsfunktion för normalfördelning\n\\(X \\sim N(\\mu,\\sigma)\\)\n\n\n\n\n\n\nstandardavvikelse eller varians\n\n\n\nVissa textböcker och program använder varians i formeln \\(N(\\mu,\\sigma^2)\\)\n\n\n\nTäthetsfunktionen för en normalfördelning ser ut som en kyrk-klocka\nNormalfördelningen är symmetrisk\n\n\\(F(x) = 1 - F(-x)\\)\n\nTypvärde, median och väntevärde sammanfaller i en normalfördelning"
  },
  {
    "objectID": "F3_anteckningar.html#fördelningsfunktion-för-en-normalfördelning",
    "href": "F3_anteckningar.html#fördelningsfunktion-för-en-normalfördelning",
    "title": "F3. Kontinuerliga slumpvariabler",
    "section": "Fördelningsfunktion för en normalfördelning",
    "text": "Fördelningsfunktion för en normalfördelning\n\\(X \\sim N(\\mu,\\sigma)\\)\n\\[\\begin{split}  P(X \\leq 0.1) & = F(0.1) = \\int_{-\\infty}^{0.1}f(x)dx = \\\\  & \\int_{-\\infty}^{0.1}\\frac{1}{\\sigma \\sqrt{2\\pi}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}dx \\end{split}\\]\nLåt oss anta att \\(\\mu=0\\) och \\(\\sigma=1\\)\n\\[=\\int_{-\\infty}^{0.1}\\frac{1}{\\sqrt{2\\pi}}e^{-x^2}dx = \\dots\\text{går att lösa men svårt}\\]"
  },
  {
    "objectID": "F3_anteckningar.html#fördelningsfunktion-för-en-normalfördelning---tabell",
    "href": "F3_anteckningar.html#fördelningsfunktion-för-en-normalfördelning---tabell",
    "title": "F3. Kontinuerliga slumpvariabler",
    "section": "Fördelningsfunktion för en normalfördelning - tabell",
    "text": "Fördelningsfunktion för en normalfördelning - tabell\nIstället för att beräkna integralen kan vi använda\n\ntabeller\nminiräknare/datorprogram\n\nI de fall vi har en tabell - hur gör man för alla möjliga värden på väntevärdet \\(\\mu\\) och variansen \\(\\sigma^2\\)?\nLösningen är att standardisera fördelningen"
  },
  {
    "objectID": "F3_anteckningar.html#standardiserad-normalfördelning-och-normalfördelning",
    "href": "F3_anteckningar.html#standardiserad-normalfördelning-och-normalfördelning",
    "title": "F3. Kontinuerliga slumpvariabler",
    "section": "Standardiserad normalfördelning och normalfördelning",
    "text": "Standardiserad normalfördelning och normalfördelning\nLåt \\(Z \\sim N(0,1)\\)\nDå är \\(X = \\mu + \\sigma \\cdot Z\\) också normalfördelad med väntevärde \\(\\mu\\) och varians \\(\\sigma^2\\), d.v.s. \\[X \\sim N(\\mu,\\sigma)\\]\n\n\n\n\n\n\nExempel. Normalfördelning\n\n\n\nLåt \\(X \\sim N(5,2)\\)\n\\[\\begin{split} & P(X \\geq 0)  = 1 - P(X &lt; 0) = 1 - P(X \\leq 0) = \\\\ & 1 - P(\\frac{X-5}{2} \\leq \\frac{0-5}{2}) = 1 - \\Phi(\\frac{-5}{2}) = \\\\ & 1 - (1-\\Phi(\\frac{5}{2})) \\end{split}\\]\n\n\n\n\n\n\n\n\n\nTentauppgift\n\n\n\nVikten hos en alpin skidåkare med utrustning anses normalfördelad med väntevärde 80 kg och varians 36 kg^2. Skidåkaren Kim åker ensam i karbinen. Vad är sannolikheten att hens vikt överstiger 90 kg?\nLåt \\(X = \\text{\"vikt i kg\"}\\)\nModell: \\(X \\sim N(80,6)\\)\n\\[\\begin{split} & P(X &gt; 90) = 1 - P(X \\leq 90) = \\\\ & 1 - P(\\frac{X-80}{6} \\leq \\frac{90-80}{6}) = 1 - \\Phi(\\frac{10}{6}) \\end{split}\\]"
  },
  {
    "objectID": "F3_anteckningar.html#exempel-på-kvantiler",
    "href": "F3_anteckningar.html#exempel-på-kvantiler",
    "title": "F3. Kontinuerliga slumpvariabler",
    "section": "Exempel på kvantiler",
    "text": "Exempel på kvantiler\n\nMedian – den kvantil som delar in fördelningen i två delar, med 50% sannolikhet i varje\nKvartiler – de kvantiler som delar in fördelningen i fyra delar som har lika stor sannolikhet:\n\nFörsta kvartilen (Q1)\nAndra kvartilen = Medianen\nTredje kvartilen (Q3)\n\nPercentil – den p:te percentilen är det värde för en slumpvariabel som är högre än p% av alla möjliga värden"
  },
  {
    "objectID": "F3_anteckningar.html#kvantiler-illustrerade-med-en-fördelningsfunktion",
    "href": "F3_anteckningar.html#kvantiler-illustrerade-med-en-fördelningsfunktion",
    "title": "F3. Kontinuerliga slumpvariabler",
    "section": "Kvantiler illustrerade med en fördelningsfunktion",
    "text": "Kvantiler illustrerade med en fördelningsfunktion"
  },
  {
    "objectID": "F3_anteckningar.html#kvantiler-illustrerade-med-en-täthetsfunktion",
    "href": "F3_anteckningar.html#kvantiler-illustrerade-med-en-täthetsfunktion",
    "title": "F3. Kontinuerliga slumpvariabler",
    "section": "Kvantiler illustrerade med en täthetsfunktion",
    "text": "Kvantiler illustrerade med en täthetsfunktion"
  },
  {
    "objectID": "F3_anteckningar.html#kvantiler-illustrerade-med-en-boxplot",
    "href": "F3_anteckningar.html#kvantiler-illustrerade-med-en-boxplot",
    "title": "F3. Kontinuerliga slumpvariabler",
    "section": "Kvantiler illustrerade med en boxplot",
    "text": "Kvantiler illustrerade med en boxplot"
  },
  {
    "objectID": "F2_anteckningar.html",
    "href": "F2_anteckningar.html",
    "title": "F2. Diskreta slumpvariabler",
    "section": "",
    "text": "En slumpvariabel beskriver ett slumpmässigt försök genom att ange en sannolikhet för alla utfall i utfallsrummet.\nEn slumpvariabel kallas även för stokastisk variabel (engleska random variable) och brukar betecknas med stora bokstäver, t.ex. \\(X\\).\n\n\n\n\n\n\nExempel: Tärningskast med en n-sidig tärning\n\n\n\n\\(X\\) = “antal prickar”\nSannolikheten för att \\(X=1\\) är \\(\\frac{1}{n}\\)\nKuriosa - det finns numer en tärning som flippas som ett mynt The First Dice You Flip Like A Coin\n\n\n\n\n\n\n\n\nExempel: Blodsockerhalten\n\n\n\n\\(X\\) = “blodsockerhalten (mmol/l) hos en slumpmässigt vald person ur en population”\n\n\n\n\nDet finns två olika sorters slumpvariabler:\n\nDiskreta s.v. antar specifika värden såsom heltal, naturliga tal eller vissa kategorier\nKontinuerliga s.v. antar reella tal\n\n\n\n\n\n\n\nÄr följande s.v. diskret eller kontinuerlig?\n\n\n\n\nMyntkast som ger krona eller klave\nAntal stjärnor i universum\nTiden (i sekunder) det tar för vinnaren att komma i mål i vasaloppet\nUppmätt vikt av en myra i milligram\n\n\n\n\n\n\nUtfallsrummet vid kast med mynt är {krona,klave}\nLåt oss definiera en diskret slumpvariabel som beskriver detta slumpförsök\n\\[X = \\left\\{ \\begin{array}{lr}\n        1 & \\text{om utfallet är krona}\\\\\n        0 & \\text{om utfallet är klave}\n        \\end{array}\\right.\\]\n\n\n\n\n\n\nTip\n\n\n\nEtt utfall för slumpvariabeln \\(X\\) betecknas med “lilla” \\(x\\)."
  },
  {
    "objectID": "F2_anteckningar.html#slumpvariabler---diskreta-och-kontinuerliga",
    "href": "F2_anteckningar.html#slumpvariabler---diskreta-och-kontinuerliga",
    "title": "F2. Diskreta slumpvariabler",
    "section": "",
    "text": "Det finns två olika sorters slumpvariabler:\n\nDiskreta s.v. antar specifika värden såsom heltal, naturliga tal eller vissa kategorier\nKontinuerliga s.v. antar reella tal\n\n\n\n\n\n\n\nÄr följande s.v. diskret eller kontinuerlig?\n\n\n\n\nMyntkast som ger krona eller klave\nAntal stjärnor i universum\nTiden (i sekunder) det tar för vinnaren att komma i mål i vasaloppet\nUppmätt vikt av en myra i milligram"
  },
  {
    "objectID": "F2_anteckningar.html#från-kvalitativ-till-kvantitativ-beskrivning-av-en-händelse",
    "href": "F2_anteckningar.html#från-kvalitativ-till-kvantitativ-beskrivning-av-en-händelse",
    "title": "F2. Diskreta slumpvariabler",
    "section": "",
    "text": "Utfallsrummet vid kast med mynt är {krona,klave}\nLåt oss definiera en diskret slumpvariabel som beskriver detta slumpförsök\n\\[X = \\left\\{ \\begin{array}{lr}\n        1 & \\text{om utfallet är krona}\\\\\n        0 & \\text{om utfallet är klave}\n        \\end{array}\\right.\\]\n\n\n\n\n\n\nTip\n\n\n\nEtt utfall för slumpvariabeln \\(X\\) betecknas med “lilla” \\(x\\)."
  },
  {
    "objectID": "F2_anteckningar.html#simulering-av-kast-med-tärning",
    "href": "F2_anteckningar.html#simulering-av-kast-med-tärning",
    "title": "F2. Diskreta slumpvariabler",
    "section": "Simulering av kast med tärning",
    "text": "Simulering av kast med tärning"
  },
  {
    "objectID": "F2_anteckningar.html#krav-på-poissonfördelning",
    "href": "F2_anteckningar.html#krav-på-poissonfördelning",
    "title": "F2. Diskreta slumpvariabler",
    "section": "Krav på Poissonfördelning",
    "text": "Krav på Poissonfördelning\nFör att en slumpvariabel ska anses följa en Poissonfördelning måste följande gälla:\n\nDet sker i snitt lika många händelser per tidsenhet\nAntal händelser i icke överlappande intervall är oberoende\nTvå händelser kan inte hända samtidigt\n\n\n\n\n\n\n\nVulkanutbrott\n\n\n\nDet är tveksamt om antal vulkanutbrott under 100 år stämmer på dessa krav - vilket?"
  },
  {
    "objectID": "F2_anteckningar.html#poisson-fördelning---sannolikhetsfunktion",
    "href": "F2_anteckningar.html#poisson-fördelning---sannolikhetsfunktion",
    "title": "F2. Diskreta slumpvariabler",
    "section": "Poisson-fördelning - Sannolikhetsfunktion",
    "text": "Poisson-fördelning - Sannolikhetsfunktion\n\\(X \\sim Po(\\lambda)\\) (symbolen \\(\\sim\\) (“tilde”) och det som följer läses som “är fördelad som en Poissonfördelning”)\n\\(\\lambda\\) (grekisk bokstav “lambda”) är parameter i sannolikhetsfunktionen\n\\(f_X(x) = P(X = x)=\\frac{e^{-\\lambda}\\cdot \\lambda^x}{x!}\\) där utfallsrummet består av alla icke-negativa heltal \\(x \\in \\{0,1,2,3,...\\}\\)\nPoisson distribution on wikipedia"
  },
  {
    "objectID": "F2_anteckningar.html#poissonfördelning-1",
    "href": "F2_anteckningar.html#poissonfördelning-1",
    "title": "F2. Diskreta slumpvariabler",
    "section": "Poissonfördelning",
    "text": "Poissonfördelning\n\n\n\n\n\n\nExempel. Poissonfördelning\n\n\n\nFör ett antal år sedan slog en hudläkare larm att i ett område i Lund, beläget i närheten av en kemisk industri, var antalet fall av en sällsynt cancersjukdom ovanligt stort. I det aktuella området hade nio personer (sex kvinnor och tre män) drabbats av sjukdomen under en femårsperiod. Då läkaren studerade det rikstäckande cancerregistret såg han att i en population like astor som den i det aktuella området borde man under denna femårsperiod förväntat sig att antalet sjukdomsfall skulle vara fyra.\nModell: \\(X=\\) “antalet sjuka i området under femårsperioden\n\\(X\\sim Po(\\lambda=4)\\)\n\\(P(\\text{precis x sjuka}) = P(X=x) = f(x) =  \\frac{e^{-4}\\cdot 4^x}{x!}\\)\n\n\n\n\n\n\n\n\n\n\nVad är sannolikheten att man observerar precis 5 sjuka?\nVad är sannolikheten att man observerar högst 2 sjuka?\nVad är sannolikheten att man observerar 9 eller fler sjuka?\n\n\nExempel. Poissonfördelning\n\nVad är sannolikheten att man observerar precis 5 sjuka?\n\n\n\n\n\n\n\n\n\n\n\n\nExempel. Poissonfördelning\n\nVad är sannolikheten att man observerar högst 2 sjuka?\n\n\\(P(X\\leq 2) = P(X=0) + P(X = 1) + P(X = 2)\\)\n\n\n\n\n\n\n\n\n\n\n\nExempel. Poissonfördelning\n\nVad är sannolikheten att man observerar 9 eller fler sjuka?\n\n\\(P(X\\geq 9) = P(X=9) + P(X = 10) + ....\\)\nkan också skrivas som\n\\(P(X\\geq 9) = 1 - P(X \\leq 8)\\)"
  },
  {
    "objectID": "F2_anteckningar.html#ta-fram-värdet-på-sannolikhet-med-hjälp-av-sannolikhetstabell",
    "href": "F2_anteckningar.html#ta-fram-värdet-på-sannolikhet-med-hjälp-av-sannolikhetstabell",
    "title": "F2. Diskreta slumpvariabler",
    "section": "Ta fram värdet på sannolikhet med hjälp av sannolikhetstabell",
    "text": "Ta fram värdet på sannolikhet med hjälp av sannolikhetstabell\nTabell för fördelningsfunktionen för olika parametervärden.\n\\(P(X \\leq 8) = 0.97864\\)\n\\(P(X \\geq 9) = 1- P(X \\leq 8) = 1 - 0.97864 = 0.02136\\)"
  },
  {
    "objectID": "F2_anteckningar.html#binomialfördelning---försök-till-härledning-av-sannolikhetsfunktionen",
    "href": "F2_anteckningar.html#binomialfördelning---försök-till-härledning-av-sannolikhetsfunktionen",
    "title": "F2. Diskreta slumpvariabler",
    "section": "Binomialfördelning - försök till härledning av sannolikhetsfunktionen",
    "text": "Binomialfördelning - försök till härledning av sannolikhetsfunktionen\nVi gör \\(n=10\\) försök där sannolikheten att lyckas i ett försök är \\(p\\)\n\\(X=\\) “antal lyckade försök”\n\\(P(X=0) = P(\\text{misslyckats i 10 försök}) = (1-p)(1-p)\\cdots (1-p) = (1-p)^{10}\\)\n\\(P(X=1)\\)?\n\\(X=1\\) motsvarar att lyckas i ett försök och misslyckas i nio försök, där ordnigen inte spelar någon roll\nlyckas i första försöket: \\(p(1-p)\\dots (1-p) = p(1-p)^{9}\\)\ndetta kan ske på 10 sätt, vilket ger \\(P(X=1) = 10\\cdot p(1-p)^{9}\\)\nPå samma sätt: \\(P(X=2) = \\frac{10\\cdot 9}{2}\\cdot p^2(1-p)^8\\)"
  },
  {
    "objectID": "F2_anteckningar.html#binomialfördelning---sannolikhetsfunktionen",
    "href": "F2_anteckningar.html#binomialfördelning---sannolikhetsfunktionen",
    "title": "F2. Diskreta slumpvariabler",
    "section": "Binomialfördelning - sannolikhetsfunktionen",
    "text": "Binomialfördelning - sannolikhetsfunktionen\n\\(X \\sim Bin(n,p)\\)\nParametrarna är \\(n\\), antal försök, och \\(p\\), sannolikheten att “lyckas” i ett försök.\n\\(f_X(x) = P(X = x)== \\frac{n!}{x!(n-x)!}p^x\\cdot (1-p)^{n-x}\\) där utfallsrummet består av heltal mellan 0 och \\(n\\), d.v.s. \\(x \\in \\{0,1,2,3,...,n\\}\\)\nBinomial distribution on wikipedia"
  },
  {
    "objectID": "F2_anteckningar.html#binomialfördelning-1",
    "href": "F2_anteckningar.html#binomialfördelning-1",
    "title": "F2. Diskreta slumpvariabler",
    "section": "Binomialfördelning",
    "text": "Binomialfördelning\n\n\n\n\n\n\nExempel: Vi kastar en fyr-sidig tärning 7 gånger\n\n\n\n\\(X=\\) “antal tärningar som visar en 1:a”\n\\(X \\sim Bin(n,p)\\) där \\(n=7\\) och \\(p=\\frac{1}{4}\\)\n\n\n\n\n\n\nVad är sannolikheten att få fyra 1:or?\n\n\\(P(X=4) = \\binom{n}{x}p^x\\cdot (1-p)^{n-x} = \\binom{7}{4}\\frac{1}{4}^4\\cdot (1-\\frac{1}{4})^{3}\\)\n\nVad är sannolikheten att få högst fyra 1:or?\n\n\\(P(X\\leq 4) = \\sum_{x = 0}^4 P(X=x) = \\sum_{x = 0}^4 \\binom{7}{x}\\frac{1}{4}^{x}\\cdot (\\frac{3}{4})^{7-x}\\)\nJobbigt att räkna ut"
  },
  {
    "objectID": "F2_anteckningar.html#ta-fram-värdet-på-sannolikhet-med-hjälp-av-sannolikhetstabell-1",
    "href": "F2_anteckningar.html#ta-fram-värdet-på-sannolikhet-med-hjälp-av-sannolikhetstabell-1",
    "title": "F2. Diskreta slumpvariabler",
    "section": "Ta fram värdet på sannolikhet med hjälp av sannolikhetstabell",
    "text": "Ta fram värdet på sannolikhet med hjälp av sannolikhetstabell\nTabell för fördelningsfunktionen för olika parametervärden.\n\\(P(X\\leq 4) = 0.98712\\)"
  },
  {
    "objectID": "F2_anteckningar.html#väntevärde-för-en-poisson-fördelning",
    "href": "F2_anteckningar.html#väntevärde-för-en-poisson-fördelning",
    "title": "F2. Diskreta slumpvariabler",
    "section": "Väntevärde för en Poisson-fördelning",
    "text": "Väntevärde för en Poisson-fördelning\n\\(X \\sim Po(\\lambda)\\)\n\\(E(X) = \\lambda\\)\nI formelsamlingen heter parametern för Poissonfördelningen \\(\\mu\\)"
  },
  {
    "objectID": "F2_anteckningar.html#väntevärde-för-en-binomialfördelning",
    "href": "F2_anteckningar.html#väntevärde-för-en-binomialfördelning",
    "title": "F2. Diskreta slumpvariabler",
    "section": "Väntevärde för en Binomialfördelning",
    "text": "Väntevärde för en Binomialfördelning\n\\(X \\sim Bin(n,p)\\)\n\\(E(X) = n\\cdot p\\)"
  },
  {
    "objectID": "F1_anteckningar.html",
    "href": "F1_anteckningar.html",
    "title": "F1. Grundläggande begrepp och regler inom sannolikhetslära",
    "section": "",
    "text": "Ett “försök” vars utfall vi inte exakt kan förutsäga\nA “trial” for which the outcome is unknown"
  },
  {
    "objectID": "F1_anteckningar.html#union-och-snitt",
    "href": "F1_anteckningar.html#union-och-snitt",
    "title": "F1. Grundläggande begrepp och regler inom sannolikhetslära",
    "section": "Union och snitt",
    "text": "Union och snitt\nSnitt: \\(A \\cap B\\) betyder A och B\nUnion: \\(A \\cup B\\) betyder A eller B"
  },
  {
    "objectID": "F1_anteckningar.html#venn-diagram",
    "href": "F1_anteckningar.html#venn-diagram",
    "title": "F1. Grundläggande begrepp och regler inom sannolikhetslära",
    "section": "Venn-diagram",
    "text": "Venn-diagram"
  },
  {
    "objectID": "F1_anteckningar.html#komplementhändelse",
    "href": "F1_anteckningar.html#komplementhändelse",
    "title": "F1. Grundläggande begrepp och regler inom sannolikhetslära",
    "section": "Komplementhändelse",
    "text": "Komplementhändelse\n\\(P(A) + P(\\bar{A}) = 1\\)\n\\(P(\\bar{A}) = 1 - P(A)\\)\n\n\n\n\n\n\nExempel: Komplementhändelse\n\n\n\nA = “Tärning visar sex prickar” \\(\\bar{A}\\) = “Tärning visar inte sex prickar”\n\\(P(A) = \\frac{1}{6}\\)\n\\(P(\\bar{A}) = 1 - P(A) = 1 - \\frac{1}{6} = \\frac{5}{6}\\)"
  },
  {
    "objectID": "F1_anteckningar.html#additionssatsen-för-uteslutande-händelser",
    "href": "F1_anteckningar.html#additionssatsen-för-uteslutande-händelser",
    "title": "F1. Grundläggande begrepp och regler inom sannolikhetslära",
    "section": "Additionssatsen för uteslutande händelser",
    "text": "Additionssatsen för uteslutande händelser\n\\(P(A \\cup B) = P(A) + P(B)\\) eftersom \\(P(A \\cap B) = 0\\)"
  },
  {
    "objectID": "F11_anteckningar.html",
    "href": "F11_anteckningar.html",
    "title": "F11. Variansanalys och statistiska modeller i praktiken",
    "section": "",
    "text": "I en variansanalys testas nollhypotesen att tre eller fler populationer har samma medelvärde\n\nModell: Vi har \\(r\\) oberoende slumpvariabler \\(Y_1, Y_2, \\dots, Y_r\\) med väntevärden \\(E(Y_i)=\\mu_i\\) och lika varians \\(V(Y_i)=\\sigma^2\\)\nHypoteser: \\(H_0: \\mu_1 = \\mu_2 = \\dots = \\mu_r\\)\n\\(H_1: \\text{Minst två av väntevärderna är olika}\\)\n\nDesto större skillnad i medelvärde som råder mellan olika populationer med samma varians, desto större kommer variationen mellan populationerna att vara jämfört med variationen inom populationerna.\n\nAntagande: \\(Y_1, Y_2, \\dots, Y_r\\) är normalfördelade.\nTestregel:\nTeststorhet \\(F = \\frac{\\text{\"skattning av varians mellan grupper\"}}{\\text{\"skattning av varians inom grupper\"}} = \\frac{\\frac{SS_{Mellan}}{r-1}}{\\frac{SS_{Inom}}{n-r}}\\)\nUnder \\(H_0\\) är \\(F = \\frac{\\frac{SS_{Mellan}}{r-1}}{\\frac{SS_{Inom}}{n-r}} \\sim F(r-1,n-r)\\).\nFörkasta \\(H_0\\) om \\(F &gt; F_{\\alpha}(r-1,n-r)\\)\n\n\n\n\n\n\nSimuleringsexempel\n\n\n\nHär simulerar vi stickprov med 10 värden i varje från tre grupper (populationer) som har samma väntvärde och lika varians. Det är nyttigt att se hur data kan se ut under nollhypotesen.\n\nr = 3\nni = c(10,10,10)\nsigma = 1\nmui = c(0,0,0)\ni=1\ny1 &lt;- rnorm(ni[i],mui[i],sigma)\ni=2\ny2 &lt;- rnorm(ni[i],mui[i],sigma)\ni=3\ny3 &lt;- rnorm(ni[i],mui[i],sigma)\ndf &lt;- data.frame(y = c(y1,y2,y3), grupp = c(rep(\"A\",ni[1]),rep(\"B\",ni[2]),rep(\"C\",ni[3])))\n\n\nggplot(df,aes(y=y,x=grupp, col = grupp)) +\n  #geom_boxplot() +\n  geom_violin(draw_quantiles = c(0.25, 0.5, 0.75)) +\n  geom_jitter(width = 0.1) +\n  ggtitle(\"Tre grupper \")\n\n\n\n\n\n\n\n\n\n\n\n\nVariansanalys tar sin utgångspunkt i att man delar upp variation utifrån var den kommer ifrån. För ensidig variansanalys delar man upp variationen i variation mellan grupper och inom grupper. Detta görs genom att skapa kvadratsummer:\n\\[SS_{Total} = SS_{Mellan} + SS_{Inom}\\]\n\n\n\n\\[SS_{Total} = \\sum_{i=1}^{r}\\sum_{j=1}^{n_i}(y_{ij}-\\bar{y}_{..})^2\\]\n\n\n\n\n\n\nSimuleringsexempel (forts.)\n\n\n\nHär beräknar vi kvadratsumman för total varians på två sätt\n\n(n = nrow(df))\n\n[1] 30\n\n(m = mean(df$y))\n\n[1] 0.08792055\n\n(ss_tot = sum((df$y-m)^2))\n\n[1] 25.57705\n\n\ndär vi skattar variansen genom att dela kvadratsumman med lämpligt antal frihetsgrader\n\nss_tot/(n-1)\n\n[1] 0.8819673\n\n\nVi konstaterar att detta ger samma värde som stickprovsvariansen.\n\nvar(df$y)\n\n[1] 0.8819673\n\n\nVilket betyder att om man känner till den, så kan man härleda kvadratsumman för den totala variationen\n\nvar(df$y)*(n-1)\n\n[1] 25.57705\n\n\n\n\n\n\n\n\\[SS_{Inom} = \\sum_{i=1}^{r}\\sum_{j=1}^{n_i}(y_{ij}-\\bar{y}_{i.})^2 = \\sum_{j=1}^{n_1} (y_{1j}-\\bar{y}_{1.})^2+\\sum_{j=1}^{n_2} (y_{2j}-\\bar{y}_{2.})^2+\\dots \\sum_{j=1}^{n_r} (y_{rj}-\\bar{y}_{r.})^2\\]\n\n\n\n\n\n\nSimuleringsexempel (forts.)\n\n\n\nHär beräknar vi först medelvärden inom grupper\n\n(ma = mean(df$y[df$grupp==\"A\"]))\n\n[1] -0.1144672\n\n(mb = mean(df$y[df$grupp==\"B\"]))\n\n[1] 0.5635954\n\n(mc = mean(df$y[df$grupp==\"C\"]))\n\n[1] -0.1853665\n\n\ndärefter kvadratsumman för variation inom grupper som en poolad skattning av variansen\n\n(ss_inom = (sum((df$y[df$grupp==\"A\"]-ma)^2)+sum((df$y[df$grupp==\"B\"]-mb)^2)+sum((df$y[df$grupp==\"C\"]-mc)^2)))\n\n[1] 22.15792\n\n\nVi kan skatta variansen genom att dela kvadratsumman med lämpligt antal frihetsgrader\n\nss_inom/(n-r)\n\n[1] 0.8206637\n\n\n\n\n\n\n\n\\[SS_{Mellan} = \\sum_{i=1}^{r} n_i (\\bar{y}_{i.}-\\bar{y}_{..})^2\\] Ett enkelt sätt att beräkna den sista kvadratsumman är \\(SS_{Mellan} = SS_{Total}-SS_{Inom}\\)\n\n\n\n\n\n\nSimuleringsexempel (forts.)\n\n\n\n\n(ss_mellan = ni[1]*(ma - m)^2 + ni[2]*(mb - m)^2 +ni[3]*(mc - m)^2)\n\n[1] 3.419132\n\nss_tot - ss_inom\n\n[1] 3.419132\n\n\nÄven här kan vi skatta variansen genom att dela kvadratsumman med lämpligt antal frihetsgrader\n\nss_mellan/(r-1)\n\n[1] 1.709566\n\n\n\n\n\n\n\nUnder \\(H_0\\) att alla grupper har lika väntevärden och därmed lika stor varians inom som mellan grupper är teststorheten \\(F = \\frac{\\frac{SS_{Mellan}}{r-1}}{\\frac{SS_{Inom}}{n-r}} \\sim F(r-1,n-r)\\).\n\n\n\n\n\n\nSimuleringsexempel (forts.)\n\n\n\nVi beräknar teststorheten som\n\n((ss_mellan/(r-1))/(ss_inom/(n-r)))\n\n[1] 2.083151\n\n\nDen jämförs med kvantil\n\nqf(1-0.05,r-1,n-r)\n\n[1] 3.354131\n\n\n\\(H_0\\) kan inte förkastas på vald signifikansnivå 0.05\n\n\n\n\n\nLåt oss göra hypotetiska upprepade observationer och beräkna teststorheten många gånger.\n\nteststorhet &lt;- replicate(1000,{\nr = 3\nni = c(10,10,10)\nsigma = 1\nmui = c(0,0,0)\ni=1\ny1 &lt;- rnorm(ni[i],mui[i],sigma)\ni=2\ny2 &lt;- rnorm(ni[i],mui[i],sigma)\ni=3\ny3 &lt;- rnorm(ni[i],mui[i],sigma)\ndf_sim &lt;- data.frame(y = c(y1,y2,y3), grupp = c(rep(\"A\",ni[1]),rep(\"B\",ni[2]),rep(\"C\",ni[3])))\nn = nrow(df_sim)\nm = mean(df_sim$y)\nss_tot = sum((df_sim$y-m)^2)\nma = mean(df_sim$y[df_sim$grupp==\"A\"])\nmb = mean(df_sim$y[df_sim$grupp==\"B\"])\nmc = mean(df_sim$y[df_sim$grupp==\"C\"])\nss_inom = (sum((df_sim$y[df$grupp==\"A\"]-ma)^2)+sum((df_sim$y[df$grupp==\"B\"]-mb)^2)+sum((df_sim$y[df$grupp==\"C\"]-mc)^2))\nss_mellan = ss_tot - ss_inom\n(ss_mellan/(r-1))/(ss_inom/(n-r))\n#mod &lt;- lm(y~grupp,df)\n#anova(mod)$`F value`[1]\n})\n\nVi gör ett histogram av teststorheter och jämför med en F-fördelning med \\(r-1\\) och \\(n-r\\) frihetsgrader\n\nr = 3\nn = 30\nhist(teststorhet, probability = TRUE, breaks = 20)\nff = seq(0,max(teststorhet),length.out = 200)\ndd = df(ff,r-1,n-r)\nlines(ff,dd,col='blue')\n\n\n\n\n\n\n\n\nKvantilen vi jämför med markerar gränsen för det kritiska området där vi ska förkasta nollhypotesen.\n\n\n\n\n\n\n\n\n\n\n\n\nI R kan man få fram kvadratsummor genom att använda funktionerna lm och anova. De ger även p-värden för F-testet.\n\nmod &lt;- lm(y~grupp,df)\nanova(mod)\n\nAnalysis of Variance Table\n\nResponse: y\n          Df  Sum Sq Mean Sq F value Pr(&gt;F)\ngrupp      2  3.4191 1.70957  2.0832 0.1441\nResiduals 27 22.1579 0.82066"
  },
  {
    "objectID": "F11_anteckningar.html#introduktion-av-kvadratsummor",
    "href": "F11_anteckningar.html#introduktion-av-kvadratsummor",
    "title": "F11. Variansanalys och statistiska modeller i praktiken",
    "section": "",
    "text": "Variansanalys tar sin utgångspunkt i att man delar upp variation utifrån var den kommer ifrån. För ensidig variansanalys delar man upp variationen i variation mellan grupper och inom grupper. Detta görs genom att skapa kvadratsummer:\n\\[SS_{Total} = SS_{Mellan} + SS_{Inom}\\]"
  },
  {
    "objectID": "F11_anteckningar.html#kvadratsumma-för-total-varians",
    "href": "F11_anteckningar.html#kvadratsumma-för-total-varians",
    "title": "F11. Variansanalys och statistiska modeller i praktiken",
    "section": "",
    "text": "\\[SS_{Total} = \\sum_{i=1}^{r}\\sum_{j=1}^{n_i}(y_{ij}-\\bar{y}_{..})^2\\]\n\n\n\n\n\n\nSimuleringsexempel (forts.)\n\n\n\nHär beräknar vi kvadratsumman för total varians på två sätt\n\n(n = nrow(df))\n\n[1] 30\n\n(m = mean(df$y))\n\n[1] 0.08792055\n\n(ss_tot = sum((df$y-m)^2))\n\n[1] 25.57705\n\n\ndär vi skattar variansen genom att dela kvadratsumman med lämpligt antal frihetsgrader\n\nss_tot/(n-1)\n\n[1] 0.8819673\n\n\nVi konstaterar att detta ger samma värde som stickprovsvariansen.\n\nvar(df$y)\n\n[1] 0.8819673\n\n\nVilket betyder att om man känner till den, så kan man härleda kvadratsumman för den totala variationen\n\nvar(df$y)*(n-1)\n\n[1] 25.57705"
  },
  {
    "objectID": "F11_anteckningar.html#kvadratsumma-för-varians-inom-grupper",
    "href": "F11_anteckningar.html#kvadratsumma-för-varians-inom-grupper",
    "title": "F11. Variansanalys och statistiska modeller i praktiken",
    "section": "",
    "text": "\\[SS_{Inom} = \\sum_{i=1}^{r}\\sum_{j=1}^{n_i}(y_{ij}-\\bar{y}_{i.})^2 = \\sum_{j=1}^{n_1} (y_{1j}-\\bar{y}_{1.})^2+\\sum_{j=1}^{n_2} (y_{2j}-\\bar{y}_{2.})^2+\\dots \\sum_{j=1}^{n_r} (y_{rj}-\\bar{y}_{r.})^2\\]\n\n\n\n\n\n\nSimuleringsexempel (forts.)\n\n\n\nHär beräknar vi först medelvärden inom grupper\n\n(ma = mean(df$y[df$grupp==\"A\"]))\n\n[1] -0.1144672\n\n(mb = mean(df$y[df$grupp==\"B\"]))\n\n[1] 0.5635954\n\n(mc = mean(df$y[df$grupp==\"C\"]))\n\n[1] -0.1853665\n\n\ndärefter kvadratsumman för variation inom grupper som en poolad skattning av variansen\n\n(ss_inom = (sum((df$y[df$grupp==\"A\"]-ma)^2)+sum((df$y[df$grupp==\"B\"]-mb)^2)+sum((df$y[df$grupp==\"C\"]-mc)^2)))\n\n[1] 22.15792\n\n\nVi kan skatta variansen genom att dela kvadratsumman med lämpligt antal frihetsgrader\n\nss_inom/(n-r)\n\n[1] 0.8206637"
  },
  {
    "objectID": "F11_anteckningar.html#kvadratsumma-för-varians-mellan-grupper",
    "href": "F11_anteckningar.html#kvadratsumma-för-varians-mellan-grupper",
    "title": "F11. Variansanalys och statistiska modeller i praktiken",
    "section": "",
    "text": "\\[SS_{Mellan} = \\sum_{i=1}^{r} n_i (\\bar{y}_{i.}-\\bar{y}_{..})^2\\] Ett enkelt sätt att beräkna den sista kvadratsumman är \\(SS_{Mellan} = SS_{Total}-SS_{Inom}\\)\n\n\n\n\n\n\nSimuleringsexempel (forts.)\n\n\n\n\n(ss_mellan = ni[1]*(ma - m)^2 + ni[2]*(mb - m)^2 +ni[3]*(mc - m)^2)\n\n[1] 3.419132\n\nss_tot - ss_inom\n\n[1] 3.419132\n\n\nÄven här kan vi skatta variansen genom att dela kvadratsumman med lämpligt antal frihetsgrader\n\nss_mellan/(r-1)\n\n[1] 1.709566"
  },
  {
    "objectID": "F11_anteckningar.html#teststorhet",
    "href": "F11_anteckningar.html#teststorhet",
    "title": "F11. Variansanalys och statistiska modeller i praktiken",
    "section": "",
    "text": "Under \\(H_0\\) att alla grupper har lika väntevärden och därmed lika stor varians inom som mellan grupper är teststorheten \\(F = \\frac{\\frac{SS_{Mellan}}{r-1}}{\\frac{SS_{Inom}}{n-r}} \\sim F(r-1,n-r)\\).\n\n\n\n\n\n\nSimuleringsexempel (forts.)\n\n\n\nVi beräknar teststorheten som\n\n((ss_mellan/(r-1))/(ss_inom/(n-r)))\n\n[1] 2.083151\n\n\nDen jämförs med kvantil\n\nqf(1-0.05,r-1,n-r)\n\n[1] 3.354131\n\n\n\\(H_0\\) kan inte förkastas på vald signifikansnivå 0.05"
  },
  {
    "objectID": "F11_anteckningar.html#samplingsfördelningen-för-teststorheten-i-variansanalys",
    "href": "F11_anteckningar.html#samplingsfördelningen-för-teststorheten-i-variansanalys",
    "title": "F11. Variansanalys och statistiska modeller i praktiken",
    "section": "",
    "text": "Låt oss göra hypotetiska upprepade observationer och beräkna teststorheten många gånger.\n\nteststorhet &lt;- replicate(1000,{\nr = 3\nni = c(10,10,10)\nsigma = 1\nmui = c(0,0,0)\ni=1\ny1 &lt;- rnorm(ni[i],mui[i],sigma)\ni=2\ny2 &lt;- rnorm(ni[i],mui[i],sigma)\ni=3\ny3 &lt;- rnorm(ni[i],mui[i],sigma)\ndf_sim &lt;- data.frame(y = c(y1,y2,y3), grupp = c(rep(\"A\",ni[1]),rep(\"B\",ni[2]),rep(\"C\",ni[3])))\nn = nrow(df_sim)\nm = mean(df_sim$y)\nss_tot = sum((df_sim$y-m)^2)\nma = mean(df_sim$y[df_sim$grupp==\"A\"])\nmb = mean(df_sim$y[df_sim$grupp==\"B\"])\nmc = mean(df_sim$y[df_sim$grupp==\"C\"])\nss_inom = (sum((df_sim$y[df$grupp==\"A\"]-ma)^2)+sum((df_sim$y[df$grupp==\"B\"]-mb)^2)+sum((df_sim$y[df$grupp==\"C\"]-mc)^2))\nss_mellan = ss_tot - ss_inom\n(ss_mellan/(r-1))/(ss_inom/(n-r))\n#mod &lt;- lm(y~grupp,df)\n#anova(mod)$`F value`[1]\n})\n\nVi gör ett histogram av teststorheter och jämför med en F-fördelning med \\(r-1\\) och \\(n-r\\) frihetsgrader\n\nr = 3\nn = 30\nhist(teststorhet, probability = TRUE, breaks = 20)\nff = seq(0,max(teststorhet),length.out = 200)\ndd = df(ff,r-1,n-r)\nlines(ff,dd,col='blue')\n\n\n\n\n\n\n\n\nKvantilen vi jämför med markerar gränsen för det kritiska området där vi ska förkasta nollhypotesen."
  },
  {
    "objectID": "F11_anteckningar.html#variansanalys-med-statistiskprogram",
    "href": "F11_anteckningar.html#variansanalys-med-statistiskprogram",
    "title": "F11. Variansanalys och statistiska modeller i praktiken",
    "section": "",
    "text": "I R kan man få fram kvadratsummor genom att använda funktionerna lm och anova. De ger även p-värden för F-testet.\n\nmod &lt;- lm(y~grupp,df)\nanova(mod)\n\nAnalysis of Variance Table\n\nResponse: y\n          Df  Sum Sq Mean Sq F value Pr(&gt;F)\ngrupp      2  3.4191 1.70957  2.0832 0.1441\nResiduals 27 22.1579 0.82066"
  },
  {
    "objectID": "F11_anteckningar.html#visualisera-skattade-väntevärden-för-grupper",
    "href": "F11_anteckningar.html#visualisera-skattade-väntevärden-för-grupper",
    "title": "F11. Variansanalys och statistiska modeller i praktiken",
    "section": "Visualisera skattade väntevärden för grupper",
    "text": "Visualisera skattade väntevärden för grupper\n\n(EMM.source &lt;- emmeans(mod, \"grupp\"))\n\n grupp emmean    SE df lower.CL upper.CL\n A        3.4 0.456  9     2.37     4.43\n B        6.1 0.456  9     5.07     7.13\n C        9.8 0.456  9     8.77    10.83\n\nConfidence level used: 0.95 \n\n\n\nggplot(data.frame(EMM.source),aes(y=emmean,x=grupp, col = grupp)) +\n  geom_point() +\n  geom_errorbar(aes(ymin=lower.CL, ymax=upper.CL, width = 0.5)) +\n  ggtitle(\"Urea i blod för tre dieter\") +\n  ylab(\"y\")"
  },
  {
    "objectID": "F11_anteckningar.html#fortsatta-tester",
    "href": "F11_anteckningar.html#fortsatta-tester",
    "title": "F11. Variansanalys och statistiska modeller i praktiken",
    "section": "Fortsatta tester",
    "text": "Fortsatta tester\nOm nollhypotesen om lika väntvärden förkastas, kan man gå vidare och undersöka mellan vilka grupper det finns en skillnad. Det finns flertalet tester för detta.\nEtt exempel är Tukeys test som används när man har lika många observationer i varje grupp. Detta test bildar konfidensintervall för alla parvisa skillnader, men delar upp signifikansnivån mellan paren där den totala konfidensgraden för intervallen är 95%.\n\nTukeyHSD(aov(mod), conf.level=.95)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = mod)\n\n$grupp\n    diff       lwr      upr     p adj\nB-A  2.7 0.8977681 4.502232 0.0060127\nC-A  6.4 4.5977681 8.202232 0.0000103\nC-B  3.7 1.8977681 5.502232 0.0007387\n\n\n\nDet är god sed att anpassa signifikansnivå till hur många test man gör, så kallad korrektion för multipla tester."
  },
  {
    "objectID": "F11_anteckningar.html#logistisk-regression",
    "href": "F11_anteckningar.html#logistisk-regression",
    "title": "F11. Variansanalys och statistiska modeller i praktiken",
    "section": "Logistisk regression",
    "text": "Logistisk regression\nNär man har observationer som är 0 eller 1 kan man formulera en modell där responsen är logaritmen av odds-kvoten\n\\[log(\\frac{P(Y_i = 1)}{P(Y_i=0)}) = \\beta_0 + \\beta_1 x_i\\]"
  },
  {
    "objectID": "F11_anteckningar.html#generaliserade-modeller",
    "href": "F11_anteckningar.html#generaliserade-modeller",
    "title": "F11. Variansanalys och statistiska modeller i praktiken",
    "section": "Generaliserade modeller",
    "text": "Generaliserade modeller\nI denna kurs är responsvariabeln kontinuerlig. Vi kan skriva enkel linjär regression som\n\\[Y_i\\sim N(\\beta_0 + \\beta_1 x_i, \\sigma)\\]\nDet är vanligt att man har observationer som är diskreta eller kategoriska variabler. Då är det lämpligt att använda generaliserade modeller för regression.\n\nPoissonregression - när man har observationer som är ett räknat antal\n\n\\[Y_i\\sim Po(\\mu_i)\\] där \\(log(\\mu_i) = \\beta_0+\\beta_1 x_i\\)\nDet finns flera möjliga val av sannolikhetsfördelning för responsvariabeln (ex \\(Po\\)) och val av funktion för att länka väntevärdet \\(\\mu_i\\) till en linjär prediktor \\(\\beta_0+\\beta_1 x_i\\).\nParametrar skattas vanligtvis som de värden som maximerar sannolikheten för observationerna (data) enligt modellen (på engelska maximum likelihood), i en del fall genom minska kvadratmetoden (OLS) eller med Bayesiansk inferens."
  },
  {
    "objectID": "F11_anteckningar.html#icke-linjära-modeller",
    "href": "F11_anteckningar.html#icke-linjära-modeller",
    "title": "F11. Variansanalys och statistiska modeller i praktiken",
    "section": "Icke-linjära modeller",
    "text": "Icke-linjära modeller\nIcke-linjära modeller används när det finns anledning att misstänka att sambandet ser ut på ett visst sätt, exempelvis ökar upp till en viss nivå."
  },
  {
    "objectID": "F11_anteckningar.html#hierariska-modeller",
    "href": "F11_anteckningar.html#hierariska-modeller",
    "title": "F11. Variansanalys och statistiska modeller i praktiken",
    "section": "Hierariska modeller",
    "text": "Hierariska modeller\nDet kan vara lämpligt att ange flera källor till variation i mätvärden. Hierarkiska modeller innehåller mer än en källa till slumpmässig variation.\nI enkel linjär regression har vi en variansterm\n\\[y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i\\] där \\(\\varepsilon_i \\sim N(0,\\sigma)\\)\nTill exempel skulle mävärden vara insamlade från olika lokaler som har olika intercept\n\\[y_{is} = \\beta_s + \\beta_1 x_i + \\varepsilon_i\\] där \\(\\beta_s \\sim N(0,\\sigma_s)\\) och \\(\\varepsilon_i \\sim N(0,\\sigma_\\varepsilon)\\)\nInterceptet \\(\\beta_s\\) har nu is sig själv en egen slumpmässig variation som beror på grupp \\(s\\) och kallas för en slumpmässig effect (på engelska random effect).\nLutningen \\(\\beta_1\\) är en fix effekt som är densamma oberoende av lokal.\nEn modell av detta slag kallas för linjär mixad modell."
  },
  {
    "objectID": "F10_anteckningar.html",
    "href": "F10_anteckningar.html",
    "title": "F10. Korrelations- och regressionsanalys",
    "section": "",
    "text": "(Teoretisk) kovarians för två slumpvariabler \\(X\\) och \\(Y\\)\n\n\\[\\sigma_{XY}=Cov(X,Y)=E[(X-E(X))(Y-E(Y))] = E(XY) - E(X)E(Y)\\]\n\n(Empirisk) kovarians för parade stickprov från \\(X\\) respektive \\(Y\\)\n\n\\[c_{xy}= \\frac{\\sum_{i=1}^n (x_i- \\bar{x})\n(y_i- \\bar{y})} {n-1}= \\frac{1}{n-1}\n\\left[\\sum_{i=1}^n x_i y_i- n \\bar{x} \\bar{y}\\right]\\]"
  },
  {
    "objectID": "F10_anteckningar.html#hypotestest-av-korrelation",
    "href": "F10_anteckningar.html#hypotestest-av-korrelation",
    "title": "F10. Korrelations- och regressionsanalys",
    "section": "Hypotestest av korrelation",
    "text": "Hypotestest av korrelation\nModell: Låt \\(X = \\text{\"täthet\"}\\) och \\(Y = \\text{\"sulfathalt\"}\\).\nHypoteser: \\(H_0: \\rho = 0\\) (inget linjärt samband)\n\\(H_1: \\rho \\neq 0\\) (något linjärt samband)\nAntagande: \\(X\\) och \\(Y\\) kommer från en bivariat normalfördelning\nTestregel: Teststorheten är \\(t=r\\sqrt{\\frac{n-2}{1-r^2}}\\). Samplingsfördelningen för \\(t\\) under \\(H_0\\) är en t-fördelning med \\(n-2\\) frihetsgrader. Förkasta \\(H_0\\) om \\(|t| &gt; t_{\\alpha/2}\\)"
  },
  {
    "objectID": "F10_anteckningar.html#korrelationstest-i-r",
    "href": "F10_anteckningar.html#korrelationstest-i-r",
    "title": "F10. Korrelations- och regressionsanalys",
    "section": "Korrelationstest i R",
    "text": "Korrelationstest i R\nI R använder vi funktionen cor.test som genererar teststorheten \\(t\\), frihetsgrader för t-fördelningen (df), p-värdet (d.v.s. sannolikheten att teststorheten antar värdet vi fått eller värre givet att nollhypotesen är sann), konfidensintervall för korrelationen \\(I_\\rho\\)\n\ncor.test(df_sam$density,df_sam$sulphates)\n\n\n    Pearson's product-moment correlation\n\ndata:  df_sam$density and df_sam$sulphates\nt = 1.4255, df = 28, p-value = 0.1651\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.1104979  0.5672370\nsample estimates:\n      cor \n0.2601267 \n\n\nMan kan specificera\n\ntyp av mothypotes\nsätt att skatta korrelationen (Pearson är förvald)\nkonfidensnivå \\(1-\\alpha\\)"
  },
  {
    "objectID": "F10_anteckningar.html#undersökning-av-antagande",
    "href": "F10_anteckningar.html#undersökning-av-antagande",
    "title": "F10. Korrelations- och regressionsanalys",
    "section": "Undersökning av antagande",
    "text": "Undersökning av antagande\nOm \\(X\\) och \\(Y\\) kommer från bivariat normalfördelning så är både \\(X\\) och \\(Y\\) normalfördelade.\n\n\n\nqqnorm(df_sam$sulphates)\nqqline(df_sam$sulphates)\n\n\n\n\n\n\n\n\n\nqqnorm(df_sam$density)\nqqline(df_sam$density)\n\n\n\n\n\n\n\n\n\nDet finns ingen stark anledning att inte anta en bivariat normalfördelning."
  },
  {
    "objectID": "F10_anteckningar.html#icke-parametriskt-alternativ",
    "href": "F10_anteckningar.html#icke-parametriskt-alternativ",
    "title": "F10. Korrelations- och regressionsanalys",
    "section": "Icke-parametriskt alternativ",
    "text": "Icke-parametriskt alternativ\nEtt alternativ som inte bygger på antagande om fördelning för \\(X\\) och \\(Y\\) är att göra ett rangkorrelationstest. Då beräknas korrelationen baserat på ranger av värden och man använder en annan teststorhet.\n\ncor.test(df_sam$density,df_sam$sulphates, method = \"spearman\")\n\nWarning in cor.test.default(df_sam$density, df_sam$sulphates, method =\n\"spearman\"): Cannot compute exact p-value with ties\n\n\n\n    Spearman's rank correlation rho\n\ndata:  df_sam$density and df_sam$sulphates\nS = 3790.4, p-value = 0.4081\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.1567625 \n\n\nLiksom förut är det icke-parametriska testet ett svagare test.\nDet tredje alternativet kendall - skattar korrelation på ett annat sätt."
  },
  {
    "objectID": "F10_anteckningar.html#specificering-av-modell-hypoteser-och-antagande",
    "href": "F10_anteckningar.html#specificering-av-modell-hypoteser-och-antagande",
    "title": "F10. Korrelations- och regressionsanalys",
    "section": "Specificering av modell, hypoteser och antagande",
    "text": "Specificering av modell, hypoteser och antagande\nSignfikansnivå: Vi väljer signifikansnivå \\(\\alpha = 0.05\\)\nModell: Låt \\(X = \\text{\"Kolesterol\"}\\) och \\(Y = \\text{\"Triglycerid\"}\\).\nHypoteser: \\(H_0: \\rho = 0\\) mot \\(H_1: \\rho \\neq 0\\)\nAntagande: \\(X\\) och \\(Y\\) kommer från en bivariat normalfördelning"
  },
  {
    "objectID": "F10_anteckningar.html#beräkning-av-kvadratsummor-och-produktsumma-för-skattning-av-korrelation",
    "href": "F10_anteckningar.html#beräkning-av-kvadratsummor-och-produktsumma-för-skattning-av-korrelation",
    "title": "F10. Korrelations- och regressionsanalys",
    "section": "Beräkning av kvadratsummor och produktsumma för skattning av korrelation",
    "text": "Beräkning av kvadratsummor och produktsumma för skattning av korrelation\nBaserat på 10 mätningar \\(\\{x_1,y_1\\},\\{x_2,y_2\\},\\dots,\\{x_{10},y_{10}\\}\\) skattar vi korrelationskoefficienten genom att först beräkna kvadratsummor:\n\\[SS_{xy} = 40.74423\\]\n\\[SS_{x} = 22.74341\\]\n\\[SS_{y} = 131.22289\\]\nDärefter skattar vi korrelationen \\(\\hat{\\rho}\\) med stickprovskorrelationskoefficienten \\[r_{xy}= \\frac{SS_{xy}}{\\sqrt{SS_x}\\sqrt{SS_y}} = 0.7458191 \\]"
  },
  {
    "objectID": "F10_anteckningar.html#jämförelse-av-teststorhet-med-kvantil",
    "href": "F10_anteckningar.html#jämförelse-av-teststorhet-med-kvantil",
    "title": "F10. Korrelations- och regressionsanalys",
    "section": "Jämförelse av teststorhet med kvantil",
    "text": "Jämförelse av teststorhet med kvantil\nEfter det, beräknar vi teststorheten:\n\\[t = 0.7458191\\sqrt{\\frac{10-2}{1-0.7458191^2}} = 3.166704\\]\nTestregel: Nollhypotesen \\(H_0\\) förkastas eftersom teststorheten \\(t&gt;t_{0.025}(10-2) = 2.3060041\\)"
  },
  {
    "objectID": "F10_anteckningar.html#undersökning-av-antagande-1",
    "href": "F10_anteckningar.html#undersökning-av-antagande-1",
    "title": "F10. Korrelations- och regressionsanalys",
    "section": "Undersökning av antagande",
    "text": "Undersökning av antagande\nInnan vi kan dra slutsats ska vi kolla att antagandet om normalfördelning för \\(X\\) och \\(Y\\) stämmer.\n\n\n\nqqnorm(df$x)\nqqline(df$x)\n\n\n\n\n\n\n\n\n\n\nqqnorm(df$y)\nqqline(df$y)\n\n\n\n\n\n\n\n\n\n\nUtifrån jämförelser av empiriska och teoretiska kvantiler finns det ingen anledning att ifrågasätta detta antagande."
  },
  {
    "objectID": "F10_anteckningar.html#slutsats",
    "href": "F10_anteckningar.html#slutsats",
    "title": "F10. Korrelations- och regressionsanalys",
    "section": "Slutsats",
    "text": "Slutsats\nEftersom vi förkastar nollhypotesen att korrelationen är noll och att antagande som ligger till grund för testet verkar stämma, drar vi slutasten att det finns ett linjärt samband Kolesterol och Triglycerid.\n\nNotera att ett statistiskt samband ej medför ett orsaksamband! Med andra ord, vi kan inte dra slutsatsen att nivån på kolesterol påverkar nivån av triglycerid eller tvärtom."
  },
  {
    "objectID": "F10_anteckningar.html#utskrifter-från-statistiskprogram",
    "href": "F10_anteckningar.html#utskrifter-från-statistiskprogram",
    "title": "F10. Korrelations- och regressionsanalys",
    "section": "Utskrifter från statistiskprogram",
    "text": "Utskrifter från statistiskprogram\nHär kan ni jämföra med vad man får när man använder funktionen cor.test\n\ncor.test(df$x,df$y)\n\n\n    Pearson's product-moment correlation\n\ndata:  df$x and df$y\nt = 3.1667, df = 8, p-value = 0.01326\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.2190612 0.9359399\nsample estimates:\n      cor \n0.7458191"
  },
  {
    "objectID": "F10_anteckningar.html#minsta-kvadratmetoden",
    "href": "F10_anteckningar.html#minsta-kvadratmetoden",
    "title": "F10. Korrelations- och regressionsanalys",
    "section": "Minsta kvadratmetoden",
    "text": "Minsta kvadratmetoden\nEtt sätt att bestämma vilka värden parametrarna ska ha är att minimera summan av de kvadratiska avstånden från linjen till varje datapunkt, Minsta kvadratmetoden (på engelska Ordinary Least Square (OLS) regression).\n\\[Q(\\beta_0,\\beta_1)=\\sum \\varepsilon_i^2 = \\sum (y_i - (\\beta_0 + \\beta_1 x_i))^2\\]\nVi vill hitta värden på \\(\\beta_0\\) och \\(\\beta_1\\) så att kvadratsumman \\(Q(\\beta_0,\\beta_1)\\) är så liten som möjligt!\nParametrarna skattas genom att\n\nBeräkna \\(\\bar{x}\\), \\(\\bar{y}\\), \\(SS_x\\), \\(SS_y\\) och \\(SS_{xy}\\)\nSkatta parametrarna enligt\n\n\\[\\hat{\\beta}_1=b_1 = \\frac{SS_{xy}}{SS_x}\\]\n\\[\\hat{\\beta}_0=b_0 = \\bar{y} - b_1 \\bar{x}\\]\nOm linjen skattas på detta sätt kan vi även skatta variationen runt linjen på följande sätt\n\\[\\hat{\\sigma}^2=s_e^2 = \\frac{\\sum (y_i - \\hat{y}_i)^2}{n-2} =\\frac{\\sum (y_i - (b_0+b_1x_i))^2}{n-2}  = \\frac{1}{n-2}\\left( SS_y - \\frac{(SS_{xy})^2}{SS_x}\\right)\\]\nVi kan även skatta variansen på skattningarna som\n\\[\\hat{V}(b_1) = \\frac{s_e^2}{SS_x}\\]\n\\[\\hat{V}(b_0) = s_e^2\\left(\\frac{1}{n}+\\frac{\\bar{x}^2}{SS_x}\\right)\\]"
  },
  {
    "objectID": "F10_anteckningar.html#inferens-vid-enkel-linjär-regression",
    "href": "F10_anteckningar.html#inferens-vid-enkel-linjär-regression",
    "title": "F10. Korrelations- och regressionsanalys",
    "section": "Inferens vid enkel linjär regression",
    "text": "Inferens vid enkel linjär regression\nDet är vanligt att vi vill dra slutsatser genom att ta fram konfidensintervall för\n\nparametrarna\n\n– lutningen på linjen \\(\\beta_1\\)\n– interceptet \\(\\beta_0\\) - mer ovanligt\n\nprognoser\n\n– det förväntade värdet på responsvariabeln givet ett visst värde på den förklarande variablen \\(\\beta_0 + \\beta_1 x_0\\)\n– en framtida observation (enskilt värde) av responsvariabeln givet ett visst värde på den förklarande variablen \\(y(x_0)\\)\n\n\n\n\n\n\nExempel. Ålder och vilopuls (forts.)\n\n\n\nVi gör ett hypotestest för att svara på om det finns ett linjärt samband mellan ålder och vilopuls\nHypoteser: \\(H_0: \\beta_1 = 0\\) (lutningen är noll, det finns inget linjärt samband)\n\\(H_1: \\beta_1 \\neq 0\\) (lutningen är inte noll, det går inte att utesluta ett linjärt samband)"
  },
  {
    "objectID": "F10_anteckningar.html#kvadrat--och-produktsummor-samt-stickprovsmedelvärden",
    "href": "F10_anteckningar.html#kvadrat--och-produktsummor-samt-stickprovsmedelvärden",
    "title": "F10. Korrelations- och regressionsanalys",
    "section": "Kvadrat- och produktsummor samt stickprovsmedelvärden",
    "text": "Kvadrat- och produktsummor samt stickprovsmedelvärden\nSå här räknar Ullrika ut sammanfattande mått för att kunna fortsätta. En del miniräknare har inbyggda funktioner för att utföra dessa beräkningar. Använd miniräknare eller R för att göra övningarna. Ni kommer få beräknade kvadrat- och produktsummor samt stickprovsmedelvärden på den skriftliga tentan.\n\ndf &lt;- data.frame(x=heart$Age, y = heart$RestBP)\nn = nrow(df)\nspxy = sum(df$x*df$y)-n*mean(df$x)*mean(df$y)\nssx = sum(df$x^2) - n*mean(df$x)^2\nssy = sum(df$y^2) - n*mean(df$y)^2\nm_y = mean(df$y)\nm_x = mean(df$x)\n\nb1 = spxy/ssx\nb0 = m_y-b1*m_x\nse2 = (ssy-(spxy)^2/ssx)/(n-2)"
  },
  {
    "objectID": "F10_anteckningar.html#konfidensintervall-för-lutningen",
    "href": "F10_anteckningar.html#konfidensintervall-för-lutningen",
    "title": "F10. Korrelations- och regressionsanalys",
    "section": "Konfidensintervall för lutningen",
    "text": "Konfidensintervall för lutningen\nFör att kunna testa om lutningen är skild från noll, ökar eller minskar, bildar vi ett konfidensintervall för lutningen.\nGivet modellen för linjär regression och om antagandet om oberoende och normalfördelade residualer med samma varians stämmer, kan vi använda en t-kvantil för att skapa konfidensintervall för lutningen på linjen\n\\[I_{\\beta_1} = b_1 \\pm t_{\\alpha/2}(n-2)\\sqrt{\\hat{V}(b_1)}\\]\n\\(b_1 = \\frac{SS_{xy}}{SS_x} = \\frac{1.3689261\\times 10^{4}}{2.467262\\times 10^{4}} = 0.555\\)\n\\(n=303\\)\n\\(\\hat{V}(b_1) = \\frac{s_e^2}{SS_x} = \\frac{285.5466516}{2.467262\\times 10^{4}} = 0.012\\)\nEtt 95%-igt konfidensintervall blir\n\nlb = b1-qt(1-0.025,n-2)*sqrt(se2/ssx) #lower bound\nub = b1+qt(1-0.025,n-2)*sqrt(se2/ssx) #upper bound\n\n\\(I_{\\beta_1} = 0.555 \\pm t_{0.025}(303-2)\\sqrt{0.012} = (0.343,0.767)\\)"
  },
  {
    "objectID": "F10_anteckningar.html#residualanalys---undersök-om-antagande-gäller",
    "href": "F10_anteckningar.html#residualanalys---undersök-om-antagande-gäller",
    "title": "F10. Korrelations- och regressionsanalys",
    "section": "Residualanalys - Undersök om antagande gäller",
    "text": "Residualanalys - Undersök om antagande gäller\nInnan vi kan dra någon slutsats behöver vi undersöka om modellen verkar rimlig. Är residualerna normalfördelade och oberoende? Man kan undersöka detta genom att rita residualer mot \\(x\\) samt undersöka om de följer en normalfördelning.\nFör att beräkna residualerna behöver jag skatta interceptet\n\\[b_0 = \\bar{y} - b_1 \\bar{x} = 131.7 - 0.6\\cdot 54.4 = 101.5\\]\ndär varje residual är\n\\[e_i = y_i - b_0 - b_1x_i\\]\n\nres = df$y - b0 - b1*df$x \n\n\n\n\nqqnorm(res)\nqqline(res)\n\n\n\n\n\n\n\n\n⇒ Det finns ingen anledning att inte anta normalfördelning\n\n\n# om oberoende och lika varians - inget mönster\nplot(df$x,res)\n\n\n\n\n\n\n\n\n⇒ Det finns ingen anledning att inte anta oberoende och lika varians"
  },
  {
    "objectID": "F10_anteckningar.html#dra-slutsats-från-hypotes-testet",
    "href": "F10_anteckningar.html#dra-slutsats-från-hypotes-testet",
    "title": "F10. Korrelations- och regressionsanalys",
    "section": "Dra slutsats från hypotes-testet",
    "text": "Dra slutsats från hypotes-testet\nEftersom intervallet \\(I_{\\beta_1}\\) inte täcker noll, förkastar vi nollhypotesen på signifikansnivå \\(\\alpha = 0.05\\)."
  },
  {
    "objectID": "F10_anteckningar.html#utskrifter-från-statistiskprogram-1",
    "href": "F10_anteckningar.html#utskrifter-från-statistiskprogram-1",
    "title": "F10. Korrelations- och regressionsanalys",
    "section": "Utskrifter från statistiskprogram",
    "text": "Utskrifter från statistiskprogram\nI R kan man göra linjär regression med funktionen lm. Funktionen summary genererar en utskrift av diverse saker, där man behöver förstå vad det är man behöver för att kunna göra den analys man vill göra.\n\nmod = lm(formula = RestBP ~ Age, data = heart)\nsummary(mod)\n\n\nCall:\nlm(formula = RestBP ~ Age, data = heart)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-38.659 -11.449  -0.904  10.218  67.444 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 101.4851     5.9364  17.095  &lt; 2e-16 ***\nAge           0.5548     0.1076   5.157 4.55e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.9 on 301 degrees of freedom\nMultiple R-squared:  0.08119,   Adjusted R-squared:  0.07814 \nF-statistic:  26.6 on 1 and 301 DF,  p-value: 4.547e-07\n\n\nKonfidensintervall beräknas med funktionen confint där man kan ange konfidensgrad (95% är förvald)\n\nconfint(mod)\n\n                 2.5 %    97.5 %\n(Intercept) 89.8028869 113.16727\nAge          0.3431323   0.76654\n\n\nMan kan också be programmet att rita upp residualer. När du gör detta, kommer du ut flera plottar med begrepp som vi inte går på djupet i inom denna kurs.\n\nplot(mod)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDu kan få fram residualer genom funktionen residuals och undersöka om residualerna är normalfördelade, oberoende och har lika varians på ditt eget sätt.\n\nres = residuals(mod)"
  },
  {
    "objectID": "F10_anteckningar.html#exempel.-lungkapacitet",
    "href": "F10_anteckningar.html#exempel.-lungkapacitet",
    "title": "F10. Korrelations- och regressionsanalys",
    "section": "Exempel. Lungkapacitet",
    "text": "Exempel. Lungkapacitet\nI en undersökning ville man studera hur lungkapaciteten (i liter, mätt med spirometer) påverkas av personers vikt (i kg). På 20 slumpmässigt utvalda kvinnor i åldern 17 till 19 år mättes de två variablerna.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPerson\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\nvikt\n54.4\n56.2\n49.0\n63.5\n60.8\n59.9\n62.6\n62.1\n52.2\n50.8\n\n\nlungkap.\n3.87\n3.26\n2.14\n4.13\n3.44\n2.78\n2.91\n3.33\n3.20\n2.17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPerson\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n\n\nvikt\n57.2\n48.1\n54.0\n50.8\n49.9\n46.3\n59.0\n56.2\n61.2\n53.1\n\n\nlungkap.\n3.13\n2.47\n3.03\n2.88\n2.65\n2.03\n3.21\n3.45\n3.61\n2.53"
  },
  {
    "objectID": "F10_anteckningar.html#intressanta-frågeställningar",
    "href": "F10_anteckningar.html#intressanta-frågeställningar",
    "title": "F10. Korrelations- och regressionsanalys",
    "section": "Intressanta frågeställningar",
    "text": "Intressanta frågeställningar\n\nFinns det ett samband mellan vikt och lungkapacitet?\nKan sambandet tänkas vara linjärt, d.v.s. om \\(Y = \\text{lungkapacitet}\\) och \\(X=\\text{vikt}\\), \\[y = \\beta_0+\\beta_1x\\]\nVad innebär det i modellbeskrivningen ovan om \\(\\beta_1=0\\)?\nHur mycket ökar lungkapaciteten i genomsnitt när vikten ökas med ett kilo?\nVilken är den genomsnittsliga lungkapaciteten för kvinnor som väger 60 kg - d.v.s. vad är den förväntade lungkapaciteten för kvinnor med vikt 60 kg?\nVilken lungkapacitetet kan en kvinna ha som väger 60 kg - d.v.s. vad skulle vi prediktera för lungkapacitet på en slumpmässigt vald kvinna?\nHur mycket av variationen i lungkapacitet förklaras av vikt?"
  },
  {
    "objectID": "F10_anteckningar.html#hypotestest---finns-det-ett-samband-mellan-vikt-och-lungkapacitet",
    "href": "F10_anteckningar.html#hypotestest---finns-det-ett-samband-mellan-vikt-och-lungkapacitet",
    "title": "F10. Korrelations- och regressionsanalys",
    "section": "Hypotestest - Finns det ett samband mellan vikt och lungkapacitet?",
    "text": "Hypotestest - Finns det ett samband mellan vikt och lungkapacitet?\nModell: \\(Y = \\text{lungkapacitet}\\) och \\(X=\\text{vikt}\\) och sambandet mellan dem är linjärt\n\\[y_i = \\beta_0+\\beta_1x_i+\\varepsilon_i\\]\nAntagande: Vi antar att residualerna är oberoende och likafördelade enligt \\(\\varepsilon_i \\sim N(0,\\sigma)\\)\nHypoteser: \\(H_0: \\beta_1 = 0\\) mot \\(H_1: \\beta_1 \\neq 0\\) testas med signifikansnivå \\(\\alpha = 0.05\\)"
  },
  {
    "objectID": "F10_anteckningar.html#beräkna-sammanfattande-mått-utifrån-stickproven",
    "href": "F10_anteckningar.html#beräkna-sammanfattande-mått-utifrån-stickproven",
    "title": "F10. Korrelations- och regressionsanalys",
    "section": "Beräkna sammanfattande mått utifrån stickproven",
    "text": "Beräkna sammanfattande mått utifrån stickproven\n\\(n = 20\\), \\(\\bar{x} = 55.365\\), \\(\\bar{y} = 55.365\\)\n\\(SS_{xy} = 41.2297\\), \\(SS_x = 530.9655\\), \\(SS_y = 6.14258\\)"
  },
  {
    "objectID": "F10_anteckningar.html#beräkna-konfidensintervall-för-lutningen",
    "href": "F10_anteckningar.html#beräkna-konfidensintervall-för-lutningen",
    "title": "F10. Korrelations- och regressionsanalys",
    "section": "Beräkna konfidensintervall för lutningen",
    "text": "Beräkna konfidensintervall för lutningen\n\\[I_{\\beta_1} = b_1 \\pm t_{0.025}(18)\\sqrt{\\frac{s_e^2}{SS_x}} = 0.078 \\pm 2.101\\sqrt{3.1\\times 10^{-4}} = (0.041,0.115) \\] Nollhypotesen förkastas på signifikansnivå \\(\\alpha=0.05\\) eftersom intervallet inte täcker noll.\n⇒ Det finns stöd för ett linjärt samband som säger att lungkapacitet ökar med vikt. Lungkapaciteten ökar genomsnitt 0.0776504 när vikten ökas med ett kilo?"
  },
  {
    "objectID": "F10_anteckningar.html#gör-prognoser",
    "href": "F10_anteckningar.html#gör-prognoser",
    "title": "F10. Korrelations- och regressionsanalys",
    "section": "Gör prognoser",
    "text": "Gör prognoser\n\nFörväntat värde av responsvariabeln\n\nVilken är den genomsnittsliga lungkapaciteten för kvinnor som väger \\(x_0=60\\) kg - d.v.s. vad är den förväntade lungkapaciteten för kvinnor med vikt 60 kg?\n\nDet förväntade värdet är \\(\\mu(60) = b_0+b_1\\cdot 60 = -1.29+0.08 \\cdot 60 = 3.37\\)\nEtt 95%-igt konfidensintervall för det förväntade värdet är\n\\[\\begin{split} & I_{\\beta_0+\\beta_1x_0} = b_0+b_1\\cdot 60 \\pm t_{0.025}(18)\\sqrt{s_e^2\\left(\\frac{1}{n}+\\frac{(x_0-\\bar{x})^2}{SS_x}\\right)} = \\\\ & -1.29+0.08 \\cdot 60 \\pm 2.101 \\sqrt{0.163\\left(\\frac{1}{20}+\\frac{(60-55.4)^2}{530.97}\\right)} = \\\\ & (3.12,4.47) \\end{split}\\]\n\n\nIllustration av konfidensintervall för förväntat värde\n\nmod = lm(y~x,data=df)\nnewdf = data.frame(x=seq(min(df$x),max(df$x),length.out=50))\nkonf = data.frame(predict(mod,newdata=newdf,interval = \"confidence\"))\n\nplot(y~x,data=df,ylim=c(1,5),xlab = 'vikt', ylab='lungkapacitet')\nlines(newdf$x,konf$fit,col='darkred')\nlines(newdf$x,konf$lwr,col = 'blue',lty=2)\nlines(newdf$x,konf$upr,col = 'blue',lty=2)\n\n\n\n\n\n\n\n\n\n\nEn framtida observation (eller enskilt värde) av responsvariabeln\n\nVilken lungkapacitetet kan en kvinna ha som väger 60 kg - d.v.s. vad skulle vi prediktera för lungkapacitet på en slumpmässigt vald kvinna?\n\nEtt 95%-igt konfidensintervall för prediktionen är\n\\[\\begin{split} & I_{\\beta_0+\\beta_1x_0+\\varepsilon} = b_0+b_1\\cdot 60 \\pm t_{0.025}(18)\\sqrt{s_e^2\\left(1+\\frac{1}{n}+\\frac{(x_0-\\bar{x})^2}{SS_x}\\right)} = \\\\ & -1.29+0.08 \\cdot 60 \\pm 2.101 \\sqrt{0.163\\left(1+\\frac{1}{20}+\\frac{(60-55.4)^2}{530.97}\\right)} = \\\\ & (2.48,4.74) \\end{split}\\]\n\nKonfidensintervallet för prediktionen (även kallad prediktionsintervallet) är bredare än konfidensintervallet för det förväntade värdet.\n\n\nTänk på att modellen gäller i intervallet för x-värden. Det innebär att prognoser har sämre tillförlitlighet för x-värden utanför detta intervall.\n\n\n\nIllustration av prediktions- och konfidensintervall för responsvariabeln\n\nmod = lm(y~x,data=df)\nnewdf = data.frame(x=seq(min(df$x),max(df$x),length.out=50))\nkonf = data.frame(predict(mod,newdata=newdf,interval = \"confidence\"))\n\npred = data.frame(predict(mod,newdata=newdf,interval = \"prediction\"))\n\nplot(y~x,data=df,ylim=c(1,5),xlab = 'vikt', ylab='lungkapacitet')\nlines(newdf$x,konf$fit,col='darkred')\nlines(newdf$x,konf$lwr,col = 'blue',lty = 2)\nlines(newdf$x,konf$upr,col = 'blue',lty = 2)\nlines(newdf$x,pred$lwr,lty = 2)\nlines(newdf$x,pred$upr,lty = 2)"
  },
  {
    "objectID": "F10_anteckningar.html#förklaringsgrad",
    "href": "F10_anteckningar.html#förklaringsgrad",
    "title": "F10. Korrelations- och regressionsanalys",
    "section": "Förklaringsgrad",
    "text": "Förklaringsgrad\n\nHur mycket av variationen i lungkapacitet förklaras av vikt?\n\nDeterminationskoefficienten (\\(R^2\\)) är en koefficient som anger hur stor del av variationerna i den beroende variabeln (\\(Y\\)) som kan förklaras av variationer i den oberoende variabeln (\\(X\\)) under förutsättning att sambandet mellan \\(X\\) och \\(Y\\) är linjärt.\n\\[R^2 = 1 - \\frac{\\sum (y_i-\\hat{y}_i)^2}{\\sum (y_i-\\bar{y})^2} = 52.1\\%\\]\ndär täljaren motsvarar icke-förklarad variation \\(\\sum (y_i-\\hat{y}_i)^2 = 2.941076\\) och nämnaren motsvarar total variation \\(\\sum (y_i-\\bar{y})^2 = 6.14258\\)\nI detta fall förklarar vikt \\(52.1\\%\\) av variationen i lungkapacitet.\n\nFörklaringsgraden är samma sak som kvadraten på korrelationskoefficienten när modellen är linjär d.v.s. \\(R^2 = r^2\\)\n\n\nFörklaringsgraden säger inget om hur bra modellen är. Man bör alltid visualisera modellen och data."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "exponential.html",
    "href": "exponential.html",
    "title": "Exponential-fördelning",
    "section": "",
    "text": "I kursen tar vi ibland upp en så kallad exponentialfördelning. Den har en parameter, \\(\\lambda\\).\n\n\nOm \\(X\\sim Exp(\\lambda)\\) så är väntevärdet \\(E(X) = \\frac{1}{\\lambda}\\)\n\n\n\nDet finns ingen tabell för exponentialfördelningen eftersom fördelningsfunktion är väldigt enkel att beräkna.\n\\(F(x) = P(X \\leq x) = 1 - e^{-\\lambda x}\\)\n\n\n\nMan kan skatta parametern med stickprovsmedelvärdet som följer\n\\(\\hat{\\lambda} = \\frac{1}{\\bar{x}}\\)"
  },
  {
    "objectID": "exponential.html#väntevärde",
    "href": "exponential.html#väntevärde",
    "title": "Exponential-fördelning",
    "section": "",
    "text": "Om \\(X\\sim Exp(\\lambda)\\) så är väntevärdet \\(E(X) = \\frac{1}{\\lambda}\\)"
  },
  {
    "objectID": "exponential.html#fördelningsfunktion",
    "href": "exponential.html#fördelningsfunktion",
    "title": "Exponential-fördelning",
    "section": "",
    "text": "Det finns ingen tabell för exponentialfördelningen eftersom fördelningsfunktion är väldigt enkel att beräkna.\n\\(F(x) = P(X \\leq x) = 1 - e^{-\\lambda x}\\)"
  },
  {
    "objectID": "exponential.html#skattning-av-parametern",
    "href": "exponential.html#skattning-av-parametern",
    "title": "Exponential-fördelning",
    "section": "",
    "text": "Man kan skatta parametern med stickprovsmedelvärdet som följer\n\\(\\hat{\\lambda} = \\frac{1}{\\bar{x}}\\)"
  },
  {
    "objectID": "F10_notes_ENG.html",
    "href": "F10_notes_ENG.html",
    "title": "F10. Correlation and regression analyses",
    "section": "",
    "text": "(Theoretical) covariance for two random variables \\(X\\) and \\(Y\\)\n\n\\[\\sigma_{XY}=Cov(X,Y)=E[(X-E(X))(Y-E(Y))] = E(XY) - E(X)E(Y)\\]\n\n(Empirical) covariance for paired random samples from \\(X\\) and \\(Y\\)\n\n\\[c_{xy}= \\frac{\\sum_{i=1}^n (x_i- \\bar{x})\n(y_i- \\bar{y})} {n-1}= \\frac{1}{n-1}\n\\left[\\sum_{i=1}^n x_i y_i- n \\bar{x} \\bar{y}\\right]\\]"
  },
  {
    "objectID": "F10_notes_ENG.html#hypothesis-test-of-a-correlation",
    "href": "F10_notes_ENG.html#hypothesis-test-of-a-correlation",
    "title": "F10. Correlation and regression analyses",
    "section": "Hypothesis test of a correlation",
    "text": "Hypothesis test of a correlation\nModel: Let \\(X = \\text{\"density\"}\\) and \\(Y = \\text{\"sulfate level\"}\\).\nHypotheses: \\(H_0: \\rho = 0\\) (no linear association)\n\\(H_1: \\rho \\neq 0\\) (some linear association)\nAssumption: \\(X\\) and \\(Y\\) come from a bivariate normal distribution\nTest rule: The test quantity is \\(t=r\\sqrt{\\frac{n-2}{1-r^2}}\\). The sampling distribution for \\(t\\) under \\(H_0\\) is a t-distribution with \\(n-2\\) degrees of freedom. Reject \\(H_0\\) if \\(|t| &gt; t_{\\alpha/2}\\)\n\\[SS_{xy} = 0.0016239\\]\n\\[SS_{x} = 8.05954\\times 10^{-5}\\]\n\\[SS_{y} = 0.4835467\\]\nThen we estimate the correlation by the sample correlation \\[\\hat{\\rho}=r_{xy}= \\frac{SS_{xy}}{\\sqrt{SS_x}\\sqrt{SS_y}} = 0.26 \\]"
  },
  {
    "objectID": "F10_notes_ENG.html#comparison-of-a-test-quantity-to-a-quantile",
    "href": "F10_notes_ENG.html#comparison-of-a-test-quantity-to-a-quantile",
    "title": "F10. Correlation and regression analyses",
    "section": "Comparison of a test quantity to a quantile",
    "text": "Comparison of a test quantity to a quantile\nThen, we derive the test quantity\n\\[t = 0.26\\sqrt{\\frac{30-2}{1-0.26^2}} = 1.426\\]\nIf we reject \\(H_0\\) or not, depends if the test quantity is larger than the critical threshold set by the quantile \\(2.048\\)"
  },
  {
    "objectID": "F10_notes_ENG.html#correlation-test-in-r",
    "href": "F10_notes_ENG.html#correlation-test-in-r",
    "title": "F10. Correlation and regression analyses",
    "section": "Correlation test in R",
    "text": "Correlation test in R\nIn R we use the function cor.test generating the test quantity \\(t\\), degrees of freedom for the t-distribution (df), the p-value (i.e. the probability that the test quantity takes the value we got or worse given that the null hypothesis is true), confidence interval for the correlation \\(I_\\rho\\)\n\ncor.test(df_sam$density,df_sam$sulphates)\n\n\n    Pearson's product-moment correlation\n\ndata:  df_sam$density and df_sam$sulphates\nt = 1.4255, df = 28, p-value = 0.1651\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.1104979  0.5672370\nsample estimates:\n      cor \n0.2601267 \n\n\nOne can specify\n\ntype of alternative hypothesis\nmethod to estimate the correlation (Pearson is default)\nconfidence level \\(1-\\alpha\\)"
  },
  {
    "objectID": "F10_notes_ENG.html#examine-the-assumption",
    "href": "F10_notes_ENG.html#examine-the-assumption",
    "title": "F10. Correlation and regression analyses",
    "section": "Examine the assumption",
    "text": "Examine the assumption\nIf \\(X\\) and \\(Y\\) come from a bivariate normal distribution, then both \\(X\\) and \\(Y\\) are normally distributed.\n\n\n\nqqnorm(df_sam$sulphates)\nqqline(df_sam$sulphates)\n\n\n\n\n\n\n\n\n\nqqnorm(df_sam$density)\nqqline(df_sam$density)\n\n\n\n\n\n\n\n\n\nThere is no strong reason to not assume a bivariate normal distribution."
  },
  {
    "objectID": "F10_notes_ENG.html#non-parametric-alternatives",
    "href": "F10_notes_ENG.html#non-parametric-alternatives",
    "title": "F10. Correlation and regression analyses",
    "section": "Non-parametric alternatives",
    "text": "Non-parametric alternatives\nOne alternative that does not require an assumption about the distribution of \\(X\\) and \\(Y\\) is to do a rang correlation test. Then the correlation is based on ranks and one uses a different test statistic.\n\ncor.test(df_sam$density,df_sam$sulphates, method = \"spearman\")\n\nWarning in cor.test.default(df_sam$density, df_sam$sulphates, method =\n\"spearman\"): Cannot compute exact p-value with ties\n\n\n\n    Spearman's rank correlation rho\n\ndata:  df_sam$density and df_sam$sulphates\nS = 3790.4, p-value = 0.4081\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.1567625 \n\n\nAs before, the non-parametric test is a less powerful test.\nThe third alternative, Kendall - estimates the correlation in a different way than Spearman’s rang correlation."
  },
  {
    "objectID": "F10_notes_ENG.html#specifying-the-model-hypotheses-and-assumptions",
    "href": "F10_notes_ENG.html#specifying-the-model-hypotheses-and-assumptions",
    "title": "F10. Correlation and regression analyses",
    "section": "Specifying the model, hypotheses and assumptions",
    "text": "Specifying the model, hypotheses and assumptions\nSignificance level: We choose a significance level \\(\\alpha = 0.05\\)\nModel: Let \\(X = \\text{\"Cholesterol\"}\\) and \\(Y = \\text{\"Triglycerid\"}\\).\nHypotheses: \\(H_0: \\rho = 0\\) against \\(H_1: \\rho \\neq 0\\)\nAssumption: \\(X\\) and \\(Y\\) come from a bivariate normal distribution"
  },
  {
    "objectID": "F10_notes_ENG.html#derive-the-quadrat-and-product-sums-to-use-for-the-estimation-of-the-correlation",
    "href": "F10_notes_ENG.html#derive-the-quadrat-and-product-sums-to-use-for-the-estimation-of-the-correlation",
    "title": "F10. Correlation and regression analyses",
    "section": "Derive the quadrat and product sums to use for the estimation of the correlation",
    "text": "Derive the quadrat and product sums to use for the estimation of the correlation\nBased on 10 measurements \\(\\{x_1,y_1\\},\\{x_2,y_2\\},\\dots,\\{x_{10},y_{10}\\}\\) we estimate the correlation coefficient by calculating quadrat and product sums:\n\\[SS_{xy} = 40.74423\\]\n\\[SS_{x} = 22.74341\\]\n\\[SS_{y} = 131.22289\\]\nThen we estimate the correlation \\(\\hat{\\rho}\\) by the sample correlation \\[r_{xy}= \\frac{SS_{xy}}{\\sqrt{SS_x}\\sqrt{SS_y}} = 0.7458191 \\]"
  },
  {
    "objectID": "F10_notes_ENG.html#comparison-of-a-test-quantity-to-a-quantile-1",
    "href": "F10_notes_ENG.html#comparison-of-a-test-quantity-to-a-quantile-1",
    "title": "F10. Correlation and regression analyses",
    "section": "Comparison of a test quantity to a quantile",
    "text": "Comparison of a test quantity to a quantile\nThen, we derive the test quantity\n\\[t = 0.7458191\\sqrt{\\frac{10-2}{1-0.7458191^2}} = 3.166704\\]\nTest rule: The null hypothesis \\(H_0\\) is rejected since the test quantity \\(t&gt;t_{0.025}(10-2) = 2.3060041\\)"
  },
  {
    "objectID": "F10_notes_ENG.html#examine-assumptions",
    "href": "F10_notes_ENG.html#examine-assumptions",
    "title": "F10. Correlation and regression analyses",
    "section": "Examine assumptions",
    "text": "Examine assumptions\nBefore we can conclude, we check the assumption of a normal distribution for \\(X\\) and \\(Y\\).\n\n\n\nqqnorm(df$x)\nqqline(df$x)\n\n\n\n\n\n\n\n\n\n\nqqnorm(df$y)\nqqline(df$y)\n\n\n\n\n\n\n\n\n\n\nBased on comparisons of empirical and theoretical quantiles, there is no reason to no assume the assumption of bivariate normal."
  },
  {
    "objectID": "F10_notes_ENG.html#conclusion",
    "href": "F10_notes_ENG.html#conclusion",
    "title": "F10. Correlation and regression analyses",
    "section": "Conclusion",
    "text": "Conclusion\nSince we reject the null hypothesis that the correlation is zero and the assumption underlying the test seems to hold, we conclude that there is a linear relationship between Cholesterol and Triglycerides.\n\nNote that a statistical relationship does not imply a causal relationship! In other words, we cannot conclude that the level of cholesterol affects the level of triglycerides or vice versa."
  },
  {
    "objectID": "F10_notes_ENG.html#printouts-from-statistical-programs",
    "href": "F10_notes_ENG.html#printouts-from-statistical-programs",
    "title": "F10. Correlation and regression analyses",
    "section": "Printouts from statistical programs",
    "text": "Printouts from statistical programs\nHere you can compare what you get when you run the command cor.test\n\ncor.test(df$x,df$y)\n\n\n    Pearson's product-moment correlation\n\ndata:  df$x and df$y\nt = 3.1667, df = 8, p-value = 0.01326\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.2190612 0.9359399\nsample estimates:\n      cor \n0.7458191"
  },
  {
    "objectID": "F10_notes_ENG.html#ordinary-least-squares",
    "href": "F10_notes_ENG.html#ordinary-least-squares",
    "title": "F10. Correlation and regression analyses",
    "section": "Ordinary Least Squares",
    "text": "Ordinary Least Squares\nOne way to decide which values on the parameters to use is to minimise the sum of the quadratic differences from the line to the data at every data point. This is the idea behind Ordinary Least Square (OLS) regression.\n\\[Q(\\beta_0,\\beta_1)=\\sum \\varepsilon_i^2 = \\sum (y_i - (\\beta_0 + \\beta_1 x_i))^2\\]\nWe would like to find values on \\(\\beta_0\\) and \\(\\beta_1\\) such that the quadratic sum \\(Q(\\beta_0,\\beta_1)\\) is as small as possible!\nThe parameters of the model are estimated as follows:\n\nCalculate \\(\\bar{x}\\), \\(\\bar{y}\\), \\(SS_x\\), \\(SS_y\\) and \\(SS_{xy}\\)\nEstimate the parameters according to the following\n\n\\[\\hat{\\beta}_1=b_1 = \\frac{SS_{xy}}{SS_x}\\]\n\\[\\hat{\\beta}_0=b_0 = \\bar{y} - b_1 \\bar{x}\\]\nIf the line is estimated in this way, then we estimate the variation\n\\[\\hat{\\sigma}^2=s_e^2 = \\frac{\\sum (y_i - \\hat{y}_i)^2}{n-2} =\\frac{\\sum (y_i - (b_0+b_1x_i))^2}{n-2}  = \\frac{1}{n-2}\\left( SS_y - \\frac{(SS_{xy})^2}{SS_x}\\right)\\]\nWe can also estimate the variance of the estimators:\n\\[\\hat{V}(b_1) = \\frac{s_e^2}{SS_x}\\]\n\\[\\hat{V}(b_0) = s_e^2\\left(\\frac{1}{n}+\\frac{\\bar{x}^2}{SS_x}\\right)\\]"
  },
  {
    "objectID": "F10_notes_ENG.html#inference-in-simple-linear-regression",
    "href": "F10_notes_ENG.html#inference-in-simple-linear-regression",
    "title": "F10. Correlation and regression analyses",
    "section": "Inference in simple linear regression",
    "text": "Inference in simple linear regression\nIt is common that we would like to make conclusions by deriving confidence intervals for\n\nthe parameters\n\n– the slope of the line \\(\\beta_1\\)\n– the intercept \\(\\beta_0\\) - not so commmon\n\nprognosis\n\n– the expected value on the response variable given a certain value on the explanatory variable \\(\\mu(x_0)=\\beta_0 + \\beta_1 x_0\\)\n– a future observation of the response variable conditional on a certain value on the explanatory variable \\(y(x_0)\\)\n\n\n\n\n\n\nExample. Age and resting heart beat (cont.)\n\n\n\nWe make an hypothesis test to answer if there is a linear association between age and resting heart beat\nHypotheses: \\(H_0: \\beta_1 = 0\\) (the slope is zero, there are no linear association)\n\\(H_1: \\beta_1 \\neq 0\\) (the slope is not zero, it cannot be excluded that there is a linear association)"
  },
  {
    "objectID": "F10_notes_ENG.html#quadrat-and-product-sums-and-sample-means",
    "href": "F10_notes_ENG.html#quadrat-and-product-sums-and-sample-means",
    "title": "F10. Correlation and regression analyses",
    "section": "Quadrat and product sums and sample means",
    "text": "Quadrat and product sums and sample means\nThis is how Ullrika calculates summary measures to proceed. Some calculators have built-in functions to perform these calculations. Use a calculator or R to do the exercises. You will receive calculated sums of squares and products as well as sample means on the written exam.\n\ndf &lt;- data.frame(x=heart$Age, y = heart$RestBP)\nn = nrow(df)\nssxy = sum(df$x*df$y)-n*mean(df$x)*mean(df$y)\nssx = sum(df$x^2) - n*mean(df$x)^2\nssy = sum(df$y^2) - n*mean(df$y)^2\nm_y = mean(df$y)\nm_x = mean(df$x)\n\nb1 = ssxy/ssx\nb0 = m_y-b1*m_x\nse2 = (ssy-(ssxy)^2/ssx)/(n-2)"
  },
  {
    "objectID": "F10_notes_ENG.html#confidence-interval-for-the-slope",
    "href": "F10_notes_ENG.html#confidence-interval-for-the-slope",
    "title": "F10. Correlation and regression analyses",
    "section": "Confidence interval for the slope",
    "text": "Confidence interval for the slope\nTo test if the slope is different from zero, increasing, or decreasing, we construct a confidence interval for the slope.\nGiven the model for linear regression and if the assumption of independent and normally distributed residuals with the same variance holds, we can use a t-quantile to create confidence intervals for the slope of the line.\n\\[I_{\\beta_1} = b_1 \\pm t_{\\alpha/2}(n-2)\\sqrt{\\hat{V}(b_1)}\\]\n\\(b_1 = \\frac{SS_{xy}}{SS_x} = \\frac{1.3689261\\times 10^{4}}{2.467262\\times 10^{4}} = 0.555\\)\n\\(n=303\\)\n\\(\\hat{V}(b_1) = \\frac{s_e^2}{SS_x} = \\frac{285.5466516}{2.467262\\times 10^{4}} = 0.012\\)\nA 95% confidence interval becomes\n\nlb = b1-qt(1-0.025,n-2)*sqrt(se2/ssx) #lower bound\nub = b1+qt(1-0.025,n-2)*sqrt(se2/ssx) #upper bound\n\n\\(I_{\\beta_1} = 0.555 \\pm t_{0.025}(303-2)\\sqrt{0.012} = (0.343,0.767)\\)"
  },
  {
    "objectID": "F10_notes_ENG.html#residual-analyses---study-if-the-assumptions-hold",
    "href": "F10_notes_ENG.html#residual-analyses---study-if-the-assumptions-hold",
    "title": "F10. Correlation and regression analyses",
    "section": "Residual analyses - study if the assumptions hold",
    "text": "Residual analyses - study if the assumptions hold\nBefore we can draw any conclusions, we need to examine if the model appears reasonable. Are the residuals normally distributed and independent? This can be investigated by plotting residuals against \\(x\\) and examining if they follow a normal distribution.\nTo calculate the residuals, I need to estimate the intercept.\n\\[b_0 = \\bar{y} - b_1 \\bar{x} = 131.7 - 0.6\\cdot 54.4 = 101.5\\]\nwhere every residual is\n\\[e_i = y_i - b_0 - b_1x_i\\]\n\nres = df$y - b0 - b1*df$x \n\n\n\n\nqqnorm(res)\nqqline(res)\n\n\n\n\n\n\n\n\n⇒ There is no reason to not assume a normal distribution\n\n\n# if independent and equal variance - no pattern\nplot(df$x,res)\n\n\n\n\n\n\n\n\n⇒ here is no reason to not assume independence and equal variance"
  },
  {
    "objectID": "F10_notes_ENG.html#make-a-conclusion-from-the-hypothesis-test",
    "href": "F10_notes_ENG.html#make-a-conclusion-from-the-hypothesis-test",
    "title": "F10. Correlation and regression analyses",
    "section": "Make a conclusion from the hypothesis test",
    "text": "Make a conclusion from the hypothesis test\nSince the interval \\(I_{\\beta_1}\\) does not cover zero, we reject the null hypothesis on the significance level \\(\\alpha = 0.05\\)."
  },
  {
    "objectID": "F10_notes_ENG.html#printouts-from-statistical-software",
    "href": "F10_notes_ENG.html#printouts-from-statistical-software",
    "title": "F10. Correlation and regression analyses",
    "section": "Printouts from statistical software",
    "text": "Printouts from statistical software\nIn R, one can perform linear regression with the function lm. The function summary creates a printout of different things, where you need to understand what you need to do the analysis you would like to do.\n\nmod = lm(formula = RestBP ~ Age, data = heart)\nsummary(mod)\n\n\nCall:\nlm(formula = RestBP ~ Age, data = heart)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-38.659 -11.449  -0.904  10.218  67.444 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 101.4851     5.9364  17.095  &lt; 2e-16 ***\nAge           0.5548     0.1076   5.157 4.55e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.9 on 301 degrees of freedom\nMultiple R-squared:  0.08119,   Adjusted R-squared:  0.07814 \nF-statistic:  26.6 on 1 and 301 DF,  p-value: 4.547e-07\n\n\nA confidence interval is calculated by the function confint were you are to provide the confidence level (95% is default)\n\nconfint(mod)\n\n                 2.5 %    97.5 %\n(Intercept) 89.8028869 113.16727\nAge          0.3431323   0.76654\n\n\nYou can also ask the program to plot residuals. When you do this, you will get several plots showing things that we do not go through in this course.\n\nplot(mod)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou can get residuals by the function residuals and examine if the residuals are normally distributed, independent and has equal variance in your own way.\n\nres = residuals(mod)"
  },
  {
    "objectID": "F10_notes_ENG.html#example.-lung-capacity",
    "href": "F10_notes_ENG.html#example.-lung-capacity",
    "title": "F10. Correlation and regression analyses",
    "section": "Example. Lung capacity",
    "text": "Example. Lung capacity\nIn a study, researchers wanted to examine how lung capacity (measured in liters using a spirometer) is influenced by individuals’ weight (in kilograms). The two variables were measured in 20 randomly selected women aged 17 to 19 years.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPerson\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\nweight\n54.4\n56.2\n49.0\n63.5\n60.8\n59.9\n62.6\n62.1\n52.2\n50.8\n\n\nlung cap.\n3.87\n3.26\n2.14\n4.13\n3.44\n2.78\n2.91\n3.33\n3.20\n2.17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPerson\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n\n\nweight\n57.2\n48.1\n54.0\n50.8\n49.9\n46.3\n59.0\n56.2\n61.2\n53.1\n\n\nlung cap.\n3.13\n2.47\n3.03\n2.88\n2.65\n2.03\n3.21\n3.45\n3.61\n2.53"
  },
  {
    "objectID": "F10_notes_ENG.html#interesting-questions",
    "href": "F10_notes_ENG.html#interesting-questions",
    "title": "F10. Correlation and regression analyses",
    "section": "Interesting questions",
    "text": "Interesting questions\n\nIs there an association between weight and lung capacity?\nCould this association be linear, i.e. if \\(Y = \\text{lung capacity}\\) and \\(X=\\text{weight}\\), \\[y = \\beta_0+\\beta_1x\\]\nWhat does it mean in this model when \\(\\beta_1=0\\)?\nHow much does the lung capacity increase in average when the weight increase with one kg?\nWhat is the average lung capacity for women weigthing 60 kg - i.e. what is the expected lung capacity for women weigthing 60 kg?\nWhat lung capacity can a woman have that weights 60 kg - i.e. what lung capacty would we predict for a randomly chosen woman weigthing 60 kg?\nHow much of the variation in lung capacity is explained by their weigths?"
  },
  {
    "objectID": "F10_notes_ENG.html#hypothesis-test---is-there-an-association-between-lung-capacity-and-weigth",
    "href": "F10_notes_ENG.html#hypothesis-test---is-there-an-association-between-lung-capacity-and-weigth",
    "title": "F10. Correlation and regression analyses",
    "section": "Hypothesis test - Is there an association between lung capacity and weigth?",
    "text": "Hypothesis test - Is there an association between lung capacity and weigth?\nModel: \\(Y = \\text{lung capacity}\\) and \\(X=\\text{weight}\\) with a linear relation\n\\[y_i = \\beta_0+\\beta_1x_i+\\varepsilon_i\\]\nAssumptions: We assume that the residuals are independent and equally distributed accordingt to \\(\\varepsilon_i \\sim N(0,\\sigma)\\)\nHypothesis: \\(H_0: \\beta_1 = 0\\) against \\(H_1: \\beta_1 \\neq 0\\) are tested with significance level \\(\\alpha = 0.05\\)"
  },
  {
    "objectID": "F10_notes_ENG.html#derive-descriptive-measures-from-the-samples",
    "href": "F10_notes_ENG.html#derive-descriptive-measures-from-the-samples",
    "title": "F10. Correlation and regression analyses",
    "section": "Derive descriptive measures from the samples",
    "text": "Derive descriptive measures from the samples\n\\(n = 20\\), \\(\\bar{x} = 55.365\\), \\(\\bar{y} = 55.365\\)\n\\(SS_{xy} = 41.2297\\), \\(SS_x = 530.9655\\), \\(SS_y = 6.14258\\)"
  },
  {
    "objectID": "F10_notes_ENG.html#derive-a-confidence-interval-for-the-slope",
    "href": "F10_notes_ENG.html#derive-a-confidence-interval-for-the-slope",
    "title": "F10. Correlation and regression analyses",
    "section": "Derive a confidence interval for the slope",
    "text": "Derive a confidence interval for the slope\n\\[I_{\\beta_1} = b_1 \\pm t_{0.025}(18)\\sqrt{\\frac{s_e^2}{SS_x}} = 0.078 \\pm 2.101\\sqrt{3.1\\times 10^{-4}} = (0.041,0.115) \\]\nThe null hypothesis is rejected on significance level \\(\\alpha=0.05\\) since the interval does not cover zero.\n⇒ There is support of a linear association between weight and lung capacity. The lung capacity increases in average 0.0776504 when the weight increases with one kg?"
  },
  {
    "objectID": "F10_notes_ENG.html#make-prognosis",
    "href": "F10_notes_ENG.html#make-prognosis",
    "title": "F10. Correlation and regression analyses",
    "section": "Make prognosis",
    "text": "Make prognosis\n\nExpected value of the response variable\n\nWhat is the average lung capacity for women weighting \\(x_0=60\\) kg - i.e. what is the expected lung capacity for women with weight 60 kg?\n\nThe expected value is \\(\\mu(60) = b_0+b_1\\cdot 60 = -1.29+0.08 \\cdot 60 = 3.37\\)\nA 95% confidence interval for this expected value is\n\\[\\begin{split} & I_{\\beta_0+\\beta_1x_0} = b_0+b_1\\cdot 60 \\pm t_{0.025}(18)\\sqrt{s_e^2\\left(\\frac{1}{n}+\\frac{(x_0-\\bar{x})^2}{SS_x}\\right)} = \\\\ & -1.29+0.08 \\cdot 60 \\pm 2.101 \\sqrt{0.163\\left(\\frac{1}{20}+\\frac{(60-55.4)^2}{530.97}\\right)} = \\\\ & (3.12,4.47) \\end{split}\\]\n\n\nIllustration of the confidence interval for the expected value\n\nmod = lm(y~x,data=df)\nnewdf = data.frame(x=seq(min(df$x),max(df$x),length.out=50))\nkonf = data.frame(predict(mod,newdata=newdf,interval = \"confidence\"))\n\nplot(y~x,data=df,ylim=c(1,5),xlab = 'weight', ylab='lung capacity')\nlines(newdf$x,konf$fit,col='darkred')\nlines(newdf$x,konf$lwr,col = 'blue',lty=2)\nlines(newdf$x,konf$upr,col = 'blue',lty=2)\n\n\n\n\n\n\n\n\n\n\nA future observation of the response variable\n\nWhat lung capacity can a woman have that weights 60 kg - i.e. what lung capacity would we predict for a randomly chosen women weigthing 60 kg?\n\nA 95% confidence interval for this prediction is\n\\[\\begin{split} & I_{\\beta_0+\\beta_1x_0+\\varepsilon} = b_0+b_1\\cdot 60 \\pm t_{0.025}(18)\\sqrt{s_e^2\\left(1+\\frac{1}{n}+\\frac{(x_0-\\bar{x})^2}{SS_x}\\right)} = \\\\ & -1.29+0.08 \\cdot 60 \\pm 2.101 \\sqrt{0.163\\left(1+\\frac{1}{20}+\\frac{(60-55.4)^2}{530.97}\\right)} = \\\\ & (2.48,4.74) \\end{split}\\]\n\nThe confidence interval for the prediction (also referred to as the prediction interval) is wider than the confidence interval for the expected value.\n\n\nBe aware that the model is valid over the interval of observed x-values. This means that prognoses outside this interval are less reliable.\n\n\n\nIllustration of prediction and confidence intervals for the response variable\n\nmod = lm(y~x,data=df)\nnewdf = data.frame(x=seq(min(df$x),max(df$x),length.out=50))\nkonf = data.frame(predict(mod,newdata=newdf,interval = \"confidence\"))\n\npred = data.frame(predict(mod,newdata=newdf,interval = \"prediction\"))\n\nplot(y~x,data=df,ylim=c(1,5),xlab = 'weight', ylab='lung capacity')\nlines(newdf$x,konf$fit,col='darkred')\nlines(newdf$x,konf$lwr,col = 'blue',lty = 2)\nlines(newdf$x,konf$upr,col = 'blue',lty = 2)\nlines(newdf$x,pred$lwr,lty = 2)\nlines(newdf$x,pred$upr,lty = 2)"
  },
  {
    "objectID": "F10_notes_ENG.html#coefficient-of-determination-r2",
    "href": "F10_notes_ENG.html#coefficient-of-determination-r2",
    "title": "F10. Correlation and regression analyses",
    "section": "Coefficient of determination \\(R^2\\)",
    "text": "Coefficient of determination \\(R^2\\)\n\nHow much of the variation in lung capacity is explained by weight?\n\nThe coefficient of determination (\\(R^2\\)) express how large part of the variance in the dependent variable (\\(Y\\)) that can be explained by the variation in the independent variable (\\(X\\)) under the condition that there is a linear association between \\(X\\) and \\(Y\\).\n\\[R^2 = 1 - \\frac{\\sum (y_i-\\hat{y}_i)^2}{\\sum (y_i-\\bar{y})^2} = 52.1\\%\\]\nwhere the nominator corresponds to non-explained variance \\(\\sum (y_i-\\hat{y}_i)^2 = 2.941076\\) and the denominator corresponds to the total variation \\(\\sum (y_i-\\bar{y})^2 = 6.14258\\)\nIn this example, weigth explains \\(52.1\\%\\) of the variation in lung capacity.\n\nThe coefficient of determination is the same as the squared correlation coefficient when the mdoel is linear, i.e. \\(R^2 = r^2\\)\n\n\nThe coefficient of determination does not say anything of how good the model is. One should always visualise the model and data."
  },
  {
    "objectID": "F11_notes_ENG.html",
    "href": "F11_notes_ENG.html",
    "title": "F11. Analysis of variance and statistical models in practice",
    "section": "",
    "text": "An analysis of variance test the null hypothesis that three or more populations have the same expected value\n\nModel: We have \\(r\\) independent random variables \\(Y_1, Y_2, \\dots, Y_r\\) with expected values \\(E(Y_i)=\\mu_i\\) and equal variance \\(V(Y_i)=\\sigma^2\\)\nHypoteses: \\(H_0: \\mu_1 = \\mu_2 = \\dots = \\mu_r\\)\n\\(H_1: \\text{At least two of the expected values are different}\\)\n\nThe larger difference in expected in populations with the same variance, the larger will the variation between populations be compared to the variation within the populations.\n\nAssumption: \\(Y_1, Y_2, \\dots, Y_r\\) are normally distributed.\nTest rule:\nTest quantity \\(F = \\frac{\\text{\"estimation of variance between groups\"}}{\\text{\"estimation of variance within groups\"}} = \\frac{\\frac{SS_{Between}}{r-1}}{\\frac{SS_{Within}}{n-r}}\\)\nUnder \\(H_0\\) then \\(F = \\frac{\\frac{SS_{Between}}{r-1}}{\\frac{SS_{Within}}{n-r}} \\sim F(r-1,n-r)\\).\nReject \\(H_0\\) if \\(F &gt; F_{\\alpha}(r-1,n-r)\\)\n\n\n\n\n\n\nSimulation example\n\n\n\nWe simulate samples with 10 values in each from three groups (populations) with the same expected value and equal variances. It is useful to become familiar with how data can look under the null hypothesis.\n\nr = 3\nni = c(10,10,10)\nsigma = 1\nmui = c(0,0,0)\ni=1\ny1 &lt;- rnorm(ni[i],mui[i],sigma)\ni=2\ny2 &lt;- rnorm(ni[i],mui[i],sigma)\ni=3\ny3 &lt;- rnorm(ni[i],mui[i],sigma)\n(df &lt;- data.frame(y = c(y1,y2,y3), group = c(rep(\"A\",ni[1]),rep(\"B\",ni[2]),rep(\"C\",ni[3]))))\n\n             y group\n1  -0.62240128     A\n2  -0.43575352     A\n3   2.02060289     A\n4   0.82739456     A\n5  -0.05018707     A\n6  -0.54836292     A\n7  -1.52961674     A\n8  -1.09302418     A\n9  -0.55519624     A\n10 -0.32250614     A\n11 -0.07667873     B\n12 -0.25828162     B\n13  0.56508774     B\n14 -0.40184213     B\n15 -0.28155599     B\n16 -0.42272049     B\n17 -0.42232708     B\n18  0.36762573     B\n19 -0.33735505     B\n20  2.07452388     B\n21 -1.53106794     C\n22 -0.97380701     C\n23 -0.25136704     C\n24 -1.13723824     C\n25 -0.82334827     C\n26  1.01476327     C\n27 -0.51004827     C\n28 -0.43948969     C\n29  0.29967221     C\n30 -1.01059657     C\n\n\n\nggplot(df,aes(y=y,x=group, col = group)) +\n  geom_boxplot() +\n  #geom_violin(draw_quantiles = c(0.25, 0.5, 0.75)) +\n  #geom_point() +\n  geom_jitter(width = 0.1) +\n  ggtitle(\"Three groups \")\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of variance is based on dividing the variation in a variable after the sources. For a one sided analysis of variance, you divide the variation into variation between and within groups. This is done by creating sums of squares:\n\\[SS_{Total} = SS_{Between} + SS_{Within}\\]\n\n\n\n\\[SS_{Total} = \\sum_{i=1}^{r}\\sum_{j=1}^{n_i}(y_{ij}-\\bar{y}_{..})^2\\]\n\n\n\n\n\n\nSimulation example (cont.)\n\n\n\nHere we calculate the sum of squares for the total variance in two ways\n\n(n = nrow(df))\n\n[1] 30\n\n(m = mean(df$y))\n\n[1] -0.2288367\n\n(ss_tot = sum((df$y-m)^2))\n\n[1] 21.48694\n\n\nwhere we estimate the variance by dividing the sum of square with a suitable number of degrees of freedom.\n\nss_tot/(n-1)\n\n[1] 0.7409291\n\n\nWe see that this gives the same value as the sample variance\n\nvar(df$y)\n\n[1] 0.7409291\n\n\nThis means that if we can derive the total sum of square from the sample variance\n\nvar(df$y)*(n-1)\n\n[1] 21.48694\n\n\n\n\n\n\n\n\\[SS_{Within} = \\sum_{i=1}^{r}\\sum_{j=1}^{n_i}(y_{ij}-\\bar{y}_{i.})^2 = \\sum_{j=1}^{n_1} (y_{1j}-\\bar{y}_{1.})^2+\\sum_{j=1}^{n_2} (y_{2j}-\\bar{y}_{2.})^2+\\dots \\sum_{j=1}^{n_r} (y_{rj}-\\bar{y}_{r.})^2\\]\n\n\n\n\n\n\nSimulation example (cont.)\n\n\n\nHere we first calculate the sample means per group\n\n(ma = mean(df$y[df$group==\"A\"]))\n\n[1] -0.2309051\n\n(mb = mean(df$y[df$group==\"B\"]))\n\n[1] 0.08064763\n\n(mc = mean(df$y[df$group==\"C\"]))\n\n[1] -0.5362528\n\n\nthen the sum of square for the variance within groups as the pooled estimate of the variance\n\n(ss_within = (sum((df$y[df$group==\"A\"]-ma)^2)+sum((df$y[df$group==\"B\"]-mb)^2)+sum((df$y[df$group==\"C\"]-mc)^2)))\n\n[1] 19.58405\n\n\nWe can estimate the variance by dividing the sum of square by a suitable number of degrees of freedom\n\nss_within/(n-r)\n\n[1] 0.7253351\n\n\n\n\n\n\n\n\\[SS_{Between} = \\sum_{i=1}^{r} n_i (\\bar{y}_{i.}-\\bar{y}_{..})^2\\] A simple way to derive the last sum of square is \\(SS_{Between} = SS_{Total}-SS_{Within}\\)\n\n\n\n\n\n\nSimulation example (cont.)\n\n\n\n\n(ss_between = ni[1]*(ma - m)^2 + ni[2]*(mb - m)^2 +ni[3]*(mc - m)^2)\n\n[1] 1.902895\n\nss_tot - ss_within\n\n[1] 1.902895\n\n\nHere, we can estimate the variance by dividing the sum of square with a suitable number of degrees of freedom\n\nss_between/(r-1)\n\n[1] 0.9514473\n\n\n\n\n\n\n\nUnder \\(H_0\\) all groups have equal expected values, and the variance within and between groups are equal. Then the test quantity \\(F = \\frac{\\frac{SS_{Between}}{r-1}}{\\frac{SS_{Within}}{n-r}} \\sim F(r-1,n-r)\\).\n\n\n\n\n\n\nSimulation example (cont.)\n\n\n\nWe derive the test quantity as\n\n((ss_between/(r-1))/(ss_within/(n-r)))\n\n[1] 1.311735\n\n\nIt is compared to a quantile\n\nqf(1-0.05,r-1,n-r)\n\n[1] 3.354131\n\n#qf(1-0.01,r-1,n-r)\n\n\\(H_0\\) cannot be rejected at the chosen significance level of 0.05\n\n\n\n\n\nLet us hypothetically do repeated observations and derive the value of the test quantity many times.\n\ntestquantity &lt;- replicate(1000,{\nr = 3\nni = c(10,10,10)\nsigma = 1\nmui = c(0,0,0)\ni=1\ny1 &lt;- rnorm(ni[i],mui[i],sigma)\ni=2\ny2 &lt;- rnorm(ni[i],mui[i],sigma)\ni=3\ny3 &lt;- rnorm(ni[i],mui[i],sigma)\ndf_sim &lt;- data.frame(y = c(y1,y2,y3), group = c(rep(\"A\",ni[1]),rep(\"B\",ni[2]),rep(\"C\",ni[3])))\nn = nrow(df_sim)\nm = mean(df_sim$y)\nss_tot = sum((df_sim$y-m)^2)\nma = mean(df_sim$y[df_sim$group==\"A\"])\nmb = mean(df_sim$y[df_sim$group==\"B\"])\nmc = mean(df_sim$y[df_sim$group==\"C\"])\nss_within = (sum((df_sim$y[df$group==\"A\"]-ma)^2)+sum((df_sim$y[df$group==\"B\"]-mb)^2)+sum((df_sim$y[df$group==\"C\"]-mc)^2))\nss_between = ss_tot - ss_within\n(ss_between/(r-1))/(ss_within/(n-r))\n})\n\nWe make an histogram of the simulated test quantities and compare to an F distribution with \\(r-1\\) and \\(n-r\\) degrees of freedom\n\nr = 3\nn = 30\nhist(testquantity, probability = TRUE, breaks = 20)\nff = seq(0,max(testquantity),length.out = 200)\ndd = df(ff,r-1,n-r)\nlines(ff,dd,col='blue')\n\n\n\n\n\n\n\n\nThe quantile we compare to denote the border to the critical area where we are to reject the null hypothesis.\n\n\n\n\n\n\n\n\n\n\n\n\nIn R you can get the sum of squares by the functions lm and anova. It also generates the p-value for the F-test.\n\nmod &lt;- lm(y~group,df)\nsummary(mod)\n\n\nCall:\nlm(formula = y ~ group, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2987 -0.4652 -0.3023  0.2588  2.2515 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  -0.2309     0.2693  -0.857    0.399\ngroupB        0.3116     0.3809   0.818    0.421\ngroupC       -0.3053     0.3809  -0.802    0.430\n\nResidual standard error: 0.8517 on 27 degrees of freedom\nMultiple R-squared:  0.08856,   Adjusted R-squared:  0.02105 \nF-statistic: 1.312 on 2 and 27 DF,  p-value: 0.286\n\nanova(mod)\n\nAnalysis of Variance Table\n\nResponse: y\n          Df  Sum Sq Mean Sq F value Pr(&gt;F)\ngroup      2  1.9029 0.95145  1.3117  0.286\nResiduals 27 19.5840 0.72534               \n\n\n\nTukeyHSD(aov(mod), conf.level=.95)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = mod)\n\n$group\n          diff        lwr       upr     p adj\nB-A  0.3115527 -0.6327996 1.2559050 0.6952938\nC-A -0.3053477 -1.2497000 0.6390046 0.7051872\nC-B -0.6169004 -1.5612527 0.3274519 0.2548194"
  },
  {
    "objectID": "F11_notes_ENG.html#introduction-of-sums-of-squares",
    "href": "F11_notes_ENG.html#introduction-of-sums-of-squares",
    "title": "F11. Analysis of variance and statistical models in practice",
    "section": "",
    "text": "Analysis of variance is based on dividing the variation in a variable after the sources. For a one sided analysis of variance, you divide the variation into variation between and within groups. This is done by creating sums of squares:\n\\[SS_{Total} = SS_{Between} + SS_{Within}\\]"
  },
  {
    "objectID": "F11_notes_ENG.html#sum-of-square-for-the-total-variance",
    "href": "F11_notes_ENG.html#sum-of-square-for-the-total-variance",
    "title": "F11. Analysis of variance and statistical models in practice",
    "section": "",
    "text": "\\[SS_{Total} = \\sum_{i=1}^{r}\\sum_{j=1}^{n_i}(y_{ij}-\\bar{y}_{..})^2\\]\n\n\n\n\n\n\nSimulation example (cont.)\n\n\n\nHere we calculate the sum of squares for the total variance in two ways\n\n(n = nrow(df))\n\n[1] 30\n\n(m = mean(df$y))\n\n[1] -0.2288367\n\n(ss_tot = sum((df$y-m)^2))\n\n[1] 21.48694\n\n\nwhere we estimate the variance by dividing the sum of square with a suitable number of degrees of freedom.\n\nss_tot/(n-1)\n\n[1] 0.7409291\n\n\nWe see that this gives the same value as the sample variance\n\nvar(df$y)\n\n[1] 0.7409291\n\n\nThis means that if we can derive the total sum of square from the sample variance\n\nvar(df$y)*(n-1)\n\n[1] 21.48694"
  },
  {
    "objectID": "F11_notes_ENG.html#sum-of-square-for-the-variance-within-groups",
    "href": "F11_notes_ENG.html#sum-of-square-for-the-variance-within-groups",
    "title": "F11. Analysis of variance and statistical models in practice",
    "section": "",
    "text": "\\[SS_{Within} = \\sum_{i=1}^{r}\\sum_{j=1}^{n_i}(y_{ij}-\\bar{y}_{i.})^2 = \\sum_{j=1}^{n_1} (y_{1j}-\\bar{y}_{1.})^2+\\sum_{j=1}^{n_2} (y_{2j}-\\bar{y}_{2.})^2+\\dots \\sum_{j=1}^{n_r} (y_{rj}-\\bar{y}_{r.})^2\\]\n\n\n\n\n\n\nSimulation example (cont.)\n\n\n\nHere we first calculate the sample means per group\n\n(ma = mean(df$y[df$group==\"A\"]))\n\n[1] -0.2309051\n\n(mb = mean(df$y[df$group==\"B\"]))\n\n[1] 0.08064763\n\n(mc = mean(df$y[df$group==\"C\"]))\n\n[1] -0.5362528\n\n\nthen the sum of square for the variance within groups as the pooled estimate of the variance\n\n(ss_within = (sum((df$y[df$group==\"A\"]-ma)^2)+sum((df$y[df$group==\"B\"]-mb)^2)+sum((df$y[df$group==\"C\"]-mc)^2)))\n\n[1] 19.58405\n\n\nWe can estimate the variance by dividing the sum of square by a suitable number of degrees of freedom\n\nss_within/(n-r)\n\n[1] 0.7253351"
  },
  {
    "objectID": "F11_notes_ENG.html#sum-of-square-for-the-variance-between-groups",
    "href": "F11_notes_ENG.html#sum-of-square-for-the-variance-between-groups",
    "title": "F11. Analysis of variance and statistical models in practice",
    "section": "",
    "text": "\\[SS_{Between} = \\sum_{i=1}^{r} n_i (\\bar{y}_{i.}-\\bar{y}_{..})^2\\] A simple way to derive the last sum of square is \\(SS_{Between} = SS_{Total}-SS_{Within}\\)\n\n\n\n\n\n\nSimulation example (cont.)\n\n\n\n\n(ss_between = ni[1]*(ma - m)^2 + ni[2]*(mb - m)^2 +ni[3]*(mc - m)^2)\n\n[1] 1.902895\n\nss_tot - ss_within\n\n[1] 1.902895\n\n\nHere, we can estimate the variance by dividing the sum of square with a suitable number of degrees of freedom\n\nss_between/(r-1)\n\n[1] 0.9514473"
  },
  {
    "objectID": "F11_notes_ENG.html#test-quantity",
    "href": "F11_notes_ENG.html#test-quantity",
    "title": "F11. Analysis of variance and statistical models in practice",
    "section": "",
    "text": "Under \\(H_0\\) all groups have equal expected values, and the variance within and between groups are equal. Then the test quantity \\(F = \\frac{\\frac{SS_{Between}}{r-1}}{\\frac{SS_{Within}}{n-r}} \\sim F(r-1,n-r)\\).\n\n\n\n\n\n\nSimulation example (cont.)\n\n\n\nWe derive the test quantity as\n\n((ss_between/(r-1))/(ss_within/(n-r)))\n\n[1] 1.311735\n\n\nIt is compared to a quantile\n\nqf(1-0.05,r-1,n-r)\n\n[1] 3.354131\n\n#qf(1-0.01,r-1,n-r)\n\n\\(H_0\\) cannot be rejected at the chosen significance level of 0.05"
  },
  {
    "objectID": "F11_notes_ENG.html#sampling-distribution-for-the-test-quantity-in-analysis-of-variance",
    "href": "F11_notes_ENG.html#sampling-distribution-for-the-test-quantity-in-analysis-of-variance",
    "title": "F11. Analysis of variance and statistical models in practice",
    "section": "",
    "text": "Let us hypothetically do repeated observations and derive the value of the test quantity many times.\n\ntestquantity &lt;- replicate(1000,{\nr = 3\nni = c(10,10,10)\nsigma = 1\nmui = c(0,0,0)\ni=1\ny1 &lt;- rnorm(ni[i],mui[i],sigma)\ni=2\ny2 &lt;- rnorm(ni[i],mui[i],sigma)\ni=3\ny3 &lt;- rnorm(ni[i],mui[i],sigma)\ndf_sim &lt;- data.frame(y = c(y1,y2,y3), group = c(rep(\"A\",ni[1]),rep(\"B\",ni[2]),rep(\"C\",ni[3])))\nn = nrow(df_sim)\nm = mean(df_sim$y)\nss_tot = sum((df_sim$y-m)^2)\nma = mean(df_sim$y[df_sim$group==\"A\"])\nmb = mean(df_sim$y[df_sim$group==\"B\"])\nmc = mean(df_sim$y[df_sim$group==\"C\"])\nss_within = (sum((df_sim$y[df$group==\"A\"]-ma)^2)+sum((df_sim$y[df$group==\"B\"]-mb)^2)+sum((df_sim$y[df$group==\"C\"]-mc)^2))\nss_between = ss_tot - ss_within\n(ss_between/(r-1))/(ss_within/(n-r))\n})\n\nWe make an histogram of the simulated test quantities and compare to an F distribution with \\(r-1\\) and \\(n-r\\) degrees of freedom\n\nr = 3\nn = 30\nhist(testquantity, probability = TRUE, breaks = 20)\nff = seq(0,max(testquantity),length.out = 200)\ndd = df(ff,r-1,n-r)\nlines(ff,dd,col='blue')\n\n\n\n\n\n\n\n\nThe quantile we compare to denote the border to the critical area where we are to reject the null hypothesis."
  },
  {
    "objectID": "F11_notes_ENG.html#analysis-of-variance-with-statistical-programs",
    "href": "F11_notes_ENG.html#analysis-of-variance-with-statistical-programs",
    "title": "F11. Analysis of variance and statistical models in practice",
    "section": "",
    "text": "In R you can get the sum of squares by the functions lm and anova. It also generates the p-value for the F-test.\n\nmod &lt;- lm(y~group,df)\nsummary(mod)\n\n\nCall:\nlm(formula = y ~ group, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2987 -0.4652 -0.3023  0.2588  2.2515 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  -0.2309     0.2693  -0.857    0.399\ngroupB        0.3116     0.3809   0.818    0.421\ngroupC       -0.3053     0.3809  -0.802    0.430\n\nResidual standard error: 0.8517 on 27 degrees of freedom\nMultiple R-squared:  0.08856,   Adjusted R-squared:  0.02105 \nF-statistic: 1.312 on 2 and 27 DF,  p-value: 0.286\n\nanova(mod)\n\nAnalysis of Variance Table\n\nResponse: y\n          Df  Sum Sq Mean Sq F value Pr(&gt;F)\ngroup      2  1.9029 0.95145  1.3117  0.286\nResiduals 27 19.5840 0.72534               \n\n\n\nTukeyHSD(aov(mod), conf.level=.95)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = mod)\n\n$group\n          diff        lwr       upr     p adj\nB-A  0.3115527 -0.6327996 1.2559050 0.6952938\nC-A -0.3053477 -1.2497000 0.6390046 0.7051872\nC-B -0.6169004 -1.5612527 0.3274519 0.2548194"
  },
  {
    "objectID": "F11_notes_ENG.html#visualise-estimated-expected-values-for-the-groups",
    "href": "F11_notes_ENG.html#visualise-estimated-expected-values-for-the-groups",
    "title": "F11. Analysis of variance and statistical models in practice",
    "section": "Visualise estimated expected values for the groups",
    "text": "Visualise estimated expected values for the groups\n\n(EMM.source &lt;- emmeans(mod, \"group\"))\n\n group emmean    SE df lower.CL upper.CL\n A        3.4 0.456  9     2.37     4.43\n B        6.1 0.456  9     5.07     7.13\n C        9.8 0.456  9     8.77    10.83\n\nConfidence level used: 0.95 \n\n\n\nggplot(data.frame(EMM.source),aes(y=emmean,x=group, col = group)) +\n  geom_point() +\n  geom_errorbar(aes(ymin=lower.CL, ymax=upper.CL, width = 0.3)) +\n  ggtitle(\"Urea in blood for three diets\") +\n  ylab(\"y\")"
  },
  {
    "objectID": "F11_notes_ENG.html#continued-testing",
    "href": "F11_notes_ENG.html#continued-testing",
    "title": "F11. Analysis of variance and statistical models in practice",
    "section": "Continued testing",
    "text": "Continued testing\nIf the null hypothesis on equal expected values is rejected, one can continue to investigate which groups that differs. There are several tests for this.\nOne example is Tukey’s test which is used when we have the same number of observations in each group. This test creates confidence intervals for all pair-wise differences, but divides the signficance level between the pairs such that the total confidence level seen over all the intervals is 95%.\n\nTukeyHSD(aov(mod), conf.level=.95)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = mod)\n\n$group\n    diff       lwr      upr     p adj\nB-A  2.7 0.8977681 4.502232 0.0060127\nC-A  6.4 4.5977681 8.202232 0.0000103\nC-B  3.7 1.8977681 5.502232 0.0007387\n\n\n\nIt is good practice to adjust the significance level to the number of tests performed, so called multiple testing correction."
  },
  {
    "objectID": "F11_notes_ENG.html#logistic-regression",
    "href": "F11_notes_ENG.html#logistic-regression",
    "title": "F11. Analysis of variance and statistical models in practice",
    "section": "Logistic regression",
    "text": "Logistic regression\nWhen observations are 0 or 1, we can specify a model where the response is the logarithm of the odds-ratio\n\\[log(\\frac{P(Y_i = 1)}{P(Y_i=0)}) = \\beta_0 + \\beta_1 x_i\\]"
  },
  {
    "objectID": "F11_notes_ENG.html#generalised-models",
    "href": "F11_notes_ENG.html#generalised-models",
    "title": "F11. Analysis of variance and statistical models in practice",
    "section": "Generalised models",
    "text": "Generalised models\nIn this course, the response variable is continuous. We can write the simple linear regression as\n\\[Y_i\\sim N(\\beta_0 + \\beta_1 x_i, \\sigma)\\]\nIt is common to have observations that are discrete or categorical. Then is generalised models for regression are suitable.\n\nPoisson regression - when observations are counts\n\n\\[Y_i\\sim Po(\\mu_i)\\] where \\(log(\\mu_i) = \\beta_0+\\beta_1 x_i\\)\nThere are several possible choices for the probability distribution of the response variable (ex \\(Po\\)) and choice of function to link the expected value \\(\\mu_i\\) to the linear predictor \\(\\beta_0+\\beta_1 x_i\\).\nParameters are usually estimated as the values that maximises the probability for data given the model (maximum likelihood), in some cases by minimising ordinary least squares (OLS), or by Bayesian inference."
  },
  {
    "objectID": "F11_notes_ENG.html#non-linear-models",
    "href": "F11_notes_ENG.html#non-linear-models",
    "title": "F11. Analysis of variance and statistical models in practice",
    "section": "Non-linear models",
    "text": "Non-linear models\nNon-linear models are used when then there is a reason to suspect that the relationship has a certain form, e.g. to increase up to a limit."
  },
  {
    "objectID": "F11_notes_ENG.html#hierarchical-models",
    "href": "F11_notes_ENG.html#hierarchical-models",
    "title": "F11. Analysis of variance and statistical models in practice",
    "section": "Hierarchical models",
    "text": "Hierarchical models\nIt can be suitable to assign variation to several sources. Hierarchical models contains more than one source for random variation.\nIn simple linear regression we have one variance term\n\\[y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i\\] where \\(\\varepsilon_i \\sim N(0,\\sigma)\\)\nAs an exampel, measurements could have been collected from sites with different intercepts\n\\[y_{is} =  \\beta_s + \\beta_1 x_i + \\varepsilon_i\\] where \\(\\beta_s \\sim N(\\beta_0,\\sigma_s)\\) and \\(\\varepsilon_i \\sim N(0,\\sigma_\\varepsilon)\\)\nThe intercept \\(\\beta_s\\) has now in itself a random variation that depends on the group \\(s\\) and is called a random effect random effect.\nThe slope \\(\\beta_1\\) is a fixed effect that is common for all sites.\nA model like this is also known as a linear mixed model."
  },
  {
    "objectID": "F1_notes_ENG.html",
    "href": "F1_notes_ENG.html",
    "title": "F1. Foundations in probability theory",
    "section": "",
    "text": "A “trial” for which the outcome is unknown"
  },
  {
    "objectID": "F1_notes_ENG.html#union-and-intersection",
    "href": "F1_notes_ENG.html#union-and-intersection",
    "title": "F1. Foundations in probability theory",
    "section": "Union and intersection",
    "text": "Union and intersection\nIntersection: \\(A \\cap B\\) means A and B\nUnion: \\(A \\cup B\\) means A or B"
  },
  {
    "objectID": "F1_notes_ENG.html#venn-diagram",
    "href": "F1_notes_ENG.html#venn-diagram",
    "title": "F1. Foundations in probability theory",
    "section": "Venn diagram",
    "text": "Venn diagram"
  },
  {
    "objectID": "F1_notes_ENG.html#complementary-event",
    "href": "F1_notes_ENG.html#complementary-event",
    "title": "F1. Foundations in probability theory",
    "section": "Complementary event",
    "text": "Complementary event\n\\(P(A) + P(\\bar{A}) = 1\\)\n\\(P(\\bar{A}) = 1 - P(A)\\)\n\n\n\n\n\n\nExample: Complementary event\n\n\n\nA = “Dice shows six dots”\n\\(\\bar{A}\\) = “Dice does not show six dots”\n\\(P(A) = \\frac{1}{6}\\)\n\\(P(\\bar{A}) = 1 - P(A) = 1 - \\frac{1}{6} = \\frac{5}{6}\\)"
  },
  {
    "objectID": "F2_notes_ENG.html",
    "href": "F2_notes_ENG.html",
    "title": "F2. Discrete random variables",
    "section": "",
    "text": "A random variable describes a random experiment by specifying the probability for every outcome in the outcome space.\n\n\n\n\n\n\nExample: Role of an n-sided dice\n\n\n\n\\(X\\) = “number of dots”\nThe probability for \\(X=1\\) is \\(\\frac{1}{n}\\)\nCuriosity - there is a dice that can be flipped like a coin The First Dice You Flip Like A Coin\n\n\n\n\n\n\n\n\nExample: Blood sugar level\n\n\n\n\\(X\\) = “blood sugar level (mmol/l) measured on an individual randomly chosen from a population”\n\n\n\n\nThere are two types of random variables:\n\nDiscrete r.v. taking specific values such as numbers, natural numbers or categories\nContinuous r.v. taking real numbers\n\n\n\n\n\n\n\nIs the following r.v. discrete or continuous?\n\n\n\n\nA toss of a coin resulting in heads or tail\nThe number of stars in the universe\nThe time (in seconds) it takes for the winner to reach the finish line in vasaloppet\nThe measured weight of an ant in milligram\n\n\n\n\n\n\nThe outcome space for a toss of a coin is {heads, tails}\nLet us define a discrete random variable for this random experiments by\n\\[X = \\left\\{ \\begin{array}{lr}\n        1 & \\text{if the outcome is heads}\\\\\n        0 & \\text{if the outcome is tails}\n        \\end{array}\\right.\\]\n\n\n\n\n\n\nTip\n\n\n\nAn outcome of the random variable \\(X\\) is denoted by a lowercase \\(x\\)."
  },
  {
    "objectID": "F2_notes_ENG.html#random-variables---discrete-and-continuous",
    "href": "F2_notes_ENG.html#random-variables---discrete-and-continuous",
    "title": "F2. Discrete random variables",
    "section": "",
    "text": "There are two types of random variables:\n\nDiscrete r.v. taking specific values such as numbers, natural numbers or categories\nContinuous r.v. taking real numbers\n\n\n\n\n\n\n\nIs the following r.v. discrete or continuous?\n\n\n\n\nA toss of a coin resulting in heads or tail\nThe number of stars in the universe\nThe time (in seconds) it takes for the winner to reach the finish line in vasaloppet\nThe measured weight of an ant in milligram"
  },
  {
    "objectID": "F2_notes_ENG.html#from-qualitative-to-quantitative-description-of-an-event",
    "href": "F2_notes_ENG.html#from-qualitative-to-quantitative-description-of-an-event",
    "title": "F2. Discrete random variables",
    "section": "",
    "text": "The outcome space for a toss of a coin is {heads, tails}\nLet us define a discrete random variable for this random experiments by\n\\[X = \\left\\{ \\begin{array}{lr}\n        1 & \\text{if the outcome is heads}\\\\\n        0 & \\text{if the outcome is tails}\n        \\end{array}\\right.\\]\n\n\n\n\n\n\nTip\n\n\n\nAn outcome of the random variable \\(X\\) is denoted by a lowercase \\(x\\)."
  },
  {
    "objectID": "F2_notes_ENG.html#simulation-of-20-rolls-with-the-dice",
    "href": "F2_notes_ENG.html#simulation-of-20-rolls-with-the-dice",
    "title": "F2. Discrete random variables",
    "section": "Simulation of 20 rolls with the dice",
    "text": "Simulation of 20 rolls with the dice"
  },
  {
    "objectID": "F2_notes_ENG.html#simulation-of-150-rolls-with-the-dice",
    "href": "F2_notes_ENG.html#simulation-of-150-rolls-with-the-dice",
    "title": "F2. Discrete random variables",
    "section": "Simulation of 150 rolls with the dice",
    "text": "Simulation of 150 rolls with the dice"
  },
  {
    "objectID": "F2_notes_ENG.html#requirements-for-a-poisson-distribution",
    "href": "F2_notes_ENG.html#requirements-for-a-poisson-distribution",
    "title": "F2. Discrete random variables",
    "section": "Requirements for a Poisson distribution",
    "text": "Requirements for a Poisson distribution\nIn order for a random variable to follow a Poisson distribution the following must hold:\n\nThe average amount of events is the same for different time intervals\nThe number of events in non-overlapping time intervals are independent\nTwo events cannot occur at the same time\n\n\n\n\n\n\n\nVolcanic eruptions\n\n\n\nNote that the number of volcanic eruptions under 100 years might not comply with these requirements - which one?"
  },
  {
    "objectID": "F2_notes_ENG.html#poisson-distribution---probability-function",
    "href": "F2_notes_ENG.html#poisson-distribution---probability-function",
    "title": "F2. Discrete random variables",
    "section": "Poisson distribution - Probability function",
    "text": "Poisson distribution - Probability function\n\\(X \\sim Po(\\lambda)\\)\n(the symbol \\(\\sim\\) (“tilde”) and what comes after is read out as “is distributed as a Poisson distribution”)\n\\(\\lambda\\) (greek letter “lambda”) is a parameter in the probability function\n\\(f_X(x) = P(X = x)=\\frac{e^{-\\lambda}\\cdot \\lambda^x}{x!}\\)\nwhere the outcome space consists of non-negative numbers \\(x \\in \\{0,1,2,3,...\\}\\)\nPoisson distribution on wikipedia"
  },
  {
    "objectID": "F2_notes_ENG.html#poisson-distribution-1",
    "href": "F2_notes_ENG.html#poisson-distribution-1",
    "title": "F2. Discrete random variables",
    "section": "Poisson distribution",
    "text": "Poisson distribution\n\n\n\n\n\n\nExample. Poisson distribution\n\n\n\nA few years ago, a dermatologist raised an alarm that in an area in Lund, located near a chemical industry, the number of cases of a rare cancer disease was unusually high. In the particular area, nine people (six women and three men) had been affected by the disease over a five-year period. When the doctor studied the nationwide cancer registry, he saw that in a population similar to that in the particular area, one would have expected the number of cases to be four during this five-year period.\nModel: \\(X=\\) “number of disease cases in the area under the five year period\n\\(X\\sim Po(\\lambda=4)\\)\n\\(P(\\text{exactly } x \\text{ cases}) = P(X=x) = f(x) =  \\frac{e^{-4} 4^x}{x!}\\)\n\n\n\n\n\n\n\n\n\n\nWhat is the probability for exactly 5 cases?\nWhat is the probability for at most 2 cases?\nWhat is the probability for at least 9 cases?\n\n\nExample. Poisson distribution\n\nWhat is the probability for exactly 5 cases?\n\n\n\n\n\n\n\n\n\n\n\n\nExample. Poisson distribution\n\nWhat is the probability for at most 2 cases?\n\n\\(P(X\\leq 2) = P(X=0) + P(X = 1) + P(X = 2)\\)\n\n\n\n\n\n\n\n\n\n\n\nExample. Poisson distribution\n\nWhat is the probability for at least 9 cases?\n\n\\(P(X\\geq 9) = P(X=9) + P(X = 10) + ....\\)\ncan also be written as\n\\(P(X\\geq 9) = 1 - P(X \\leq 8)\\)"
  },
  {
    "objectID": "F2_notes_ENG.html#derive-the-value-on-the-probability-using-a-probability-table",
    "href": "F2_notes_ENG.html#derive-the-value-on-the-probability-using-a-probability-table",
    "title": "F2. Discrete random variables",
    "section": "Derive the value on the probability using a probability table",
    "text": "Derive the value on the probability using a probability table\nA table for the distribution function for different parameter values\n\\(P(X \\leq 8) = 0.97864\\)\n\\(P(X \\geq 9) = 1- P(X \\leq 8) = 1 - 0.97864 = 0.02136\\)"
  },
  {
    "objectID": "F2_notes_ENG.html#binomial-distribution---an-attempt-to-derive-the-formula-for-its-probability-function",
    "href": "F2_notes_ENG.html#binomial-distribution---an-attempt-to-derive-the-formula-for-its-probability-function",
    "title": "F2. Discrete random variables",
    "section": "Binomial distribution - an attempt to derive the formula for it’s probability function",
    "text": "Binomial distribution - an attempt to derive the formula for it’s probability function\nWe make \\(n=10\\) trials where the probability to succeed in one trial is \\(p\\)\n\\(X=\\) “number of successful trials”\n\\(P(X=0) = P(\\text{failure in all 10 trials}) = (1-p)(1-p)\\cdots (1-p) = (1-p)^{10}\\)\n\\(P(X=1)\\)?\n\\(X=1\\) correspond to one successful trial and nine failed trials, where the order does not matter\nSuccess in the first trial: \\(p(1-p)\\dots (1-p) = p(1-p)^{9}\\)\ncan happen in 10 ways, which gives \\(P(X=1) = 10\\cdot p(1-p)^{9}\\)\nBy a similar reasoning: \\(P(X=2) = \\frac{10\\cdot 9}{2}\\cdot p^2(1-p)^8\\)"
  },
  {
    "objectID": "F2_notes_ENG.html#binomial-distribution---probability-function",
    "href": "F2_notes_ENG.html#binomial-distribution---probability-function",
    "title": "F2. Discrete random variables",
    "section": "Binomial distribution - probability function",
    "text": "Binomial distribution - probability function\n\\(X \\sim Bin(n,p)\\)\nThe parameters are \\(n\\), number of trials, and \\(p\\), the probability to succeed in a trial.\n\\(f_X(x) = P(X = x)== \\frac{n!}{x!(n-x)!}p^x\\cdot (1-p)^{n-x}\\) where the outcome space consists of whole numbers between 0 and \\(n\\), i.e. \\(x \\in \\{0,1,2,3,...,n\\}\\)\nBinomial distribution on wikipedia"
  },
  {
    "objectID": "F2_notes_ENG.html#binomial-distribution-1",
    "href": "F2_notes_ENG.html#binomial-distribution-1",
    "title": "F2. Discrete random variables",
    "section": "Binomial distribution",
    "text": "Binomial distribution\n\n\n\n\n\n\nExample: We role a four-sided dice 7 times\n\n\n\n\\(X=\\) “the number of times the dice shows one dot”\n\\(X \\sim Bin(n,p)\\) where \\(n=7\\) and \\(p=\\frac{1}{4}\\)\n\n\n\n\n\n\nWhat is the probability to one dot 4 times?\n\n\\(P(X=4) = \\binom{n}{x}p^x\\cdot (1-p)^{n-x} = \\binom{7}{4}\\frac{1}{4}^4\\cdot (1-\\frac{1}{4})^{3}\\)\n\nWhat is the probability to have one dot at the most four times?\n\n\\(P(X\\leq 4) = \\sum_{x = 0}^4 P(X=x) = \\sum_{x = 0}^4 \\binom{7}{x}\\frac{1}{4}^{x}\\cdot (\\frac{3}{4})^{7-x}\\)\nTedious to calculate"
  },
  {
    "objectID": "F2_notes_ENG.html#derive-the-value-on-the-probability-using-a-probability-table-1",
    "href": "F2_notes_ENG.html#derive-the-value-on-the-probability-using-a-probability-table-1",
    "title": "F2. Discrete random variables",
    "section": "Derive the value on the probability using a probability table",
    "text": "Derive the value on the probability using a probability table\nA table for the distribution function for different parameter values\n\\(P(X\\leq 4) = 0.98712\\)"
  },
  {
    "objectID": "F2_notes_ENG.html#expected-value-for-a-poisson-distribution",
    "href": "F2_notes_ENG.html#expected-value-for-a-poisson-distribution",
    "title": "F2. Discrete random variables",
    "section": "Expected value for a Poisson distribution",
    "text": "Expected value for a Poisson distribution\n\\(X \\sim Po(\\lambda)\\)\n\\(E(X) = \\lambda\\)\nIn the formula sheet the parameter for the Poisson distribution is \\(\\mu\\)"
  },
  {
    "objectID": "F2_notes_ENG.html#expected-value-for-a-binomial-distribution",
    "href": "F2_notes_ENG.html#expected-value-for-a-binomial-distribution",
    "title": "F2. Discrete random variables",
    "section": "Expected value for a Binomial distribution",
    "text": "Expected value for a Binomial distribution\n\\(X \\sim Bin(n,p)\\)\n\\(E(X) = n\\cdot p\\)"
  },
  {
    "objectID": "F3_notes_ENG.html",
    "href": "F3_notes_ENG.html",
    "title": "F3. Continuous random variables",
    "section": "",
    "text": "Let \\(\\mu\\) be the expected value for the random variable \\(X\\)\nThe variance describes the spread around the expected value. More specifically is the variance the expected value of the quadratic distance to the expected value of \\(X\\).\n\\(V(X) = E((X-E(X))^2)\\)\nor\n\\(V(X) = E((X-\\mu)^2)\\)\n\n\n\n\n\n\nNote\n\n\n\nThe reason for squaring the distances is that there will be both negative and positive distances, and they can ‘cancel each other out’ if summed directly.\nThe deviation is \\(\\sqrt{V(X)}\\) and is a measure of spread on the same scale as the random variable \\(X\\)."
  },
  {
    "objectID": "F3_notes_ENG.html#distribution-function-for-an-exponential-distribution",
    "href": "F3_notes_ENG.html#distribution-function-for-an-exponential-distribution",
    "title": "F3. Continuous random variables",
    "section": "Distribution function for an exponential distribution",
    "text": "Distribution function for an exponential distribution\nThe distribution function for an exponential distribution is\n\\[F(x) = 1 - e^{-\\lambda x}\\]"
  },
  {
    "objectID": "F3_notes_ENG.html#density-function-for-a-normal-distribution",
    "href": "F3_notes_ENG.html#density-function-for-a-normal-distribution",
    "title": "F3. Continuous random variables",
    "section": "Density function for a normal distribution",
    "text": "Density function for a normal distribution\n\\(X \\sim N(\\mu,\\sigma)\\)\n\n\n\n\n\n\nstandard deviation or variance\n\n\n\nSome text books and software use variance in the formula for the normal distribution \\(N(\\mu,\\sigma^2)\\)\n\n\n\nThe density function for a normal distribution looks like a church bell\nThe normal distribution is symmetrical\n\n\\(F(x) = 1 - F(-x)\\)\n\nMode, median and expeted value coincide fora normal distribution"
  },
  {
    "objectID": "F3_notes_ENG.html#distribution-function-for-a-normal-distribution",
    "href": "F3_notes_ENG.html#distribution-function-for-a-normal-distribution",
    "title": "F3. Continuous random variables",
    "section": "Distribution function for a normal distribution",
    "text": "Distribution function for a normal distribution\n\\(X \\sim N(\\mu,\\sigma)\\)\n\\[\\begin{split}  P(X \\leq 0.1) & = F(0.1) = \\int_{-\\infty}^{0.1}f(x)dx = \\\\  & \\int_{-\\infty}^{0.1}\\frac{1}{\\sigma \\sqrt{2\\pi}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}dx \\end{split}\\]\nLet us assume that \\(\\mu=0\\) and \\(\\sigma=1\\)\n\\[=\\int_{-\\infty}^{0.1}\\frac{1}{\\sqrt{2\\pi}}e^{-x^2}dx = \\dots\\text{is possible to solve but difficult}\\]"
  },
  {
    "objectID": "F3_notes_ENG.html#distribution-function-for-a-normal-distribution---table",
    "href": "F3_notes_ENG.html#distribution-function-for-a-normal-distribution---table",
    "title": "F3. Continuous random variables",
    "section": "Distribution function for a normal distribution - table",
    "text": "Distribution function for a normal distribution - table\nInstead of calcluating the integral we can use\n\ntables\ncalculators/computer programs\n\nIf we only have a table - how to do for all possible values on the expted value \\(\\mu\\) and the variance \\(\\sigma^2\\)?\nThe solution is to standardise the distribution"
  },
  {
    "objectID": "F3_notes_ENG.html#standardised-normal-distribution-and-normal-distribution",
    "href": "F3_notes_ENG.html#standardised-normal-distribution-and-normal-distribution",
    "title": "F3. Continuous random variables",
    "section": "Standardised normal distribution and normal distribution",
    "text": "Standardised normal distribution and normal distribution\nLet \\(Z \\sim N(0,1)\\)\nThen \\(X = \\mu + \\sigma \\cdot Z\\) is also normally distributed with expected value \\(\\mu\\) and variance \\(\\sigma^2\\), i.e.\n\\[X \\sim N(\\mu,\\sigma)\\]\n\n\n\n\n\n\nExample. Normal distribution\n\n\n\nLet \\(X \\sim N(5,2)\\)\n\\[\\begin{split} & P(X \\geq 0)  = 1 - P(X &lt; 0) = 1 - P(X \\leq 0) = \\\\ & 1 - P(\\frac{X-5}{2} \\leq \\frac{0-5}{2}) = 1 - \\Phi(\\frac{-5}{2}) = \\\\ & 1 - (1-\\Phi(\\frac{5}{2})) \\end{split}\\]\n\n\n\n\n\n\n\n\n\nExam question\n\n\n\nThe weight of a skier with equipment is modelled by a normal distribution with expected value 80 kg and varians 36 kg^2. The skiier Kim is alone in the lift. What is the probability that his weight exceeds 90 kg?\nLet \\(X = \\text{\"weight in kg\"}\\)\nModel: \\(X \\sim N(80,6)\\)\n\\[\\begin{split} & P(X &gt; 90) = 1 - P(X \\leq 90) = \\\\ & 1 - P(\\frac{X-80}{6} \\leq \\frac{90-80}{6}) = 1 - \\Phi(\\frac{10}{6}) \\end{split}\\]"
  },
  {
    "objectID": "F3_notes_ENG.html#examples-of-quantiles",
    "href": "F3_notes_ENG.html#examples-of-quantiles",
    "title": "F3. Continuous random variables",
    "section": "Examples of quantiles",
    "text": "Examples of quantiles\n\nMedian – the quantile that divides the distribution into two halfs with 50% probability each\nQuartiles – the quantiles that split a distribution into four parts with equal probability:\n\nFirst quartile (Q1)\nSecond quartile = Median\nThird quartile (Q3)\n\nPercentile – the p:th percentile is the value of a random variable that is higher than p% of all possible values"
  },
  {
    "objectID": "F3_notes_ENG.html#quantiles-illustrated-with-a-distribution-function",
    "href": "F3_notes_ENG.html#quantiles-illustrated-with-a-distribution-function",
    "title": "F3. Continuous random variables",
    "section": "Quantiles illustrated with a distribution function",
    "text": "Quantiles illustrated with a distribution function"
  },
  {
    "objectID": "F3_notes_ENG.html#quantiles-illustrated-with-a-density-function",
    "href": "F3_notes_ENG.html#quantiles-illustrated-with-a-density-function",
    "title": "F3. Continuous random variables",
    "section": "Quantiles illustrated with a density function",
    "text": "Quantiles illustrated with a density function"
  },
  {
    "objectID": "F3_notes_ENG.html#quantiles-illustrated-with-a-boxplot",
    "href": "F3_notes_ENG.html#quantiles-illustrated-with-a-boxplot",
    "title": "F3. Continuous random variables",
    "section": "Quantiles illustrated with a boxplot",
    "text": "Quantiles illustrated with a boxplot"
  },
  {
    "objectID": "F4_notes_ENG.html",
    "href": "F4_notes_ENG.html",
    "title": "F4. Continuous random variables, linear combination and sums of random variables",
    "section": "",
    "text": "A quantile is found by reading the distribution function backwards. The figure shows \\(z_{.25}\\) (red) and \\(z_{.90}\\) for a standardised normal distribution \\(Z \\sim N(0,1)\\).\n\n\n\n\n\n\n\n\n\nIn R we derive a quantile from a normal distribution with the command qnorm(p), where p is the probability for the area below the quantile.\nQuantiles are derived for common distributions in a similar way by putting a q in front of the name of the distribution, such as qexp or qt.\n\n\nA way to examine the fit of model (probability distribution for which data can be seen as observations from) is to compare theoretical quantiles (from the model) to empirical quantiles (from the sample).\n\n\n\n\n\n\nEmpirical distribution function\n\n\n\nThe empirical distribution function can be created as follows:\n\nsort the the \\(n\\) values in the sample - these are the values for the x-axis\ndivide the interval 0 to 1 into equal sized steps and for every value on the x-scale (start with the smallest one) you increase \\(F(x)\\)-value one step.\n\n\nload(\"data/lab1_filer/jordprov.Rdata\")\nx &lt;- sort(jordprov$al) # sort\nn &lt;- length(x) # the number of values in the sample\neFx &lt;- (1:n)/n # steps\nplot(x,eFx,main=\"Empirical distribution function\") # plot eFx against x\n\n\n\n\n\n\n\n\nThis is what is done, but with a nicer layouy, with the command plot.ecdf()\n\nplot.ecdf(jordprov$al)\n\n\n\n\n\n\n\n\nEmpirical distribution function on wiki\n\n\n\n\nLet us evaluate if a sample can be seen as coming from a normal distribution.\n\nThe theoretical distribution is a normal distribution with parameters estimated from the sample.\n\n\nm = mean(jordprov$al)\ns = sd(jordprov$al)\n\n\nWe derive the quantiles for the theoretical distribution corresponding to the values on the empirical distribution\n\n\ntFx &lt;- qnorm(eFx,m,s)\n\n\nThen we draw the theoretical quantiles against the sorted values in the sample. The values should be on a straight line if the theoretical models is a good model for the sample.\n\n\nplot(tFx,x)\nabline(0,1) # add a 1:1-line\n\n\n\n\n\n\n\n\nThe same thing can be done with the commands qqnorm and qqline but with a nice layout\n\nqqnorm(jordprov$al)\nqqline(jordprov$al)"
  },
  {
    "objectID": "F4_notes_ENG.html#quantile-quantile-plot",
    "href": "F4_notes_ENG.html#quantile-quantile-plot",
    "title": "F4. Continuous random variables, linear combination and sums of random variables",
    "section": "",
    "text": "A way to examine the fit of model (probability distribution for which data can be seen as observations from) is to compare theoretical quantiles (from the model) to empirical quantiles (from the sample).\n\n\n\n\n\n\nEmpirical distribution function\n\n\n\nThe empirical distribution function can be created as follows:\n\nsort the the \\(n\\) values in the sample - these are the values for the x-axis\ndivide the interval 0 to 1 into equal sized steps and for every value on the x-scale (start with the smallest one) you increase \\(F(x)\\)-value one step.\n\n\nload(\"data/lab1_filer/jordprov.Rdata\")\nx &lt;- sort(jordprov$al) # sort\nn &lt;- length(x) # the number of values in the sample\neFx &lt;- (1:n)/n # steps\nplot(x,eFx,main=\"Empirical distribution function\") # plot eFx against x\n\n\n\n\n\n\n\n\nThis is what is done, but with a nicer layouy, with the command plot.ecdf()\n\nplot.ecdf(jordprov$al)\n\n\n\n\n\n\n\n\nEmpirical distribution function on wiki\n\n\n\n\nLet us evaluate if a sample can be seen as coming from a normal distribution.\n\nThe theoretical distribution is a normal distribution with parameters estimated from the sample.\n\n\nm = mean(jordprov$al)\ns = sd(jordprov$al)\n\n\nWe derive the quantiles for the theoretical distribution corresponding to the values on the empirical distribution\n\n\ntFx &lt;- qnorm(eFx,m,s)\n\n\nThen we draw the theoretical quantiles against the sorted values in the sample. The values should be on a straight line if the theoretical models is a good model for the sample.\n\n\nplot(tFx,x)\nabline(0,1) # add a 1:1-line\n\n\n\n\n\n\n\n\nThe same thing can be done with the commands qqnorm and qqline but with a nice layout\n\nqqnorm(jordprov$al)\nqqline(jordprov$al)"
  },
  {
    "objectID": "F4_notes_ENG.html#expected-value-of-the-random-variables-x",
    "href": "F4_notes_ENG.html#expected-value-of-the-random-variables-x",
    "title": "F4. Continuous random variables, linear combination and sums of random variables",
    "section": "Expected value of the random variables \\(X\\)",
    "text": "Expected value of the random variables \\(X\\)\n\\(X\\) is discrete: \\(E(X) = \\sum_{all \\ x}xf(x)\\)\n\\(X\\) is continuous: \\(E(X) = \\int_{-\\infty}^{\\infty}xf(x)dx\\)\n\n\n\n\n\n\nThe expected value for a uniform distribution\n\n\n\n\\(X \\sim U(a,b)\\)\n\\[f(x) = \\left\\{ \\begin{array}{lr}\n        \\frac{1}{b-a} & \\text{if }a \\leq x \\leq b\\\\\n        0 & \\text{otherwise}\n        \\end{array}\\right.\\]\n\\[\\begin{split} & E(X)  =\\int_{-\\infty}^{\\infty}xf(x)dx =  \\\\ & \\int_{a}^{b}\\frac{x}{b-a}dx = [\\frac{x^2}{2(b-a)}]_{x=a}^{b}  = \\\\ & \\frac{b^2-a^2}{2(b-a)} = \\frac{(b+a)(b-a)}{2(b-a)} = \\frac{a+b}{2}  \\end{split}\\]\nThe expected value of \\(X \\sim U(0,10)\\) is \\(E(X)=\\frac{0+10}{2} = 5\\)"
  },
  {
    "objectID": "F4_notes_ENG.html#expected-value-of-a-function-g-of-x",
    "href": "F4_notes_ENG.html#expected-value-of-a-function-g-of-x",
    "title": "F4. Continuous random variables, linear combination and sums of random variables",
    "section": "Expected value of a function \\(g\\) of \\(X\\)",
    "text": "Expected value of a function \\(g\\) of \\(X\\)\n\\(X\\) is discrete: \\(E(g(X)) = \\sum_{alla \\ x}g(x)f(x)\\)\n\\(X\\) is continuous: \\(E(g(X)) = \\int_{-\\infty}^{\\infty}g(x)f(x)dx\\)\n\n\n\n\n\n\nExpected value of a function of a uniform distribution\n\n\n\ncontinuation of \\(X \\sim U(0,10)\\)\n\\(g(x) = x^2\\)\n\\[\\begin{split}  E(g(X)) = \\int_{-\\infty}^{\\infty}g(x)f(x)dx = \\int_{0}^{10}x^2\\frac{1}{10}dx = & \\\\  [\\frac{x^3}{3\\cdot 10}]_{x=0}^{10} = \\frac{10^3}{30} = \\frac{100}{3} \\end{split}\\]"
  },
  {
    "objectID": "F4_notes_ENG.html#expected-value-of-a-linar-combination-of-a-discrete-random-variable",
    "href": "F4_notes_ENG.html#expected-value-of-a-linar-combination-of-a-discrete-random-variable",
    "title": "F4. Continuous random variables, linear combination and sums of random variables",
    "section": "Expected value of a linar combination of a discrete random variable",
    "text": "Expected value of a linar combination of a discrete random variable\n\\(X\\) is a discrete random variable\n\nWhat is the expected value \\(E(X + b)\\)?\n\n\\[\\begin{split} E(X+b)=\\sum_{all \\ x}(x+b)f(x) = \\sum_{all \\ x}xf(x) + \\sum_{all \\ x}bf(x) = & \\\\ E(X) + b\\sum_{all \\ x}f(x) = E(X) + b \\end{split}\\]\n\nWhat is the expected value of \\(E(aX + b)\\)?\n\n\\[\\begin{split} E(aX+b)=\\sum_{all \\ x}(ax+b)f(x) = & \\\\ \\sum_{all \\ x}axf(x) + \\sum_{all \\ x}bf(x) = & \\\\ a\\sum_{all \\ x}xf(x) + b\\sum_{all \\ x}f(x) = aE(X) + b \\end{split}\\]"
  },
  {
    "objectID": "F4_notes_ENG.html#variance-of-a-linear-combination-of-a-discrete-random-variable",
    "href": "F4_notes_ENG.html#variance-of-a-linear-combination-of-a-discrete-random-variable",
    "title": "F4. Continuous random variables, linear combination and sums of random variables",
    "section": "Variance of a linear combination of a discrete random variable",
    "text": "Variance of a linear combination of a discrete random variable\n\nVariance in at least three ways\n\\(E(X) = \\mu\\)\n\\[\\begin{split} & V(X) =  \\underbrace{E[(X-E(X))^2]}_{I} =   \\underbrace{\\sum (x-\\mu)^2f(x)}_{II} = \\\\ & \\sum (x^2 - 2x\\mu + \\mu^2)f(x) = \\sum x^2f(x) - \\sum 2x\\mu f(x) + \\sum \\mu^2 f(x) = \\\\ &  E(X^2) - 2\\mu \\sum xf(x) + \\mu^2 \\sum f(x) = E(X^2)-2(E(X))^2+(E(X))^2 =  \\\\ &  \\underbrace{E(X^2)-[E(X)]^2}_{III} \\end{split}\\]\n\n\\(X\\) is a discrete random variable\n\nWhat is the variance \\(V(X + b)\\)?\n\n\\[\\begin{split} & V(X + b) = E[(X+b-E(X+b))^2] =  \\\\ & E[(X+b-E(X)-b)^2] = E[(X-E(X))^2] = V(X) \\end{split}\\]\n\nAdding the constant \\(b\\) shifts the distribution but does not change the spread.\n\n\nWhat is the variance \\(V(a \\cdot X + b)\\)?\n\n\\[\\begin{split} & V(a \\cdot X + b) = E[(aX+b-aE(X)-b)^2] = E[(aX-aE(X))^2]= \\\\ & E[a^2(X-E(X))^2] = a^2E[(X-E(X))^2] = a^2V(X) \\end{split}\\]"
  },
  {
    "objectID": "F4_notes_ENG.html#important-special-case",
    "href": "F4_notes_ENG.html#important-special-case",
    "title": "F4. Continuous random variables, linear combination and sums of random variables",
    "section": "Important special case",
    "text": "Important special case\n\nThe distribution of a sum of independent equally distributed normally distributed random variables\n\\(\\sum_{i=1}^n X_i \\sim N(n\\mu,\\sqrt{n}\\sigma)\\)\n\n\nThe distribution of an average of independent equally distributed normal distributions\n\\(\\bar{X} \\sim N(\\mu,\\frac{\\sigma}{\\sqrt{n}})\\)\n\n\n\n\n\n\nExample 1\n\n\n\nLet \\(X \\sim N(0,1)\\) och \\(Y=3X+2\\)\nDerive \\(E(Y)\\) and \\(D(Y)\\)\n\\(E(Y)=E(3X+2)=3E(X)+2=3\\cdot 0 + 2 = 2\\)\n\\(V(Y)=V(3X+2)=3^2V(X)=3^2\\cdot 1\\)\n\\(D(Y)=\\sqrt{V(Y)}=\\sqrt{3^2} = 3\\)\n\n\n\n\n\n\n\n\nExample 2\n\n\n\nLet \\(X \\sim N(1,1)\\) and \\(Y \\sim N(-1,2)\\) be independent random variables\n\nWhich distribution has \\(X+Y\\)?\n\nIt will be a normal distribution with expected value\n\\(E(X+Y) = E(X) + E(Y) = 1 + (-1) = 0\\)\nand variance\n\\(V(X+Y)=V(X)+V(Y)=1^2+2^2=5\\)\n\nWhich distribution has \\(X-Y\\)?\n\nIt will be a normal distribution with expected value\n\\(E(X-Y) = E(X) + E(-Y) = E(X) + (-1)\\cdot E(Y) = 1 + (-1)\\cdot(-1) = 1 + 1 = 2\\)\nand variance\n\\(V(X-Y)=V(X)+V(-Y)=V(X)+(-1)^2\\cdot V(Y)= V(X)+V(Y) = 1^2+2^2 = 5\\)\n\n\n\n\n\n\n\n\nExample 3\n\n\n\nThe independent random variables \\(X_1\\) and \\(X_2\\) both belongs to \\(N(1,2)\\)\n\nSpecify the distribution for \\(\\frac{X_1+X_2}{2}\\)\n\nThis is an average of two independent and normally distributed random variables with the same expected value \\(\\mu=1\\) and variance \\(\\sigma^2=2\\)\n\\(\\bar{X}=\\frac{X_1+X_2}{2}\\)\nFollowing the rules for a linear combination of random variables and knowing that “a sum of normal is also normal”, then\n\\(\\bar{X} \\sim N(\\mu,\\frac{\\sigma}{\\sqrt{2}})\\)\nLet us derive the expected value and variance\n\\[E(\\bar{X}) = E(\\frac{X_1+X_2}{2}) = \\frac{1}{2}E(X_1+X_2) = \\frac{1}{2} (1 + 1) = 1\\]\n\\[V(\\bar{X}) = V(\\frac{X_1+X_2}{2}) = \\frac{1}{2^2}V(X_1+X_2)=\\frac{1}{2^2}(2^2+2^2)=2\\]\n\nNote that the sum has a lower variance than the individual variances. The reason is that the two r.v. “cancels each other out” resulting in a lower variance in the sum."
  },
  {
    "objectID": "F5_notes_ENG.html",
    "href": "F5_notes_ENG.html",
    "title": "F5. Sums, Central Limit Theorem, Statistical model",
    "section": "",
    "text": "Uptil now we have looked at random variables with known parameters\nIn reality, the parameter values are unknown to us.\nWe have to “estimate” the parameters from our observations"
  },
  {
    "objectID": "F5_notes_ENG.html#example.-weight-of-ants.",
    "href": "F5_notes_ENG.html#example.-weight-of-ants.",
    "title": "F5. Sums, Central Limit Theorem, Statistical model",
    "section": "Example. Weight of ants.",
    "text": "Example. Weight of ants.\n\n\n\nAssume that the weights of ants follows a normal distribution, for which the expected value is unknown to us.\nWe find an anthill and choose 10 ants\n\nModel: \\(X_i=\\text{\"ant i's weight in mg\"}\\) \\[X_i \\sim N(\\mu,\\sigma)\\]\nrandom sample: \\(\\{x_1,x_2,\\dots,x_{10}\\}\\)\n\nWe estimate the expected value with the sample mean\n\n\\[\\hat{\\mu} = \\bar{x}\\]\n\n\n\n\n\nWe derive the sample mean to be \\(\\bar{x} = 5.2\\) mg.\n\n\nIf we were to choose 10 new ants, will the sample mean also be 5.2 mg? Unlikely.\n\nImagine collecting samples of ants many times and calculating the mean each time. There is random variation in the sample means. One can say that the mean of repeated samples is a random variable which we can denote as \\(\\bar{X}\\)."
  },
  {
    "objectID": "F5_notes_ENG.html#when-the-population-can-be-assumed-to-be-normally-distributed",
    "href": "F5_notes_ENG.html#when-the-population-can-be-assumed-to-be-normally-distributed",
    "title": "F5. Sums, Central Limit Theorem, Statistical model",
    "section": "When the population can be assumed to be normally distributed",
    "text": "When the population can be assumed to be normally distributed\n\nWe assumed that the weight of an ant is normally distributed.\nAccording to the rule “a sum of normally distributed variables is also normally distributed,” the sample mean will also be normally distributed.\nWe consider the weights of ants as observations from the same distribution (they are identically distributed).\nWe also consider them as independent.\n\nThen: \\[\\bar{X} \\sim N(\\mu,\\frac{\\sigma}{\\sqrt{n}})\\]\nwhere \\(n=10\\). We can the estimate the expected value \\(\\mu\\) and the standard deviation \\(\\sigma\\) with the sample we have.\n\nmu = 5 # expected value\nsigma = 1 # standard deviation\n\nn = 10 # sample size\n\niter = 1000 # number of iterations we draw a new random sample and calculate the sample mean\n\nsample_mean &lt;- replicate(iter,mean(rnorm(n,mu,sigma))) \n\nhist(sample_mean,prob = TRUE)\nxx = seq(4,6,by=0.01)\nyy = dnorm(xx,mu,sigma/sqrt(n))\nlines(xx,yy,col='blue') #the density distribution for N(mu,sigma/sqrt(n))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\bar{X}\\) or \\(\\bar{x}\\)\n\n\n\nIt is also fine to use \\(\\bar{x}\\) to denote sample mean as a random variable, as long as it is provided in the context that it is a random variable and not your calcluated valeu. E.g.\n\\[\\bar{x} \\sim N(\\mu,\\frac{\\sigma}{\\sqrt{n}})\\]"
  },
  {
    "objectID": "F5_notes_ENG.html#when-we-do-not-know-which-distribution-a-population-has",
    "href": "F5_notes_ENG.html#when-we-do-not-know-which-distribution-a-population-has",
    "title": "F5. Sums, Central Limit Theorem, Statistical model",
    "section": "When we do not know which distribution a population has",
    "text": "When we do not know which distribution a population has\n\nWhat happens if we do not know which distribution the weights of the ants have?\nWhich distribution will the sample mean then have?\n\n\nn = 10 # sample size\n\niter = 1000 # number of iterations we draw a new random sample and calculate the sample mean\n\n\nsample_mean &lt;- replicate(iter,mean(rexp(n))) # the true distribution is exponential\nhist(sample_mean,prob = TRUE)\n\n\n\n\n\n\n\nsample_mean &lt;- replicate(iter,mean(runif(n))) # the true distribution is uniform\nhist(sample_mean,prob = TRUE)\n\n\n\n\n\n\n\nsample_mean &lt;- replicate(iter,mean(rlnorm(n))) # the true distribution is log-normal\nhist(sample_mean,prob = TRUE)"
  },
  {
    "objectID": "F5_notes_ENG.html#example.-100-pills",
    "href": "F5_notes_ENG.html#example.-100-pills",
    "title": "F5. Sums, Central Limit Theorem, Statistical model",
    "section": "Example. 100 pills",
    "text": "Example. 100 pills\nThe weight (in gram) of a randomly chosen pill from a batch of similar pills is a r.v. with expected value \\(\\mu = 0.65\\) and variance \\(\\sigma^2=0.0004\\)\n\nWhat is the expected value and variance for the total weight of the 100 pills (the weights are independent of each other)?\n\nModel: Let \\(X_i = \\text{\"weight of pill i\"}\\), where \\(i = 1,\\dots,100\\)\nLet \\(Y_{100} = \\sum_{i=1}^{100} X_i\\)\n\\(E(Y_{100})=100\\cdot \\mu = 65\\)\n\\(V(Y_{100})=100\\cdot \\sigma^2 = 0.04\\)\n\nWhat is the probability that 100 pills weights at the most 65.3 gram?\n\nBecause 100 is a large number, according to the CLT, the sum of the weights is approximately normal.\n\\[\\begin{split} & P(Y_{100} \\leq 65.3) = P(\\frac{Y_{100}-\\mu}{\\sigma} \\leq \\frac{65.3-\\mu}{\\sigma}) = \\\\ & \\Phi(\\frac{65.3-65}{\\sqrt{0.04}}) = \\Phi(\\frac{0.3}{0.2}) = \\Phi(1.5) =  0.9332 \\end{split}\\]"
  },
  {
    "objectID": "F5_notes_ENG.html#what-is-the-difference-between-probability-theory-and-statistical-inference",
    "href": "F5_notes_ENG.html#what-is-the-difference-between-probability-theory-and-statistical-inference",
    "title": "F5. Sums, Central Limit Theorem, Statistical model",
    "section": "What is the difference between probability theory and statistical inference?",
    "text": "What is the difference between probability theory and statistical inference?\nProbability theory\n\nProbability distributions are known (we know all parameters in the model for the population)\n\nStatistical inference\n\nThe probability distributions are not known, the type of distribution or parameters unknown\nWe use data to estimate parameters and choose model, and make conclusions with the help of the models"
  },
  {
    "objectID": "F5_notes_ENG.html#bias-and-precision",
    "href": "F5_notes_ENG.html#bias-and-precision",
    "title": "F5. Sums, Central Limit Theorem, Statistical model",
    "section": "Bias and precision",
    "text": "Bias and precision\n\nUnbiased implies high accuracy\nLow variance corresponds to high precision in an estimate"
  },
  {
    "objectID": "F6_notes_ENG.html",
    "href": "F6_notes_ENG.html",
    "title": "F6. Estimation and confidence intervals",
    "section": "",
    "text": "Population\n\nAny random variable (regardless of distribution) has an expected value and variance \\[\\mu = E(X)\\] \\[\\sigma^2 = V(X)=E((X-\\mu)^2) = E(X^2)-\\mu^2\\]\n\nRandom sample\n\nSample mean and sample variance are measures of the location and spread in a sample of a population\n\\[\\bar{x} = \\frac{\\sum x_i}{n}\\]\n\\[s^2 = \\frac{\\sum (x_i-\\bar{x})^2 }{n-1} = \\frac{\\sum x^2 - n(\\bar{x})^2 }{n-1}\\]\n\n\n\n\nUnbiased estimates are\n\n\\[\\hat{\\mu} = \\frac{\\sum_{i=1}^n x_i}{n} = \\bar{x}\\]\n\\[\\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n-1} = s^2\\]\n\n\n\n\\[\\bar{x} \\sim N(\\mu,\\frac{\\sigma}{\\sqrt{n}})\\]\nWhich when we standardise is the same thing as\n\\[\\frac{\\bar{x} - \\mu}{\\sigma / \\sqrt{n}} \\sim N(0,1)\\]\n\n\n\nThe Student-t distribution is the sampling distribution for the ratio\n\\[\\frac{\\bar{x}-\\mu}{s/\\sqrt{n}} \\sim t(n-1)\\]\nThe expected value and variance of a Student-t distribution is always 0 and 1, respectively. The shape of the density distribution depends on the degrees of freedom of the distribution, which for the ratio above is \\(n-1\\).\n\n\n\nmu = 3.3\nsigma = 2\n\nn = 5\n\nt_sample &lt;- replicate(1000,{\n  x &lt;- rnorm(n,mu,sigma)\n  m &lt;- mean(x)\n  s &lt;- sd(x)\n  (m-mu)/(s/sqrt(n))})\n\n\ntt = seq(min(t_sample),max(t_sample),by=0.01)\npdf_norm = dnorm(tt)\npdf_t = dt(tt,n-1)\nhist(t_sample,prob=TRUE,ylim=c(0,max(pdf_norm)),breaks = 20)\nlines(tt,pdf_norm,col='red')\nlines(tt,pdf_t,col='blue')\n\n\n\n\n\n\n\n\n\n\n\nqqnorm(t_sample, main = \"\")\nqqline(t_sample, col='red')\ntitle('QQ-plot for a Normal')\n\n\n\n\n\n\n\n\n\n\nplot(qt(ppoints(1000),n-1),sort(t_sample))\nabline(0,1,col='blue')\ntitle('QQ-plot for a Student-t')\n\n\n\n\n\n\n\n\n\n\n\nt = seq(-6,6,by=0.01)\npdf_norm = dnorm(tt)\nplot(tt,pdf_norm,type = 'l',xlab='t',ylab='density',col='red')\nn = 5\npdf_t_5 = dt(tt,n-1)\nlines(tt,pdf_t_5,col = 'blue')\nn = 10\npdf_t_10 = dt(tt,n-1)\nlines(tt,pdf_t_10,col = 'green')\ntitle('Student-t goes towards a normal distribution when n increase')\n\n\n\n\n\n\n\n\n\n\n\n\nThe same way as we can create a quantile in a normal distribution based on a quantile from a quantile in a standardised normal distribution \\[\\bar{x}_{1-\\alpha} = \\mu + \\lambda_{\\alpha} \\cdot \\frac{\\sigma}{\\sqrt{n}}\\]\nwe can also create a quantile in the sampling distribution for the sample mean when the variance is unknown and must be estimated by \\(s^2\\)\n\\[\\bar{x}_{1-\\alpha} = \\mu + t_{\\alpha}(n-1) \\cdot \\frac{s}{\\sqrt{n}}\\]\nwhere \\(t_{\\alpha}(n-1)\\) is a quantile in the t-distribution with \\(n-1\\) degrees of freedom."
  },
  {
    "objectID": "F6_notes_ENG.html#population-and-random-samples",
    "href": "F6_notes_ENG.html#population-and-random-samples",
    "title": "F6. Estimation and confidence intervals",
    "section": "",
    "text": "Population\n\nAny random variable (regardless of distribution) has an expected value and variance \\[\\mu = E(X)\\] \\[\\sigma^2 = V(X)=E((X-\\mu)^2) = E(X^2)-\\mu^2\\]\n\nRandom sample\n\nSample mean and sample variance are measures of the location and spread in a sample of a population\n\\[\\bar{x} = \\frac{\\sum x_i}{n}\\]\n\\[s^2 = \\frac{\\sum (x_i-\\bar{x})^2 }{n-1} = \\frac{\\sum x^2 - n(\\bar{x})^2 }{n-1}\\]"
  },
  {
    "objectID": "F6_notes_ENG.html#unbiased-estimates-of-the-expected-value-and-variance",
    "href": "F6_notes_ENG.html#unbiased-estimates-of-the-expected-value-and-variance",
    "title": "F6. Estimation and confidence intervals",
    "section": "",
    "text": "Unbiased estimates are\n\n\\[\\hat{\\mu} = \\frac{\\sum_{i=1}^n x_i}{n} = \\bar{x}\\]\n\\[\\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n-1} = s^2\\]"
  },
  {
    "objectID": "F6_notes_ENG.html#sampling-distribution-for-barx-when-variance-is-known",
    "href": "F6_notes_ENG.html#sampling-distribution-for-barx-when-variance-is-known",
    "title": "F6. Estimation and confidence intervals",
    "section": "",
    "text": "\\[\\bar{x} \\sim N(\\mu,\\frac{\\sigma}{\\sqrt{n}})\\]\nWhich when we standardise is the same thing as\n\\[\\frac{\\bar{x} - \\mu}{\\sigma / \\sqrt{n}} \\sim N(0,1)\\]"
  },
  {
    "objectID": "F6_notes_ENG.html#sampling-distribution-for-barx-when-the-variance-is-unknown-and-estimated-with-s2",
    "href": "F6_notes_ENG.html#sampling-distribution-for-barx-when-the-variance-is-unknown-and-estimated-with-s2",
    "title": "F6. Estimation and confidence intervals",
    "section": "",
    "text": "The Student-t distribution is the sampling distribution for the ratio\n\\[\\frac{\\bar{x}-\\mu}{s/\\sqrt{n}} \\sim t(n-1)\\]\nThe expected value and variance of a Student-t distribution is always 0 and 1, respectively. The shape of the density distribution depends on the degrees of freedom of the distribution, which for the ratio above is \\(n-1\\).\n\n\n\nmu = 3.3\nsigma = 2\n\nn = 5\n\nt_sample &lt;- replicate(1000,{\n  x &lt;- rnorm(n,mu,sigma)\n  m &lt;- mean(x)\n  s &lt;- sd(x)\n  (m-mu)/(s/sqrt(n))})\n\n\ntt = seq(min(t_sample),max(t_sample),by=0.01)\npdf_norm = dnorm(tt)\npdf_t = dt(tt,n-1)\nhist(t_sample,prob=TRUE,ylim=c(0,max(pdf_norm)),breaks = 20)\nlines(tt,pdf_norm,col='red')\nlines(tt,pdf_t,col='blue')\n\n\n\n\n\n\n\n\n\n\n\nqqnorm(t_sample, main = \"\")\nqqline(t_sample, col='red')\ntitle('QQ-plot for a Normal')\n\n\n\n\n\n\n\n\n\n\nplot(qt(ppoints(1000),n-1),sort(t_sample))\nabline(0,1,col='blue')\ntitle('QQ-plot for a Student-t')\n\n\n\n\n\n\n\n\n\n\n\nt = seq(-6,6,by=0.01)\npdf_norm = dnorm(tt)\nplot(tt,pdf_norm,type = 'l',xlab='t',ylab='density',col='red')\nn = 5\npdf_t_5 = dt(tt,n-1)\nlines(tt,pdf_t_5,col = 'blue')\nn = 10\npdf_t_10 = dt(tt,n-1)\nlines(tt,pdf_t_10,col = 'green')\ntitle('Student-t goes towards a normal distribution when n increase')"
  },
  {
    "objectID": "F6_notes_ENG.html#quantile-in-a-t-distribution",
    "href": "F6_notes_ENG.html#quantile-in-a-t-distribution",
    "title": "F6. Estimation and confidence intervals",
    "section": "",
    "text": "The same way as we can create a quantile in a normal distribution based on a quantile from a quantile in a standardised normal distribution \\[\\bar{x}_{1-\\alpha} = \\mu + \\lambda_{\\alpha} \\cdot \\frac{\\sigma}{\\sqrt{n}}\\]\nwe can also create a quantile in the sampling distribution for the sample mean when the variance is unknown and must be estimated by \\(s^2\\)\n\\[\\bar{x}_{1-\\alpha} = \\mu + t_{\\alpha}(n-1) \\cdot \\frac{s}{\\sqrt{n}}\\]\nwhere \\(t_{\\alpha}(n-1)\\) is a quantile in the t-distribution with \\(n-1\\) degrees of freedom."
  },
  {
    "objectID": "F6_notes_ENG.html#interpretation-of-a-confidence-interval",
    "href": "F6_notes_ENG.html#interpretation-of-a-confidence-interval",
    "title": "F6. Estimation and confidence intervals",
    "section": "Interpretation of a confidence interval",
    "text": "Interpretation of a confidence interval\nConfidence intervals occur in frequentistic statistical inference. Their interpretation is how often an interval derived the same way but on new random samples would cover the true parameter value.\nBelow we illustrate a 95% confidence interval with 10 and 20 measurements, where we can see that the measurements are narrower with more measurements.\nThe red intervals corresponds to the intervals not covering the true expected value. This should happen in \\(1-0.95 = 5\\%\\) of the cases.\n\nsource(\"kod/funktioner_raknamedvariation_light.R\")\n\nskattningar(mu=195, sigma=3, n1=10, n2=20, alternativ = 'konfint') \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample: radon continuation\n\n\n\nThe limit for radon in a house is 200 \\(Bq/m^3\\). We make 10 measurements with an average of 195 \\(Bq/m^3\\). Can we be certain that the radon-levels in the house is below the limit?\nAfter calculating the 95% confidence interval \\(I_{\\mu}: (193.1, 196.9)\\), we can say that we are quite confident that the radon level is below the limit.\n\n\n\n\n\n\n\n\nWhere does the word confidence come from and why is not the word probability used?\n\n\n\nTo put it simple, the term probability was taken by Bayesian statistical inference. Probability intervals are used within Bayesian inference with the interpretation how certain we are that the true value on the parameter is in the interval. It is common that the confidence interval is given the same interpretation.\nAnother reason is that those that invented confidence intervals did not want it to be misunderstood as a probability for the true value of the parameter \\(\\mu\\), which according to frequentist way of thinking cannot in itself be a random variable and therefore cannot have a distribution.\nNote that all intervals are not confidence intervals, even if it is common to call them that. For something to be a confidence interval it must be preceded by a model for a population and a derived sampling distribution for the estimated parameter."
  },
  {
    "objectID": "F6_notes_ENG.html#known-variance",
    "href": "F6_notes_ENG.html#known-variance",
    "title": "F6. Estimation and confidence intervals",
    "section": "Known variance",
    "text": "Known variance\nThe confidence interval for expected value when the variance is known is obtained by taking a quantile from the standardised normal distribution multiplied with the standard error \\(\\frac{\\sigma}{\\sqrt{n}}\\)\n\\[I_{\\mu}: \\bar{x} \\pm \\lambda_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\]\nWe want as narrow confidence intervals as possible. The interval becomes narrower when\n\nthe confidence level \\(1-\\alpha\\) is reduced\nthe number of measurements \\(n\\) is increased\nthe spread in measurements \\(\\sigma\\) is reduced"
  },
  {
    "objectID": "F6_notes_ENG.html#unknown-variance",
    "href": "F6_notes_ENG.html#unknown-variance",
    "title": "F6. Estimation and confidence intervals",
    "section": "Unknown variance",
    "text": "Unknown variance\nWhen the variance \\(\\sigma_2\\) is unknown, we estimate it with the sample variance \\(s^2\\)\nSince we have estimated the variance, we use a quantile from the sampling distribution for the ratio \\(t\\) and the standard error \\(\\frac{s}{\\sqrt{n}}\\)\n\\[I_{\\mu}: \\bar{x} \\pm t_{\\alpha/2}(n-1) \\frac{s}{\\sqrt{n}}\\]\n\n\n\n\n\n\nExercise 1\n\n\n\nWe have a random sample from \\(X\\sim N(\\mu,2)\\)\n44.3 45.1 46.1 45.3\nThe sample mean is \\(\\bar{x} = 45.2\\)\n\nProvide a 95% confidence interval with the expected value \\(\\mu\\)\n\n\\(\\alpha = 0.05\\)\nThe standard deviation \\(\\sigma=2\\) is known\n\\[I_{\\mu}: \\bar{x} \\pm \\lambda_{0.05/2} \\frac{\\sigma}{\\sqrt{n}} = 45.2 \\pm 1.96 \\frac{2}{\\sqrt{4}} = (43.24, 47.16)\\]\n\nWhat does a 99% confidence interval looks like?\n\n\\[I_{\\mu}: \\bar{x} \\pm \\lambda_{0.01/2} \\frac{\\sigma}{\\sqrt{n}} = 45.2 \\pm 2.58 \\frac{2}{\\sqrt{4}} = (42.62, 47.78)\\]\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\nWe have 10 measurements from \\(X\\sim N(\\mu,\\sigma)\\)\n7.3 7.2 7.8 7.1 8.0 6.9 7.5 8.1 7.7 7.5\n\nx = c(7.3, 7.2, 7.8, 7.1, 8.0, 6.9, 7.5, 8.1, 7.7, 7.5)\nm = mean(x)\ns = sd(x)\n\nDerive a 95% confidence interval for the expected value \\(\\mu\\)!\nSample mean is \\(\\bar{x} = 7.51\\)\n\\(\\alpha = 0.05\\)\nThe standard deviation of the population \\(\\sigma\\) is unknown and is estimated with the sample standard deviation \\(s = 0.393\\)\n\\[I_{\\mu}: \\bar{x} \\pm t_{0.05/2}(n-1) \\frac{s}{\\sqrt{n}} = 7.51 \\pm 2.26 \\frac{0.393}{\\sqrt{10}} = (7.23,7.79)\\]"
  },
  {
    "objectID": "F7_notes_ENG.html",
    "href": "F7_notes_ENG.html",
    "title": "F7. Hypotesis testing",
    "section": "",
    "text": "Exempel. Drunk driving\n\n\n\nThe limit for drunk driving is 0.2 per mille. At a traffic control, three determinations of the blood alcohol level are made.\nModel: The three measurements \\(x_1, x_2, x_3\\) are assumed to be a sample from the random variable \\(X = \\text{\"measured alcohol level\"}\\), which is assumed to be normally distributed \\(N(\\mu, 0.07)\\), where \\(\\mu\\) is the true alcohol level of the person and 0.07 is a measure of the instrument’s precision.\nFor Kalle, the sample mean were \\(\\bar{x}=0.27\\) per mille. Should he be convicted of drunk driving?"
  },
  {
    "objectID": "F7_notes_ENG.html#confidence-interval",
    "href": "F7_notes_ENG.html#confidence-interval",
    "title": "F7. Hypotesis testing",
    "section": "Confidence interval",
    "text": "Confidence interval\nConfidence interval:\nIf \\(\\sigma\\) is known:\n\\(I_{\\mu}: \\bar{x} \\pm \\lambda_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\)\nIf \\(\\sigma\\) is unknown and estimated with \\(s\\):\n\\(I_{\\mu}: \\bar{x} \\pm t_{\\alpha/2}(n-1)\\frac{s}{\\sqrt{n}}\\)\nRule: Reject \\(H_0\\) if \\(\\mu_0\\) is not in the range \\(I_\\mu\\)"
  },
  {
    "objectID": "F7_notes_ENG.html#test-quantity-with-critical-area",
    "href": "F7_notes_ENG.html#test-quantity-with-critical-area",
    "title": "F7. Hypotesis testing",
    "section": "Test quantity with critical area",
    "text": "Test quantity with critical area\nTest quantity:\nIf \\(\\sigma\\) is known:\n\\(z = |\\frac{\\bar{x}-\\mu_0}{\\sigma/\\sqrt{n}}|\\)\nIf \\(\\sigma\\) is unknown and estimated with \\(s\\):\n\\(t = |\\frac{\\bar{x}-\\mu_0}{s/\\sqrt{n}}|\\)\nRule: Reject \\(H_0\\) if \\(z &gt; \\lambda_{\\alpha/2}\\) alternatively if \\(t &gt; t_{\\alpha/2}(n-1)\\)"
  },
  {
    "objectID": "F7_notes_ENG.html#hypothesis-testing-with-the-direct-method",
    "href": "F7_notes_ENG.html#hypothesis-testing-with-the-direct-method",
    "title": "F7. Hypotesis testing",
    "section": "Hypothesis testing with the direct method",
    "text": "Hypothesis testing with the direct method\nRule: Reject \\(H_0\\) if\n\\(\\begin{split} & \\text{p-value} = \\\\&  P(\\text{\"to get what we got or worse\"}|H_0 \\text{ is true}) &lt; \\alpha \\end{split}\\)\n\n\n\n\n\n\nExample. Drunk driving\n\n\n\nHypotheses:\n\\(H_0: \\mu \\leq 0.2\\) against \\(H_1: \\mu &gt; 0.2\\)\n\\(\\begin{split} & \\text{p-value} = P(\\bar{x} \\geq 0.27|\\mu = 0.2) = \\\\ &  P(\\frac{\\bar{x}-\\mu}{\\sigma/\\sqrt{n}} \\geq \\frac{0.27 - 0.2}{0.07/\\sqrt{3}}) = P(Z \\geq \\sqrt{3}) =  1-\\Phi(\\sqrt{3}) = \\\\ & 1 - \\Phi(1.73) = 1 - 0.958 =0.042 \\end{split}\\)\nRule: Reject \\(H_0\\) because the p-value is less than the chosen significance level of 0.05.\nIf we had chosen a significance level of 0.01 instead, we would not have rejected instead because the p-value is higher than the chosen significance level.\n\n\n\n\n\n\n\n\nDo not change the significance level during the test\n\n\n\nYou must select a significance level before doing the test. It is unfortunately common for researchers/statisticians to choose the lowest significance level which leads to the rejection of the null hypothesis, but this leads in the long run to overconfidence in results.\nSignificance level is determined when planning a study."
  },
  {
    "objectID": "F7_notes_ENG.html#power-definition",
    "href": "F7_notes_ENG.html#power-definition",
    "title": "F7. Hypotesis testing",
    "section": "Power definition",
    "text": "Power definition\nThe probability of rejecting the null hypothesis when it is false is the power of a test.\nThe strength depends on the true value of the parameter being tested.\n\\(S(\\mu) = P(\\text{Reject }H_0 |\\mu\\text{ is the true expected value})\\)\nThe power is \\(1-\\beta = 1 - P(\\text{Type II error})\\)\n\n\n\n\n\n\nExample: Drunk driving\n\n\n\nHypotheses: \\(H_0: \\mu = 0.2\\) against \\(H_1: \\mu &gt; 0.2\\)\nSignificance level: \\(\\alpha = 0.05\\)\nTest rule: Reject \\(H_0\\) if \\(\\bar{x} &gt; \\mu_0 + \\lambda_{\\alpha}\\frac{\\sigma}{\\sqrt{n}} = 0.2 + 1.64 \\frac{0.07}{\\sqrt{3}} = 0.27\\)\n\\(\\begin{split} & S(\\mu) = P(\\bar{x} &gt; 0.27|\\mu) = P(\\frac{\\bar{x}-\\mu}{\\sigma/\\sqrt{n}} &gt; \\frac{0.27-\\mu}{0.07/\\sqrt{3}}) = // & P(Z &gt; \\frac{0.27-\\mu}{0.07/\\sqrt{3}}) = 1 - \\Phi(\\frac{0.27-\\mu}{0.07/\\sqrt{3}}) \\end{split}\\)\n\ndf &lt;- data.frame(mu = seq(0.1,0.4,by=0.005)) # possible mu-values\nn = 3\ntest_quantity = 0.2+1.64*0.07/sqrt(n)\ndf$styrka_mu_n3 &lt;- 1 - pnorm((test_quantity-df$mu)/(0.07/sqrt(n))) # power when n=3\nn = 6\ntest_quantity = 0.2+1.64*0.07/sqrt(n)\ndf$styrka_mu_n6 &lt;- 1 - pnorm((test_quantity-df$mu)/(0.07/sqrt(n)))\n\nggplot(df,aes(x=mu,y=styrka_mu_n3)) +\n  geom_line() +\n  geom_line(aes(x=mu,y=styrka_mu_n6), col = 'blue') +\n  ylab(\"P(Kalle is convicted)\") +\n  xlab(\"Actual level in Kalle\") +\n  ggtitle(\"The power function of the test\") +\n  annotate(\"text\",x = 0.3, y = 0.6, label = \"n = 3\") + \n  annotate(\"text\",x = 0.25, y = 0.75, label = \"n = 6\", col = \"blue\")\n\n\n\n\n\n\n\n\n\n\nThe power function is influenced by\n-The true value of the parameter \\(\\mu\\)\n\nThe sample size \\(n\\)\nThe variance in the population \\(\\sigma^2\\)\nThe level of significance - the lower \\(\\alpha\\), the worse strength\n\n\nA hypothesis test involves a trade-off between making type I and type II errors\n\n\nhypotes(sigma=0.07,n=3,mu0=0.2,alfa=0.05,riktning=\"&gt;\",mu.sant = 0.35)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample: Food coloring\n\n\n\nA pharmaceutical manufacturer uses a certain food coloring. You want to know how the color affects the appearance of the medicine. Without the dye, turbidity is usually 4.0.\nModel: \\(X = \\text{\"Turbidity\"}\\) \\(X \\sim N(\\mu, 0.2)\\)\nTurbidity is measured on \\(n=10\\) randomly selected tablets. The sample mean of the turbidity was \\(\\bar{x}=4.1\\).\n\nTest if the drug is to turbid at a significance level \\(\\alpha = 0.05\\)\n\nHypotheses: \\(H_0: \\mu = 4.0\\) against \\(H_1: \\mu &gt; 4.0\\)\nTest rule: Reject \\(H_0\\) if \\(\\bar{x} &gt; \\mu_0 + \\lambda_{0.05}\\frac{\\sigma}{\\sqrt{n}} = 4.0 + 1.64 \\frac{0.2}{\\sqrt{10}} = 4.104\\)\n⇒ We cannot reject \\(H_0\\)\n\nWhat is the power of the test when the turbidity is 3.8?\n\n\\(\\begin{split} & S(\\mu) = P(\\text{Reject }H_0|\\mu = 4.3) = \\\\ & P(\\bar{x} &gt; 4.104|\\mu = 4.3) = P(\\frac{\\bar{x}-\\mu}{\\sigma/\\sqrt{n}} &gt; \\frac{4.104-4.3}{0.2/\\sqrt{10}}) = \\\\ &  P(Z &gt;-3.1) = 1-\\Phi(-3.1) = \\Phi(3.1) = 0.999 \\end{split}\\)\n⇒ The power is 0.999"
  },
  {
    "objectID": "F8_notes_ENG.html",
    "href": "F8_notes_ENG.html",
    "title": "F8. Two samples",
    "section": "",
    "text": "Both ways to assign a distribution for a random variable are used:\n\\[X \\sim N(\\mu,\\sigma)\\]\n\\[X \\in N(\\mu,\\sigma)\\]\nUllrika uses \\(\\sim\\) with the interpretation “distributed as” and \\(\\in\\) with the interpretation “an element in”.\nIt it ok to use \\(X \\in N(\\mu,\\sigma)\\)."
  },
  {
    "objectID": "F8_notes_ENG.html#chi2-distribution",
    "href": "F8_notes_ENG.html#chi2-distribution",
    "title": "F8. Two samples",
    "section": "\\(\\chi^2\\)-distribution",
    "text": "\\(\\chi^2\\)-distribution\nA sum of squared \\(k\\) independent standardised normal distributed random variables follows a \\(\\chi^2\\)-distribution with \\(k\\) degrees of freedom.\nIf \\(Z_i\\sim N(0,1)\\) and all \\(i=1,\\dots,k\\) are independent then\n\\[\\sum_{i=1}^k Z_i^2\\sim \\chi^2(k)\\]"
  },
  {
    "objectID": "F8_notes_ENG.html#a-simulation-of-chi2-distribution-from-n01",
    "href": "F8_notes_ENG.html#a-simulation-of-chi2-distribution-from-n01",
    "title": "F8. Two samples",
    "section": "A simulation of \\(\\chi^2\\)-distribution from N(0,1)",
    "text": "A simulation of \\(\\chi^2\\)-distribution from N(0,1)\n\n# simul av eation of chi2 with 1 degrees of freedom\nhist((rnorm(10^4))^2,probability=TRUE,main=latex2exp::TeX(\"$\\\\chi^2(1)$\"))\nxx = seq(0,25,by = 0.1)\nlines(xx,dchisq(xx,1),col='darkred')\n\n\n\n\n\n\n\n\n\n# simulation of a chi2 with four degress of freedom\nhist((rnorm(10^4))^2+(rnorm(10^4))^2+(rnorm(10^4))^2+(rnorm(10^4))^2,probability=TRUE,ylim = c(0,0.2),main=latex2exp::TeX(\"$\\\\chi^2(4)$\"))\nxx = seq(0,25,by = 0.1)\nlines(xx,dchisq(xx,4),col='darkred')"
  },
  {
    "objectID": "F8_notes_ENG.html#sampling-distribution-for-the-sample-variance-1",
    "href": "F8_notes_ENG.html#sampling-distribution-for-the-sample-variance-1",
    "title": "F8. Two samples",
    "section": "Sampling distribution for the sample variance",
    "text": "Sampling distribution for the sample variance\nImagine that we repeatedly make new observations of a normally distributed random variable \\(X \\sim N(\\mu,\\sigma)\\) and calculate the sample variance. In the same way as we can consider the sample mean \\(\\bar{x}\\) as a random variable, we can also do so with the sample variance \\(s^2\\).\n\\[s^2 = \\frac{\\sum_{i=1}^n (X_i-\\bar{x})^2}{n-1}\\]\nWe rewrite it to a sum \\((n-1)s^2\\). If we then divide by the true value of the variance we get the ratio\n\\[\\frac{(n-1)s^2}{\\sigma^2} = \\frac{\\sum_{i=1}^n (X_i-\\bar{x})^2}{\\sigma^2}\\]\nThe ratio can be written as a sum of the square root of random variables. Because we subtract the estimate of the expected value and divide by the variance, one can show that it is a sum of squared standardised random variables, however only \\(n-1\\) independent ones. We will not go through this proof in this course, but one can show that the ratio follows a \\(\\chi^2\\)-distribution:\n\\[\\frac{(n-1)s^2}{\\sigma^2} \\sim \\chi^2(n-1)\\]"
  },
  {
    "objectID": "F8_notes_ENG.html#simulation-of-the-sampling-distribution-of-the-sample-variance",
    "href": "F8_notes_ENG.html#simulation-of-the-sampling-distribution-of-the-sample-variance",
    "title": "F8. Two samples",
    "section": "Simulation of the sampling distribution of the sample variance",
    "text": "Simulation of the sampling distribution of the sample variance\n\n# Simulation of the sampling distribution of the sample variance\n\nmu = 3.3 #choose what you want\nsigma2 = 20 #choose what you want\nn = 10 #sample size\nniter = 1e4 #number of times we \"make\" new observations\nsam &lt;- replicate(niter,\n  {\n  x &lt;- rnorm(n,mu,sqrt(sigma2))\n  s2 &lt;- var(x)\n  s2*(n-1)/sigma2\n  }\n)\n\nhist(sam,prob = TRUE,main=latex2exp::TeX(\"$\\\\chi\\\\^2(n-1)\"),xlab = latex2exp::TeX(\"(n-1)s\\\\^2/\\\\sigma\\\\^2\"))\npp &lt;- ppoints(200)\nxx &lt;- qchisq(pp,n-1)\nyy &lt;- dchisq(xx,n-1)\nlines(xx,yy,col='darkred')\n\n\n\n\n\n\n\n\nNote that the ratio between the sample variance and the real variance can be rewritten to be a \\(\\chi^2\\)-distributed random variable divided by \\(n-1\\)\n\\[\\frac{s^2}{\\sigma^2}=\\frac{(n-1)s^2}{(n-1)\\sigma^2}=\\frac{\\frac{(n-1)s^2}{\\sigma^2}}{(n-1)} \\tag{1}\\]"
  },
  {
    "objectID": "F8_notes_ENG.html#two-independent-samples",
    "href": "F8_notes_ENG.html#two-independent-samples",
    "title": "F8. Two samples",
    "section": "Two independent samples",
    "text": "Two independent samples\n\\(X_i = \\text{\"oxygen saturation in patient i receiving the medicine}\\)\n\\(Y_j = \\text{\"oxygen saturation in patient j receiving a placebo\"}\\)\nStrategy: The study needs to be conducted so one can assume that \\(X_i\\), \\(i=1,\\dots,n_x\\) och \\(Y_j\\), \\(j=1,\\dots,n_y\\) are independent, e.g. by randomly placing the patients in the two groups (randomise)."
  },
  {
    "objectID": "F8_notes_ENG.html#samples-in-pairs",
    "href": "F8_notes_ENG.html#samples-in-pairs",
    "title": "F8. Two samples",
    "section": "Samples in pairs",
    "text": "Samples in pairs\n\\(X_i = \\text{\"oxygen saturation in patient i before treatment\"}\\)\n\\(Y_i = \\text{\"oxygen saturation in patient i after treatment\"}\\)\nStrategy: Make a new random variable \\(\\Delta_i = X_i - Y_i\\) and continue with the statistical inference on \\(\\Delta_i\\).\n\n\n\n\n\n\nExample: Does alcohol consumption increase fat in the liver?\n\n\n\nTwelve test subjects were selected, who can be considered a random sample among healthy individuals in their mid-twenties. The test subjects have abstained from all alcohol consumption for an extended period, and samples of their livers have been taken. Subsequently, they were given to drink 4 cans of beer per day, and after one month, new liver tests were conducted. The following liver fat values were obtained:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPerson nr\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\nBefore\n0.25\n0.19\n0.13\n0.23\n0.15\n0.14\n0.24\n0.23\n0.17\n0.15\n0.10\n0.17\n\n\nAfter\n0.50\n0.28\n0.18\n0.18\n0.34\n0.41\n0.33\n0.26\n0.35\n0.42\n0.22\n0.29\n\n\nDifference After-Before\n0.25\n0.09\n0.05\n-0.05\n0.19\n0.27\n0.09\n0.03\n0.18\n0.27\n0.12\n0.12\n\n\n\n\n\nModel: \\(X=\\text{\"Change in fat level\"} \\sim N(\\mu,\\sigma)\\)\nThis looks like a reasonable model when we compare theoretical to empirical quantiles:\n\ndiff &lt;- c(0.25, 0.09, 0.05, -0.05, 0.19, 0.27, 0.09, 0.03, 0.18, 0.27, 0.12, 0.12)\nqqnorm(diff)\nqqline(diff)\n\n\n\n\n\n\n\n\nSpecify suitable hypotheses and test if the data supports the suspicion of increased liver fat from alcohol consumption.\nHypotheses:\n\\(H_0: \\mu = 0\\) (Alcohol does not influence liver fat)\n\\(H_1: \\mu &gt; 0\\) (Alcohol influence liver fat)\n\nTest quantity with critical area\n\nIf \\(\\sigma\\) is known, we create \\(z = \\frac{\\bar{x}-0}{\\sigma/\\sqrt{12}}\\)\n\nReject \\(H_0\\) at the significance level \\(\\alpha = 0.05\\) if \\(z &gt; \\lambda_{\\alpha}\\)\n\nIf \\(\\sigma\\) is unknown, create \\(t = \\frac{\\bar{x}-0}{s/\\sqrt{12}}\\)\n\nReject \\(H_0\\) at the significance level \\(\\alpha = 0.05\\) if \\(t &gt; t_{\\alpha}(n-1)\\)\nIn this example, the standard deviation is unknown. We reject \\(H_0\\) since \\(t = \\frac{ 0.1342-0}{0.1008/\\sqrt{12}} = 4.61 &gt; 1.8\\)\n\n\nTest with confidence interval\nWe create a confidence for the expected value bounded from below.\n\\(\\begin{split} I_\\mu: & (\\bar{x} - t_{\\alpha}(n-1)\\frac{s}{\\sqrt{n}},\\infty) = \\\\ & (0.1342 - 1.796 \\frac{0.1008}{\\sqrt{12}}, \\infty) = (0.082,\\infty) \\end{split}\\)\nWe reject \\(H_0\\) at a significance level \\(\\alpha = 0.05\\) because the 95% confidence interval does not cover 0.\n\n\nTest with the direct method\nIf \\(H_0\\) is true it holds that \\(t=\\frac{\\bar{x}-0}{s/\\sqrt{n}} \\sim t(n-1)\\) (t-distributed).\n\\(P(\\text{\"what we got or worse\"}|H_0) = P(t &gt; 4.61) =\\) \\(1 - P(t \\leq 4.61) = 1 - 0.9996 = 4\\times 10^{-4}\\)\nWe reject \\(H_0\\) at the significance level \\(\\alpha = 0.05\\) because the \\(p\\)-value \\(&lt; \\alpha\\)."
  },
  {
    "objectID": "F8_notes_ENG.html#why-use-p-values",
    "href": "F8_notes_ENG.html#why-use-p-values",
    "title": "F8. Two samples",
    "section": "Why use p-values",
    "text": "Why use p-values\n\np-value is often included in print outs from statistical programs\nit is handy in situations when it is not possible to make a normal approximation\n\n\n\n\n\n\n\nExample. Disease Cases\n\n\n\nIn an area located near a refinery, during a 5-year period, there were 9 cases of leukemia compared to an “expected” 4.3 cases. Is the area more affected than the rest of the country? For a “rare” disease, the variation in the number of disease cases can often be described by a Poisson distribution.\nModel: \\(X = \\text{\"number of leukemia cases in the area\"} \\sim Po(\\lambda)\\)\nHypotheses:\n\\(H_0: \\lambda = 4.3\\)\n\\(H_1: \\lambda &gt; 4.3\\)\nIf \\(H_0\\) is true it holds that \\(X\\sim Po(4.3)\\)\nWe observed \\(x = 9\\) cases. How likely is it under the \\(H_0\\)-distribution?\n\n\n\n\n\n\n\n\n\n\n\n\nUse the p-value when it is not possible to do normal approximation\nTest the hypotheses with a significance level at \\(\\alpha=0.05\\).\n\\(\\begin{split} p\\text{-value} & = P(\\text{\"what we got or worse\"}) = P(X\\geq 9) = \\\\& 1-P(X\\leq 8) = 0.032 \\end{split}\\)\nWe can reject \\(H_0\\) because the \\(p\\)-value &lt; 0.05.\nIf we instead had chosen a significance level of \\(\\alpha = 0.01\\), then we would not have been able to reject \\(H_0\\) because the \\(p\\)-value &gt; 0.01.\n\n\n\n\n\n\nWarning\n\n\n\n\nthe \\(p\\)-value is not the probability that the null-hypothesis is true\none have to choose a significance level even if you use the direct method\nthe \\(p\\)-value does not say anything about how large the difference is\n\n\n\n\n\nWhat would it look like if we could do a normal approximation\nWe can do a normal approximation of a Poisson distribution when the expected value \\(\\lambda &gt; 15\\). What happens if we instead had a more common disease?\nModel: \\(Y = \\text{\"number of flu cases in the area\"} \\sim Po(\\lambda)\\)\nHypotheses:\n\\(H_0: \\lambda = 16\\)\n\\(H_1: \\lambda &gt; 16\\)\nIf \\(H_0\\) is true then \\(X\\sim Po(16)\\)\nWe observed \\(x = 30\\) cases. How likely is it under the \\(H_0\\)-distribution?\nTest the hypotheses with a significance level of \\(\\alpha=0.05\\).\n\\(\\begin{split} p\\text{-value} & = P(\\text{\"what we got or worse\"}) = P(Y\\geq 30) = \\\\& 1-P(Y\\leq 29) = 1 - P(\\frac{Y-\\lambda}{\\sqrt{\\lambda}} \\leq \\frac{29-16}{4}) = \\\\ & 1 - P(Z \\leq 3.25) \\overset{A} = 1 -\\Phi(3.25) = 0.001 \\end{split}\\)\nWe can reject \\(H_0\\) because the \\(p\\)-value &lt; 0.05."
  },
  {
    "objectID": "F8_notes_ENG.html#test-the-difference-in-expected-values",
    "href": "F8_notes_ENG.html#test-the-difference-in-expected-values",
    "title": "F8. Two samples",
    "section": "Test the difference in expected values",
    "text": "Test the difference in expected values\nModel:\n\\(X = \\text{\"liver fat without the medicine\"} \\sim N(\\mu_x,\\sigma_x)\\)\n\\(Y = \\text{\"lever fat with the medicine\"} \\sim N(\\mu_y,\\sigma_y)\\)\nWe assume that the variances are equal \\(\\sigma^2_x=\\sigma^2_y=\\sigma^2\\)\nHypotheses: We want to test\n\\(H_0: \\mu_x = \\mu_y\\)\n\\(H_1: \\mu_x \\neq \\mu_y\\)\nThis is the same thing as\n\\(H_0: \\mu_x - \\mu_y = 0\\)\n\\(H_1: \\mu_x - \\mu_y \\neq 0\\)\nEstimation: We estimate the expected values with the sample means \\(\\hat{\\mu}_x = \\bar{x}= 148.2\\) and \\(\\hat{\\mu}_y = \\bar{y}= 151.7\\)\nTest rule: We test the hypotheses at a significance level \\(\\alpha\\) by creating a \\((1-\\alpha)\\cdot 100\\)% confidence interval for the difference in the expected values.\nThe confidence interval we create shall have the form estimation plus/minus a suitable quantile multiplied with the standard deviation for the estimate based on the sampling distributions.\n\\[I_{\\mu_x - \\mu_y} = \\hat{\\mu}_x-\\hat{\\mu}_y \\pm \\text{quantile}\\cdot\\sqrt{V(\\hat{\\mu}_x-\\hat{\\mu}_y)}\\]"
  },
  {
    "objectID": "F8_notes_ENG.html#the-variance-for-the-estimate-of-the-difference-in-the-expected-values",
    "href": "F8_notes_ENG.html#the-variance-for-the-estimate-of-the-difference-in-the-expected-values",
    "title": "F8. Two samples",
    "section": "The variance for the estimate of the difference in the expected values",
    "text": "The variance for the estimate of the difference in the expected values\n\\(\\begin{split} & V(\\hat{\\mu}_x-\\hat{\\mu}_y) = V(\\bar{x}-\\bar{y}) \\underset{assume \\\\ independence} = V(\\bar{x})+V(\\bar{y}) = \\\\ & \\frac{\\sigma_x^2}{n_x} + \\frac{\\sigma_y^2}{n_y} \\underset{equal \\\\ variance} = \\frac{\\sigma^2}{n_x} + \\frac{\\sigma^2}{n_y} = \\sigma^2 (\\frac{1}{n_x} + \\frac{1}{n_y}) \\end{split}\\)\n\nHow to estimate the variance with two samples? By combining (pool) the squared distances to respective sample mean.\n\n\\[s^2_{pooled}=\\frac{\\sum (x_i-\\bar{x})^2 + \\sum (y_i-\\bar{y})^2}{n_x+n_y-2} = \\frac{(n_x-1)s_x^2 + (n_y-1)s_y^2}{n_x+n_y-2}\\]\nwhere \\(s_x^2 = \\frac{\\sum (x_i-\\bar{x})^2}{n-1}\\) and \\(s_y^2\\) can be found by running a routine on a hand calculator or a computer.\n\\(s^2_x = 10.0^2\\), \\(s^2_y = 8.0^2\\)\n\\(n_x = 50\\), \\(n_y = 25\\)\n\\(s^2_{pooled} = \\frac{49\\cdot 10.0^2 + 24\\cdot 8.0^2}{50+25-2} = 88.2\\)"
  },
  {
    "objectID": "F8_notes_ENG.html#sampling-distribution-for-the-estimate-of-the-difference-in-the-expected-values",
    "href": "F8_notes_ENG.html#sampling-distribution-for-the-estimate-of-the-difference-in-the-expected-values",
    "title": "F8. Two samples",
    "section": "Sampling distribution for the estimate of the difference in the expected values",
    "text": "Sampling distribution for the estimate of the difference in the expected values\nBecause both \\(X\\) and \\(Y\\) are normally distributed, so will both sampling distributions for the sample means \\(\\bar{x}\\) och \\(\\bar{y}\\). So will also the difference between these, i.e.  \\[\\bar{x} - \\bar{y} \\sim N(\\mu_x - \\mu_y, \\sqrt{\\sigma^2 (\\frac{1}{n_x} + \\frac{1}{n_y})})\\]"
  },
  {
    "objectID": "F8_notes_ENG.html#confidence-interval-of-the-difference-in-expected-values",
    "href": "F8_notes_ENG.html#confidence-interval-of-the-difference-in-expected-values",
    "title": "F8. Two samples",
    "section": "Confidence interval of the difference in expected values",
    "text": "Confidence interval of the difference in expected values\nBecause the variance \\(\\sigma^2\\) is unknown and estimated with the samples, we will use a t-distribution with \\(n_x+n_y-2\\) degrees of freedom when we create the confidence interval.\n\\[I_{\\mu_x - \\mu_y} = \\bar{x}-\\bar{y} \\pm t_{\\alpha/2}(n_x+n_y-2)\\cdot\\sqrt{s^2_{pooled}(\\frac{1}{n_x} + \\frac{1}{n_y})}\\]\n\n\n\n\n\n\nExample. Compare treatements (cont.)\n\n\n\n\\[\\begin{split} I_{\\mu_x - \\mu_y} & = 148.2-151.7 \\pm \\underbrace{t_{0.05/2}(50+25-2)}_{1.99}\\sqrt{88.2(\\frac{1}{50} + \\frac{1}{25})}   = \\\\ & (-8.08,1.08) \\end{split}\\] We do not reject \\(H_0\\) at the significance level 5% because the 95%-th confidence interval covers zero."
  },
  {
    "objectID": "F8_notes_ENG.html#test-if-equal-variance",
    "href": "F8_notes_ENG.html#test-if-equal-variance",
    "title": "F8. Two samples",
    "section": "Test if equal variance",
    "text": "Test if equal variance\nIt will be a stronger test if one can assume equal variance in the populations (e.g. \\(X\\) and \\(Y\\)). It is often not enough to assume equal variance, and then one has to test this assumption.\nHypotheses:\n\\(H_0: \\sigma^2_x = \\sigma^2_y\\)\n\\(H_1: \\sigma^2_x \\neq \\sigma^2_y\\)"
  },
  {
    "objectID": "F8_notes_ENG.html#sampling-distribution-for-the-ratio-between-sample-variances",
    "href": "F8_notes_ENG.html#sampling-distribution-for-the-ratio-between-sample-variances",
    "title": "F8. Two samples",
    "section": "Sampling distribution for the ratio between sample variances",
    "text": "Sampling distribution for the ratio between sample variances\nF-distribution\nAn F-distribution is, like the t-distribution, a (by Fisher) constructed sampling distribution, that has shown to be useful in many situations. It is the distribution for the ratio between two \\(\\chi^2\\)-distributed random variables.\n\\[\\frac{\\chi_1^2/\\nu_1}{\\chi_2^2/\\nu_2} \\sim F(\\nu_1,\\nu_2)\\]\nwhere \\(\\nu_1\\) and \\(\\nu_2\\) are the degrees of freedom for the F-distribution."
  },
  {
    "objectID": "F8_notes_ENG.html#the-ratio-between-two-sample-variances",
    "href": "F8_notes_ENG.html#the-ratio-between-two-sample-variances",
    "title": "F8. Two samples",
    "section": "The ratio between two sample variances",
    "text": "The ratio between two sample variances\nLet us create the following ratio and use what we showed in Equation 1\n\\[\\frac{s^2_x/\\sigma_x^2}{s^2_y/\\sigma_y^2}=\\frac{\\frac{(n_x-1)s^2_x/\\sigma_x^2}{n_x-1}}{\\frac{(n_y-1)s^2_y/\\sigma_y^2}{n_y-1}} \\sim F(n_x-1,n_y-1)\\]\nIf the populations \\(X\\) and \\(Y\\) are normally distributed, or if \\(n_x\\) and \\(n_y\\) are sufficiently large to apply the Central Limit Theorem, both the nominator and the denominator will be ratios of \\(\\chi^2\\)-distributed random variables and their corresponding degrees of freedom. This implies that the ratio follows a F-distribution with \\(n_x-1\\) and \\(n_y-1\\) degrees of freedom.\nBut we do not know the variances!"
  },
  {
    "objectID": "F8_notes_ENG.html#the-ratio-under-h_0",
    "href": "F8_notes_ENG.html#the-ratio-under-h_0",
    "title": "F8. Two samples",
    "section": "The ratio under \\(H_0\\)",
    "text": "The ratio under \\(H_0\\)\nThe variances are equal under \\(H_0\\), and we can simplify the ratio since the true variances cancels each other out. The F-distribution is the sampling distribution for the ratio \\(F\\) of the sample variances under the null hypothesis.\n\\[F = \\frac{s^2_x/\\sigma^2}{s^2_y/\\sigma^2} = \\frac{s^2_x}{s^2_y} \\sim F(n_x-1,n_y-1)\\]"
  },
  {
    "objectID": "F8_notes_ENG.html#test-rule",
    "href": "F8_notes_ENG.html#test-rule",
    "title": "F8. Two samples",
    "section": "Test rule",
    "text": "Test rule\nWe reject \\(H_0\\) with a chosen significance level \\(\\alpha\\) by comparing the test quantity \\(F\\) with a quantile from the F-distribution:\n\\[F = \\frac{s^2_x}{s^2_y} &gt; F_{\\alpha}(n_x-1,n_y-1)\\]\nwhere the quantile is defined based on the probability area above the quantile.\n\n\n\n\n\n\n\n\nComparison between treatments (cont.)\n\n\n\nLet us test on a significance level \\(\\alpha=0.05\\) if the assumption of equal variances is holds.\n\\(F = \\frac{s^2_x}{s^2_y} = \\frac{10.0^2}{8.0^2} = 1.5625\\)\n\\(H_0\\) is not rejected because the test quantity is inside the sampling distribution under \\(H_0\\), i.e. \\(F &lt; F_{0.025}(n_x-1,n_y-1) = 2.11\\)\n\nIf you cannot calculate exactly or the degrees of freedom are missing from the table, then from the degrees of freedom that are the closest. In this case \\(f_1 = 50\\) and \\(f_2=24\\) for \\(\\alpha=0.05\\)\n\n\n\nTip - The F-distribution is not symmetric. You can still compare to just one quantile even if you have a two-sided hypothesis, by making sure the largest sample variance is in the nominator and that you have the correct order of the degrees of freedom."
  },
  {
    "objectID": "F8_notes_ENG.html#summary-comparing-two-samples",
    "href": "F8_notes_ENG.html#summary-comparing-two-samples",
    "title": "F8. Two samples",
    "section": "Summary comparing two samples",
    "text": "Summary comparing two samples\nWe have observations from two statistical populations defined by the random variables \\(X\\) and \\(Y\\), where we are interested in comparing the expected values between the populations\n\nAre the observations paired samples?\n\n\nIf yes, form pairwise differences \\(\\Delta_i = X_i - Y_i\\), discard the old samples, and continue only with the sample of differences. Formulate a model for the difference and set up hypotheses for the expected value of the difference. Since we discard the old ones, we can call the random variable for the differences whatever we want, such as \\(X\\). In the table, you see different ways to test for the random variable \\(X\\) with expected value \\(\\mu\\) and standard deviation \\(\\sigma\\).\n\n\nSelect an appropriate quantile depending on whether the differences are normally distributed or not, whether you can use the Central Limit Theorem or not, and whether the variance of the difference is known or not.\n\n\n\n\n\n\n\n\n\n\nConfidence interval for \\(\\mu\\)\nTest quantity\n\n\n\n\nPopulation normal\n\\(\\sigma\\) is known\n\\(\\bar{x}\\pm \\lambda_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\)\n\\(z = \\frac{\\bar{x}-\\mu}{\\sigma/\\sqrt{n}}\\)\n\n\nPopulation normal\n\\(\\sigma\\) is unknown, \\(n\\) small\n\\(\\bar{x}\\pm t_{\\alpha/2}(n-1)\\frac{s}{\\sqrt{n}}\\)\n\\(t = \\frac{\\bar{x}-\\mu}{s/\\sqrt{n}}\\)\n\n\nNo assumption that pop normal\n\\(\\sigma\\) is unknown, \\(n\\) large\n\\(\\bar{x}\\pm \\lambda_{\\alpha/2}     \\frac{s}{\\sqrt{n}}\\)\n\\(z = \\frac{\\bar{x}-\\mu}{s/\\sqrt{n}}\\)\n\n\n\n\nIf no (the observations are not from paired samples), specify a model for \\(X\\) and \\(Y\\) separately and formulate hypotheses for the difference in expected values.\n\n\nThe minimal way to specify a model is to define \\(X\\) and \\(Y\\) and describe that the expected value and standard deviation for \\(X\\) is \\(\\mu_x\\) and \\(\\sigma_x\\), and that the expected value and standard deviation for \\(Y\\) is \\(\\mu_y\\) and \\(\\sigma_y\\).\n\n\nAre the variances equal?\n\n\nIf yes, use a normal quantile and continue with the hypotheses testing.\nIf not, make assumptions about the variances and estimate based on the assumption.\n\n\nCan we assume equal variance?\n\n\nIf yes, estimate the variance by pooling the sample variances \\(s^2_{pooled}\\). If normally distributed populations, then use a suitable quantile from a normal distribution i the testing. Test if the variances are equal with an F-test.\nIf no, estimate the variances separately. If normally distributed populations or if sample sizes are large enough to apply the Central Limit Theorem, use a suitable quantile from a t-distribution in the testing.\n\n\n\n\n\n\n\n\n\n\nConfidence interval for \\(\\mu_x-\\mu_y\\)\nTest quantity\n\n\n\n\nBoth pop. normal\n\\(\\sigma_x\\) and \\(\\sigma_y\\) are known\n\\(\\bar{x}-\\bar{y}\\pm\\lambda_{\\alpha/2}\\sqrt{\\frac{\\sigma_x^2}{n_x}+\\frac{\\sigma_y^2}{n_y}}\\)\n\\(z = \\frac{(\\bar{x}-\\bar{y})-(\\mu_x-\\mu_y)}{\\sqrt{\\frac{\\sigma_x^2}{n_x}+\\frac{\\sigma_y^2}{n_y}}}\\)\n\n\nBoth pop. normal \\(\\sigma_x\\) and \\(\\sigma_y\\) are unknown, \\(n_x\\) and \\(n_y\\) small\n\\(\\bar{x}-\\bar{y}\\pm t_{\\alpha/2}(n_x+n_y-2)s_{pooled}\\sqrt{\\frac{1}{n_x}+\\frac{1}{n_y}}\\)\n\\(t = \\frac{(\\bar{x}-\\bar{y})-(\\mu_x-\\mu_y)}{s_{pooled}\\sqrt{\\frac{1}{n_x}+\\frac{1}{n_y}}}\\)\n\n\nNo assumption of pop. normal \\(\\sigma_x\\) and \\(\\sigma_y\\) are unknown, \\(n_x\\) and \\(n_y\\) large\n\\(\\bar{x}-\\bar{y}\\pm\\lambda_{\\alpha/2}\\sqrt{\\frac{s_x^2}{n_x}+\\frac{s_y^2}{n_y}}\\)\n\\(z = \\frac{(\\bar{x}-\\bar{y})-(\\mu_x-\\mu_y)}{\\sqrt{\\frac{s_x^2}{n_x}+\\frac{s_y^2}{n_y}}}\\)\n\n\n\n\\(s^2_{pooled} = \\frac{(n_x-1)s_x^2 + (n_y-1)s_y^2}{n_x+n_y-2}\\)\n\nIf normal distribution does not apply or normal approximation with CLT is not justified:\n\n\nMake an appropriate transformation to achieve faster convergence to normal distribution.\nUse the direct method and calculate the p-value without normal approximation."
  },
  {
    "objectID": "F9_notes_ENG.html",
    "href": "F9_notes_ENG.html",
    "title": "F9. Statistical inference on discrete and categorical data",
    "section": "",
    "text": "Test by assuming or not assuming a distribution for the population\n\n\n\n\n\n\nExample. Energy consumption\n\n\n\nOne wants to investigate whether energy expenditure at rest differs between individuals affected by cystic fibrosis compared to healthy individuals. Ten pairs were matched, with one individual in each pair having the disease while the other was healthy. Otherwise, the individuals in each pair were similar in terms of gender, age, weight, and height. Results in energy expenditure (kcal/day):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPair\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\nCF\n1153\n1132\n1165\n1460\n1634\n1493\n1358\n1453\n1185\n1824\n\n\nHealthy\n996\n1080\n1182\n1452\n1162\n1619\n1140\n1123\n1113\n1463\n\n\nDifference CF-Healthy\n157\n52\n-17\n8\n427\n-126\n218\n330\n72\n361\n\n\n\nThe sample size \\(n=10\\) is to small for using the Central Limit Theorem to approximate to a normal distribution.\n\nWhat do we do if it is reasonable to assume that the differences come from a normal distribution?\nWhat do we do without the assumption of a distribution?\n\n\n\n\n\nLet us check if that assumption seems correct by making a quantile-quantile plot for the sample of differences with the normal distribution.\n\nx = c(157, 52, -17, 8, 427, -126, 218, 330, 72, 361)\nqqnorm(x)\nqqline(x)\n\n\n\n\n\n\n\n\nYes, it seems to be ok.\nModel: \\(X = \\text{\"Difference in energy consumption within pairs\"} \\sim N(\\mu,\\sigma)\\)\nHypothesis: \\(H_0: \\mu = 0\\) against \\(H_1: \\mu \\neq 0\\)\nTest rule: We choose a significance level of \\(\\alpha=0.05\\) and test by making a 95% two-sided confidence interval for the expected value and reject \\(H_0\\) if the interval does not cover zero.\n\\[I_{\\mu}: \\bar{x} \\pm t_{\\alpha/2}(n-1)\\frac{s}{\\sqrt{n}}\\]\nWe derive the following \\(\\bar{x} = 148.2\\) and \\(s = 182\\) from the sample and get the interval \\((17.99,278.41)\\)\n⇒ \\(H_0\\) is rejected on the significance level 5%.\n\n\n\nWe are looking for a test where we do not assume any distribution about the sample. Such a test is also called a non-parametric test.\nAn example is a \\(\\chi^2\\)-test, Mann-Whitney U-test.\n\n\nWe note how many of the differences that are positive and how many that are negative.\n\n\n\n\n\n\nExample. Energy consumption (cont.)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPair\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\nDiff CF-Healthy\n157\n52\n-17\n8\n427\n-126\n218\n330\n72\n361\n\n\nSign\n+\n+\n-\n+\n+\n-\n+\n+\n+\n+\n\n\n\n\n\nModel: \\(W = \\text{\"Number of positive differences out of 10 possible\"}\\sim Bin(10,p)\\) where \\(p\\) is the probability to have a positive difference.\nWe observe \\(w = 8\\)\nHypotheses:\n\\(H_0: p = 0.5\\) (it is as likely to get + as -)\n\\(H_1: p &gt; 0.5\\) (it is more likely to get + than -)\nTest rule: We choose a significance level to be \\(\\alpha = 0.05\\).\nUnder \\(H_0\\), then \\(W\\sim Bin(10,0.5)\\)\nWe test with the direct method:\n\\(p\\)-value \\(=P(W\\geq 8|H_0) = 1 -P(W\\leq 7|H_0) = 1 - 0.945 = 0.055\\)\n⇒ \\(H_0\\) cannot be rejected because the \\(p\\)-value &gt; \\(\\alpha\\).\n\n\n\n\n\n\n\n\n\n\nExample. Physio at a school\n\n\n\nIn a school, a small pilot study was conducted to see if a different type of sports training could affect the physical performance of schoolchildren in a short period. Sixteen children, similar in terms of physical capacity, were selected. The children were randomly divided into two groups. For one month, half of the children (Group A) followed the normal curriculum in Physical Education and Health, while the other children (Group B) also participated in the special training. After one month, the children collectively ran a short cross-country track, and their times were recorded. Two children in Group A were sick on the day of the test. Results (seconds):\n\n\n\nGroup A\n64\n62\n73\n54\n66\n71\n\n\n\n\nGroup B\n53\n74\n70\n59\n42\n38\n48\n60\n\n\n\n\n\nIf one does not want to assume a distribution for the two samples, instead, a non-parametric (or distribution-free) test can be conducted. The Mann-Whitney rank sum test involves ranking the values in both samples and calculating the sum of ranks for each, then creating a test statistic and comparing it to a critical region for the test statistic. Below, I have replaced the observations with their ranks and calculated the rank sum for each group.\n\nRanks\n\n\n\n\n\n\n\n\n\n\n\nRank sum\n\n\n\n\nGroup A\n9\n8\n13\n5\n10\n12\n\n\n57\n\n\nGroup B\n4\n14\n11\n6\n2\n1\n3\n7\n48\n\n\n\nWe won’t go into detail about non-parametric tests. However, it’s important to be aware that non-parametric tests exist and understand their advantages and disadvantages.\n\n\n\n\n\n\n\n\n\n\n\nSituation\nAssumes normal distribution\nParametric test\nDistribution free\nNon-parametric test\n\n\n\n\nOne sample\nt-test\nsign test\n\n\nTwo paired samples\nt-test on differences\nsign test on differences\n\n\nTwo independent samples\nt-test for two independent samples\nrank sum test (Mann-Whitney)\n\n\nSeveral independent samples\none way analysis of variance\nrank sum test (Kruskal-Wallis)\n\n\n\n\n\n\n(+) Do not require assumptions about data distribution.\n(+) Work for small samples.\n(+) Robust against outliers.\n(-) Not as “sensitive” (have lower power) as tests based on normal distribution.\n(-) The null hypothesis is usually not as precisely specified as in “traditional” tests.\n(-) Do not utilise all information about the distribution provided in the data - often based on ranks, not the actual values."
  },
  {
    "objectID": "F9_notes_ENG.html#a-assume-that-the-sample-is-from-a-normal-distribution",
    "href": "F9_notes_ENG.html#a-assume-that-the-sample-is-from-a-normal-distribution",
    "title": "F9. Statistical inference on discrete and categorical data",
    "section": "",
    "text": "Let us check if that assumption seems correct by making a quantile-quantile plot for the sample of differences with the normal distribution.\n\nx = c(157, 52, -17, 8, 427, -126, 218, 330, 72, 361)\nqqnorm(x)\nqqline(x)\n\n\n\n\n\n\n\n\nYes, it seems to be ok.\nModel: \\(X = \\text{\"Difference in energy consumption within pairs\"} \\sim N(\\mu,\\sigma)\\)\nHypothesis: \\(H_0: \\mu = 0\\) against \\(H_1: \\mu \\neq 0\\)\nTest rule: We choose a significance level of \\(\\alpha=0.05\\) and test by making a 95% two-sided confidence interval for the expected value and reject \\(H_0\\) if the interval does not cover zero.\n\\[I_{\\mu}: \\bar{x} \\pm t_{\\alpha/2}(n-1)\\frac{s}{\\sqrt{n}}\\]\nWe derive the following \\(\\bar{x} = 148.2\\) and \\(s = 182\\) from the sample and get the interval \\((17.99,278.41)\\)\n⇒ \\(H_0\\) is rejected on the significance level 5%."
  },
  {
    "objectID": "F9_notes_ENG.html#b-assume-no-distribution-for-the-sample",
    "href": "F9_notes_ENG.html#b-assume-no-distribution-for-the-sample",
    "title": "F9. Statistical inference on discrete and categorical data",
    "section": "",
    "text": "We are looking for a test where we do not assume any distribution about the sample. Such a test is also called a non-parametric test.\nAn example is a \\(\\chi^2\\)-test, Mann-Whitney U-test.\n\n\nWe note how many of the differences that are positive and how many that are negative.\n\n\n\n\n\n\nExample. Energy consumption (cont.)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPair\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\nDiff CF-Healthy\n157\n52\n-17\n8\n427\n-126\n218\n330\n72\n361\n\n\nSign\n+\n+\n-\n+\n+\n-\n+\n+\n+\n+\n\n\n\n\n\nModel: \\(W = \\text{\"Number of positive differences out of 10 possible\"}\\sim Bin(10,p)\\) where \\(p\\) is the probability to have a positive difference.\nWe observe \\(w = 8\\)\nHypotheses:\n\\(H_0: p = 0.5\\) (it is as likely to get + as -)\n\\(H_1: p &gt; 0.5\\) (it is more likely to get + than -)\nTest rule: We choose a significance level to be \\(\\alpha = 0.05\\).\nUnder \\(H_0\\), then \\(W\\sim Bin(10,0.5)\\)\nWe test with the direct method:\n\\(p\\)-value \\(=P(W\\geq 8|H_0) = 1 -P(W\\leq 7|H_0) = 1 - 0.945 = 0.055\\)\n⇒ \\(H_0\\) cannot be rejected because the \\(p\\)-value &gt; \\(\\alpha\\)."
  },
  {
    "objectID": "F9_notes_ENG.html#rang-sum-test",
    "href": "F9_notes_ENG.html#rang-sum-test",
    "title": "F9. Statistical inference on discrete and categorical data",
    "section": "",
    "text": "Example. Physio at a school\n\n\n\nIn a school, a small pilot study was conducted to see if a different type of sports training could affect the physical performance of schoolchildren in a short period. Sixteen children, similar in terms of physical capacity, were selected. The children were randomly divided into two groups. For one month, half of the children (Group A) followed the normal curriculum in Physical Education and Health, while the other children (Group B) also participated in the special training. After one month, the children collectively ran a short cross-country track, and their times were recorded. Two children in Group A were sick on the day of the test. Results (seconds):\n\n\n\nGroup A\n64\n62\n73\n54\n66\n71\n\n\n\n\nGroup B\n53\n74\n70\n59\n42\n38\n48\n60\n\n\n\n\n\nIf one does not want to assume a distribution for the two samples, instead, a non-parametric (or distribution-free) test can be conducted. The Mann-Whitney rank sum test involves ranking the values in both samples and calculating the sum of ranks for each, then creating a test statistic and comparing it to a critical region for the test statistic. Below, I have replaced the observations with their ranks and calculated the rank sum for each group.\n\nRanks\n\n\n\n\n\n\n\n\n\n\n\nRank sum\n\n\n\n\nGroup A\n9\n8\n13\n5\n10\n12\n\n\n57\n\n\nGroup B\n4\n14\n11\n6\n2\n1\n3\n7\n48\n\n\n\nWe won’t go into detail about non-parametric tests. However, it’s important to be aware that non-parametric tests exist and understand their advantages and disadvantages."
  },
  {
    "objectID": "F9_notes_ENG.html#overview-of-tests-for-continuous-data",
    "href": "F9_notes_ENG.html#overview-of-tests-for-continuous-data",
    "title": "F9. Statistical inference on discrete and categorical data",
    "section": "",
    "text": "Situation\nAssumes normal distribution\nParametric test\nDistribution free\nNon-parametric test\n\n\n\n\nOne sample\nt-test\nsign test\n\n\nTwo paired samples\nt-test on differences\nsign test on differences\n\n\nTwo independent samples\nt-test for two independent samples\nrank sum test (Mann-Whitney)\n\n\nSeveral independent samples\none way analysis of variance\nrank sum test (Kruskal-Wallis)"
  },
  {
    "objectID": "F9_notes_ENG.html#advantages-and-disadvantages-of-non-parametric-tests",
    "href": "F9_notes_ENG.html#advantages-and-disadvantages-of-non-parametric-tests",
    "title": "F9. Statistical inference on discrete and categorical data",
    "section": "",
    "text": "(+) Do not require assumptions about data distribution.\n(+) Work for small samples.\n(+) Robust against outliers.\n(-) Not as “sensitive” (have lower power) as tests based on normal distribution.\n(-) The null hypothesis is usually not as precisely specified as in “traditional” tests.\n(-) Do not utilise all information about the distribution provided in the data - often based on ranks, not the actual values."
  },
  {
    "objectID": "F9_notes_ENG.html#expected-value-and-variance-for-the-estimate-of-the-proportion",
    "href": "F9_notes_ENG.html#expected-value-and-variance-for-the-estimate-of-the-proportion",
    "title": "F9. Statistical inference on discrete and categorical data",
    "section": "Expected value and variance for the estimate of the proportion",
    "text": "Expected value and variance for the estimate of the proportion\nShow that the estimate of the proportion is unbiased and derive its variance.\n\\(E(\\hat{p})=E(\\frac{X}{n}) = \\frac{E(X)}{n} = \\frac{n\\cdot p}{n} = p\\) ⇒ Unbiased!\n\\(V(\\hat{p})=V(\\frac{X}{n}) = \\frac{V(X)}{n^2} = \\frac{n\\cdot p \\cdot (1-p)}{n^2} = \\frac{p \\cdot (1-p)}{n}\\)\nThe variance for the estimate of the proportion becomes smaller when increasing \\(n\\)!"
  },
  {
    "objectID": "F9_notes_ENG.html#normal-approximation-of-the-estimate-of-the-proportion",
    "href": "F9_notes_ENG.html#normal-approximation-of-the-estimate-of-the-proportion",
    "title": "F9. Statistical inference on discrete and categorical data",
    "section": "Normal approximation of the estimate of the proportion",
    "text": "Normal approximation of the estimate of the proportion\nIf \\(n\\cdot p \\cdot (1-p) &gt; 10\\) then we can apply normal approximation of the binomial distribution into a normal distribution\n\\[X \\overset{A} \\sim N(np,\\sqrt{np(1-p)})\\]\nWe can with the same way of reasoning, say that the sampling distribution for the estimate of the proportion is approximately normal\n\\[\\frac{X}{n} \\overset{A} \\sim N(p,\\sqrt{\\frac{p(1-p)}{n}})\\] where \\(\\hat{p} = \\frac{x}{n}\\)."
  },
  {
    "objectID": "F9_notes_ENG.html#hypoteses-test-for-a-propotion",
    "href": "F9_notes_ENG.html#hypoteses-test-for-a-propotion",
    "title": "F9. Statistical inference on discrete and categorical data",
    "section": "Hypoteses test for a propotion",
    "text": "Hypoteses test for a propotion\nThis is how the hypotheses for a proportion looks like when we have a two-sided alternatvie hypothesis.\n\\(H_0: p = p_0\\)\n\\(H_1: p \\neq p_0\\)\n\nNote that it is rare to test if a proportion is zero. Instead, one may be interested in whether it is equal to a certain value \\(p_0\\).\n\n\nConfidence interval for an estimate of a proportion\nWhen we perform a normal approximation, we can proceed to create a two-sided confidence interval by inserting the proportion estimate into the margin of error:\n\\[I_p: \\hat{p} \\pm \\lambda_{\\alpha/2}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\]\nThe interval \\(I_p\\) can be used for hypotheses testing. We reject \\(H_0\\) at the significance level \\(\\alpha\\) if \\(p_0\\) is not in the range of the interval.\n\n\nTest quantity for an estimate of a proportion\nWe can also test the hypotheses with a test quantity that we compare to a critical area.\nIf \\(H_0\\) is true, then \\(X \\sim Bin(n,p_0)\\)\nHere we can examine if is possible to do normal approximation by checking if \\(n\\cdot p_0 \\cdot (1-p_0) &gt; 10\\).\nThe test quantity is\n\\[z = \\left|  \\frac{\\hat{p}-p_0}{\\sqrt{\\frac{p_0(1-p_0)}{n}}} \\right|\\]\n\nNote that this time we specify the error of the sample mean by \\(p_0\\) and not with \\(\\hat{p}\\).\n\nWe reject \\(H_0\\) if \\(z &gt; \\lambda_{\\alpha/2}\\).\n\n\n\n\n\n\nExample. Pollen allergy\n\n\n\nIn 1990, a survey was conducted in Stockholm County to investigate the prevalence of pollen allergy among certain sensitive groups. 500 individuals aged 20-64 years were randomly selected, and among these, 23% had pollen allergy.\n\nWhat can we say about the proportion of pollen allergy sufferers in the population?\n\n\n\n\\(X = \\text{\"number if persons allergic to pollen\"} \\sim Bin(500,p)\\)\nCan we do normal approximation? Yes, since \\(500\\cdot 0.23 \\cdot (1-0.23) = 88.55 &gt; 10\\)\nWe create a 95% confidence interval for the proportion of people allergic to pollen:\n\\[I_p: 0.23 \\pm 1.96\\sqrt{\\frac{0.23(1-0.23)}{500}} = (0.19, 0.27)\\]"
  },
  {
    "objectID": "F9_notes_ENG.html#two-proportions",
    "href": "F9_notes_ENG.html#two-proportions",
    "title": "F9. Statistical inference on discrete and categorical data",
    "section": "Two proportions",
    "text": "Two proportions\nModel:\n\\(X = \\text{\"number of individuals allergic to pollen 1990\"} \\sim Bin(500,p_x)\\)\n\\(Y = \\text{\"number of individuals allergic to pollen 1994\"} \\sim Bin(500,p_y)\\)\nHypotheses:\n\\(H_0: p_x = p_y\\)\n\\(H_1: p_x \\neq p_y\\)\nThis is the same thing as\n\\(H_0: p_x - p_y = 0\\)\n\\(H_1: p_x - p_y \\neq 0\\)"
  },
  {
    "objectID": "F9_notes_ENG.html#test-with-a-confidence-interval-for-the-difference-in-proportions",
    "href": "F9_notes_ENG.html#test-with-a-confidence-interval-for-the-difference-in-proportions",
    "title": "F9. Statistical inference on discrete and categorical data",
    "section": "Test with a confidence interval for the difference in proportions",
    "text": "Test with a confidence interval for the difference in proportions\nCan we do normal approximation? We have already shown it for \\(X\\). We can do normal approximation for \\(Y\\) since \\(500\\cdot 0.29 \\cdot (1-0.29) = 102.95 &gt; 10\\)\n\\[I_{p_x-p_y}: \\hat{p}_x-\\hat{p}_y \\pm \\underbrace{\\lambda_{\\alpha/2}}_{\\alpha=0.05}\\sqrt{\\frac{\\hat{p}_x(1-\\hat{p}_x)}{500}+\\frac{\\hat{p}_y(1-\\hat{p}_y)}{500} } = (-0.114,-0.006)\\]\n\\(H_0\\) is rejected at the significance level 0.05 since the interval does not cover zero."
  },
  {
    "objectID": "F9_notes_ENG.html#test-with-test-quantity-and-critical-area",
    "href": "F9_notes_ENG.html#test-with-test-quantity-and-critical-area",
    "title": "F9. Statistical inference on discrete and categorical data",
    "section": "Test with test quantity and critical area",
    "text": "Test with test quantity and critical area\nWhen \\(H_0\\) is true, then \\(p_x = p_y = p_0\\)\nWe estimate \\(p_0\\) by combining the information from 1990 and 1994 (pooled estimation)\n\\[\\hat{p}_0 = \\frac{x + y}{n_x + n_y} = \\frac{0.23\\cdot 500 + 0.29\\cdot 500}{500 + 500} = 0.26\\]\nCan we do normal approximation? Yes, since \\(500\\cdot \\hat{p}_0 \\cdot (1-\\hat{p}_0) = 96.2 &gt; 10\\)\nTest quantity\n\\[z = \\left| \\frac{\\hat{p}_x - \\hat{p}_y - 0}{\\sqrt{\\hat{p}_0 (1-\\hat{p}_0)(\\frac{1}{n_x}+\\frac{1}{n_y})}} \\right| = 2.1628\\]\nReject \\(H_0\\) at significance level 0.05 since \\(z &gt; \\lambda_{\\alpha/2} = 1.96\\).\n⇒ There has been a change in the proportion of individuals with allergy to pollen during the period 1990 to 1994."
  },
  {
    "objectID": "F9_notes_ENG.html#categorical-data",
    "href": "F9_notes_ENG.html#categorical-data",
    "title": "F9. Statistical inference on discrete and categorical data",
    "section": "Categorical data",
    "text": "Categorical data\nA random experiment has \\(k\\) different outcomes. Make \\(n\\) independent experiments, and count the number of times an experiment ends up in each category.\n\n\n\n\n\n\nExample. Genetics\n\n\n\nEach individual in a population belongs in one out of four gene categories \\(K_1\\),\\(K_2\\),\\(K_3\\) och \\(K_4\\). A study included 160 randomly chosen individuals that were examined and categorised. The results were :\n\n\n\ncategory\n\\(K_1\\)\n\\(K_2\\)\n\\(K_3\\)\n\\(K_4\\)\n\n\nfrequency\n78\n42\n27\n13"
  },
  {
    "objectID": "F9_notes_ENG.html#two-situations-with-categorical-data",
    "href": "F9_notes_ENG.html#two-situations-with-categorical-data",
    "title": "F9. Statistical inference on discrete and categorical data",
    "section": "Two situations with categorical data",
    "text": "Two situations with categorical data\nWe will go through two common situations for hypotheses testing with categorical data using \\(\\chi^2\\)-tests.\n\nTest fit of a model\nContingency table analysis (Homogeneity or independence tests)"
  },
  {
    "objectID": "F9_notes_ENG.html#test-of-model-fit",
    "href": "F9_notes_ENG.html#test-of-model-fit",
    "title": "F9. Statistical inference on discrete and categorical data",
    "section": "Test of model fit",
    "text": "Test of model fit\n\n\n\n\n\n\nExample. Genetics (cont.)\n\n\n\nEach individual in a population belongs in one out of four gene categories \\(K_1\\),\\(K_2\\),\\(K_3\\) och \\(K_4\\). A study included 160 randomly chosen individuals that were examined and categorised. The results were :\n\nObservation-table\n\n\ncategory\n\\(K_1\\)\n\\(K_2\\)\n\\(K_3\\)\n\\(K_4\\)\n\n\nfrequency\n78\n42\n27\n13\n\n\n\n\nAccording to a theoretical model, the sizes of the four categories should have the following relation 9:3:3:1. Do the observed data confirm the theory?\n\n\n\nLet \\(O_i\\) be the observations for category \\(i=1,\\dots,r\\). In total we have \\(n\\) observations.\nHypotheses:\n\\(H_0: p_1=\\frac{9}{16}, p_2=\\frac{3}{16}, p_3=\\frac{3}{16}, p_4=\\frac{1}{16}\\) (the theoretical model)\n\\(H_1: \\text{some of these probabilities are wrong}\\) (the theoretical model is not correct)\nTest rule:\nLet \\(E_i=n\\cdot p_i\\) be the expected number of outcomes for category \\(i\\) when \\(H_0\\) is true.\n\nE-table\n\n\ncategory\n\\(K_1\\)\n\\(K_2\\)\n\\(K_3\\)\n\\(K_4\\)\n\n\nfrequency\n\\(160\\frac{9}{16}=90\\)\n30\n30\n10\n\n\n\nWe create a test quantity that when \\(H_0\\) is true becomes\n\\[\\chi^2=\\sum_{i=1}^r \\frac{(O_i-E_i)^2}{E_i} \\sim \\chi^2(r-1)\\]\nReject \\(H_0\\) if the test quantity \\(\\chi^2 &gt; \\chi^2_{\\alpha}\\)\nIn the example, the test quantity is \\(\\chi^2 = \\sum_{i=1}^r \\frac{(O_i-E_i)^2}{E_i} = \\frac{(78-90)^2}{90} +\\frac{(42-30)^2}{30}\\)+\\(\\frac{(42-30)^2}{30}\\)+\\(\\frac{(13-10)^2}{10} = 7.6\\)\nThe quantile from the \\(\\chi^2\\)-distribution with 4-1 degrees of freedom is 7.8147279\n⇒ \\(H_0\\) can not be rejected at significance level 0.05"
  },
  {
    "objectID": "F9_notes_ENG.html#contingency-table",
    "href": "F9_notes_ENG.html#contingency-table",
    "title": "F9. Statistical inference on discrete and categorical data",
    "section": "Contingency table",
    "text": "Contingency table\n\n\n\n\n\n\nExample. Ulcer and blood type\n\n\n\nIs there a correlation between blood type and the risk of stomach ulcers? Blood type was determined for 1655 stomach ulcer patients and for a control group of 10,000 individuals from the same city. Results:\n\nO-table\n\n\n\n\n\n\n\n\n\n\n\n0\nA\nB\nAB\nTotal\n\n\n\n\nPatients with ulcer\n911\n59\n124\n41\n\\(n_{1.}\\)=1655\n\n\nControl group\n4578\n4219\n890\n313\n\\(n_{2.}\\)=10000\n\n\nTotal\n\\(n_{.1}\\)=5489\n\\(n_{.2}\\)=4798\n\\(n_{.3}\\)=1014\n\\(n_{.4}\\)=354\n\\(n\\) = 11655\n\n\n\n\n\nLet \\(p_{i.}\\) and \\(p_{.j}\\) be the probabilities that an observation belong to the category on row \\(i\\) or column \\(j\\), respectively.\nWe can estimate \\(p_{i.}\\) with \\(n_{i.}\\), the number of observations in row \\(i\\), divided by the the total number of observations \\(n\\):\n\\[p_{i.} = \\frac{n_{i.}}{n}\\]\nHypotheses:\n\\(H_0: p_{ij} = p_{i.}p_{.j}\\) for all \\(i\\) and \\(j\\) (blood group and ulcer are independent)\n\\(H_1: H_0 \\text{ is not true}\\)\nTest rule: When \\(H_0\\) is true the expected number of observations in each combination of categories r are \\(E_{ij}=np_{ij}=np_{i.}p_{.j}\\), which when we consider the estimated probabilities become \\(n\\frac{n_{i.}}{n}\\frac{n_{.j}}{n} = \\frac{n_{i.}n_{.j}}{n}\\)\nE.g. \\(E_{11}=\\frac{n_{1.}n_{.1}}{n} = \\frac{1655 \\cdot 5489}{11655} = 779.4332904\\)\n\nE-table\n\n\n\n0\nA\nB\nAB\n\n\n\n\nPatients with ulcer\n779.4\n681.3\n144.0\n50.3\n\n\nControl group\n4709.6\n4116.7\n870.0\n303.7\n\n\n\nUnder \\(H_0\\) the sampling distribution for the test quantity\n\\[\\chi^2=\\sum_{i=1}^r \\sum_{j=1}^c \\frac{(O_{ij}-E_{ij})^2}{E_{ij}} \\sim \\chi^2((r-1)(c-1))\\]\nwhere \\(r\\) is the number of categories distributed over the rows and \\(c\\) is the number distributed over the columns.\n\\(\\chi^2=\\frac{(911-779.4)^2}{779.4} + \\frac{(579-681.3)^2}{681.3} \\dots = 49.0153\\)\nWe reject \\(H_0\\) since the test quantity is larger than the quantile \\(\\chi^2_{\\alpha}((2-1)(4-1)) = 7.8147279\\)\n\nTest of independence or homogeneity\nA contingency table test can be done to test\n\nindependence, if we want to show that the occurrences of two categories are independent of each other\nhomogeneity, if we want to show that two samples come from the same distribution."
  },
  {
    "objectID": "F9_notes_ENG.html#contingency-table-test-test-for-homogeneity",
    "href": "F9_notes_ENG.html#contingency-table-test-test-for-homogeneity",
    "title": "F9. Statistical inference on discrete and categorical data",
    "section": "Contingency table test (Test for homogeneity)",
    "text": "Contingency table test (Test for homogeneity)\nWe have \\(c\\) categories and \\(r\\) samples.\n\nO-table\n\n\n\n\ncategories\n\n\n\n\n\n\n\nsample\n1\n2\n\\(\\dots\\)\nc\nsum\n\n\n1\n\\(O_{11}\\)\n\\(O_{12}\\)\n\n\\(O_{1c}\\)\n\\(n_{1.}\\)\n\n\n2\n\\(O_{21}\\)\n\n\n\n\n\n\n\\(\\vdots\\)\n\n\n\n\n\n\n\nr\n\\(O_{r1}\\)\n\n\n\\(O_{rc}\\)\n\\(n_{r.}\\)\n\n\nsum\n\\(n_{.1}\\)\n\n\n\\(n_{.c}\\)\n\\(n\\)\n\n\n\nThe null hypothesis when we make a homogeneity test is that the probability to fall into a category (e.g. \\(j\\)) is the same regardless of which sample one belong to (\\(i = 1, \\dots, r\\)).\n\\(H_0: p_{1j} = p_{2j} = \\dots = p_{rj}\\text{ for all categories }j\\)\n\\(H_1: H_0 \\text{ does not hold}\\)\nEstimation of probabilities, test quantity and test rule is the same as for an independence test."
  },
  {
    "objectID": "index_eng.html",
    "href": "index_eng.html",
    "title": "Lecture notes in English",
    "section": "",
    "text": "L0. Descriptive statistics (self studies)\nQualitative and quantitative data\nMedian\nMean\nVariance\nStandard deviation\nQuantile\nBar chart\nHistogram\nBoxplot\nCoefficient of correlation\nQuantile-quantile plot\n\n\nL1. Basic probability theory\nProbability\nRandom experiment\nOutcome\nOutcome space\nEvent\nProbability rules\nVenn diagram\nComplementary event\nComplementary event\nThe additive theorem of probability\nIndependent events\nConditional probability\nThe multiplication theorem of probability\nThe law of total probability\nBayes’ theorem (Bayes’ rule)\nCombinatorics\nLecture notes 1\n\n\nL2. Discrete random variables\nRandom variables\nDiscrete and continuous r.v.\nProbability function\nDistribution function\nBernoulli distribution\nDiscrete uniform distribution\nExpected value, variance, standard deviation\nExpected value of a discrete r.v.\nVariance of a discrete r.v.\nPoisson distribution\nBinomial distribution\nExpected value of a function of a random variable\nLecture notes 2\n\n\nL3. Discrete and continuous random variables\nContinuous random variables\nUniform distribution\nDensity function\nDistribution function for a continuous random variable\nComplementary event for a continuous r.v.\nExpected value of a continuous r.v.\nVariance of a continuous r.v.\nExponential distribution\nNormal distribution\nLecture notes 3\n\n\nL4. Continuous random variables, normal distribution\nLognormal distribution\nStandardised normal distribution\nNormal quantiles\nExpected value and variance of a linear combination of a random variable\nExpected value and variance for sums of random variables\nDistribution of a sum of normally distributed random variables\nPopulation/parameters - Sample/statistica\nLecture notes 4\n\n\nL5. Sampling distributions, Central Limit Theorem\nStatistical model\nSampling distributions\nCentral Limit Theorem\nCGS and the binomial distribution\nNormal approximations\nFrom probability theory to statistical inference\nParameter estimation (estimates)\nUnbiased and effective estimator\nt-distribution\nLecture notes 5\n\n\nL6. Estimation, confidence interval\nSampling distributions for known and unknown variance\nEstimation and confidence interval\nConfidence interval with known and unknown variance\nParameter estimation in a lognormal distribution\nLecture notes 6\n\n\nL7. Hypothesis testing\nHypothesis testing in three ways\n\nTest statistic with critical area\nConfidence interval\nP-value (direct method)\n\nPossible errors\nPower of an hypothesis test\nLecture notes 7\n\n\nL8. Two random samples\n\\(\\chi^2\\) distribution\nHypothesis test for variance\nCompare populations\nPaired samples\nIndependent samples\nEqual and non-equal variance\nTest if variances are equal\nF distribution\nLecture notes 8\n\n\nL9. Inference of discrete and categorical data\nDirect method and normal approximation\nNon-parametric test\nInference of a proportion\nInference on the difference between to proportions\nAnalysis of categorical data:\n\nTest model fit\nTesting homogeneity\nIndependence test\n\nParametric vs non-parametric test\nLecture notes 9\n\n\nL10. Simple linear regression\nBivariate normal distribution\nCovariance\nCorrelation\nCorrelation analysis\nRegression analysis\nLecture notes 10\n\n\nL11. Analysis of variance\nAnalysis of variance\nRepetition\nLecture notes 11\n\n\nL12. Reserv och repetition\nRepetition"
  }
]